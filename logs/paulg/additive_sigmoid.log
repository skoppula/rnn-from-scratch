(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=2 python lstm_all_variants.py -t -d paulg -v additive_sigmoid
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'paulg', 'train': True, 'variant': 'additive_sigmoid', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:81:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3167 get requests, put_count=2834 evicted_count=1000 eviction_rate=0.352858 and unsatisfied allocation rate=0.452479
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3337 get requests, put_count=3497 evicted_count=1000 eviction_rate=0.285959 and unsatisfied allocation rate=0.258616
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
('epoch', 0, 'train_loss:', 9.3310651278495786, 'val_loss:', 2.006901240348816)
('epoch', 1, 'train_loss:', 7.3215971565246578, 'val_loss:', 1.7198780465126038)
('epoch', 2, 'train_loss:', 6.7000696730613711, 'val_loss:', 1.6428778219223021)
('epoch', 3, 'train_loss:', 6.4872528028488157, 'val_loss:', 1.6027914118766784)
('epoch', 4, 'train_loss:', 6.3568532133102416, 'val_loss:', 1.5778402829170226)
('epoch', 5, 'train_loss:', 6.2535359239578243, 'val_loss:', 1.5508255791664123)
('epoch', 6, 'train_loss:', 6.1649486875534061, 'val_loss:', 1.530852084159851)
('epoch', 7, 'train_loss:', 6.1005704140663148, 'val_loss:', 1.517227520942688)
('epoch', 8, 'train_loss:', 6.0323760223388669, 'val_loss:', 1.4972694706916809)
('epoch', 9, 'train_loss:', 5.9699284291267398, 'val_loss:', 1.4846889400482177)
('epoch', 10, 'train_loss:', 5.9119739747047424, 'val_loss:', 1.4687648749351501)
('epoch', 11, 'train_loss:', 5.8556317496299748, 'val_loss:', 1.456592915058136)
('epoch', 12, 'train_loss:', 5.8017728877067567, 'val_loss:', 1.4419283390045166)
('epoch', 13, 'train_loss:', 5.7476075696945195, 'val_loss:', 1.4272926926612854)
('epoch', 14, 'train_loss:', 5.6968693733215332, 'val_loss:', 1.4159982204437256)
('epoch', 15, 'train_loss:', 5.6525520086288452, 'val_loss:', 1.4049598217010497)
('epoch', 16, 'train_loss:', 5.6056950736045836, 'val_loss:', 1.3945604968070984)
('epoch', 17, 'train_loss:', 5.5625159072875974, 'val_loss:', 1.3808421850204469)
('epoch', 18, 'train_loss:', 5.5218483090400694, 'val_loss:', 1.3721554517745971)
('epoch', 19, 'train_loss:', 5.4812703299522401, 'val_loss:', 1.3649239182472228)
('epoch', 20, 'train_loss:', 5.4393279027938846, 'val_loss:', 1.3512202095985413)
('epoch', 21, 'train_loss:', 5.3999640536308284, 'val_loss:', 1.3459016609191894)
('epoch', 22, 'train_loss:', 5.3589142966270451, 'val_loss:', 1.3349497222900391)
('epoch', 23, 'train_loss:', 5.3181421732902523, 'val_loss:', 1.3253421521186828)
('epoch', 24, 'train_loss:', 5.289146797657013, 'val_loss:', 1.3165877842903138)
('epoch', 25, 'train_loss:', 5.2536837911605838, 'val_loss:', 1.307945249080658)
('epoch', 26, 'train_loss:', 5.2241537809371952, 'val_loss:', 1.2959718751907348)
('epoch', 27, 'train_loss:', 5.1846437811851498, 'val_loss:', 1.289044599533081)
('epoch', 28, 'train_loss:', 5.1470076966285703, 'val_loss:', 1.2804649996757507)
('epoch', 29, 'train_loss:', 5.1198670268058777, 'val_loss:', 1.2730913949012757)
('epoch', 30, 'train_loss:', 5.0786591672897341, 'val_loss:', 1.2669014453887939)
('epoch', 31, 'train_loss:', 5.0533692312240603, 'val_loss:', 1.2580859994888305)
('epoch', 32, 'train_loss:', 5.0170482742786406, 'val_loss:', 1.2520973932743074)
('epoch', 33, 'train_loss:', 4.992224153280258, 'val_loss:', 1.2399921405315399)
('epoch', 34, 'train_loss:', 4.9523510396480557, 'val_loss:', 1.2346601200103759)
('epoch', 35, 'train_loss:', 4.9250442838668826, 'val_loss:', 1.2267634427547456)
('epoch', 36, 'train_loss:', 4.8950524616241458, 'val_loss:', 1.2204960787296295)
('epoch', 37, 'train_loss:', 4.8681293129920959, 'val_loss:', 1.214742830991745)
('epoch', 38, 'train_loss:', 4.8390029346942898, 'val_loss:', 1.2053207552433014)
('epoch', 39, 'train_loss:', 4.8152328228950498, 'val_loss:', 1.1960321211814879)
('epoch', 40, 'train_loss:', 4.7843141245841982, 'val_loss:', 1.1912704348564147)
('epoch', 41, 'train_loss:', 4.7591449594497677, 'val_loss:', 1.1888376915454864)
('epoch', 42, 'train_loss:', 4.733785774707794, 'val_loss:', 1.1781494319438934)
('epoch', 43, 'train_loss:', 4.706133066415787, 'val_loss:', 1.1728024232387542)
('epoch', 44, 'train_loss:', 4.6780364859104155, 'val_loss:', 1.1691634011268617)
('epoch', 45, 'train_loss:', 4.6597976756095889, 'val_loss:', 1.1635060584545136)
('epoch', 46, 'train_loss:', 4.6346438813209536, 'val_loss:', 1.1535241580009461)
('epoch', 47, 'train_loss:', 4.6114938473701477, 'val_loss:', 1.1505489981174468)
('epoch', 48, 'train_loss:', 4.5814914739131929, 'val_loss:', 1.1453653514385223)
('epoch', 49, 'train_loss:', 4.5662373065948483, 'val_loss:', 1.1390531349182129)
('epoch', 50, 'train_loss:', 4.5426547133922579, 'val_loss:', 1.1351767456531525)
('epoch', 51, 'train_loss:', 4.5217725181579587, 'val_loss:', 1.127030107975006)
('epoch', 52, 'train_loss:', 4.5003154957294464, 'val_loss:', 1.1232512760162354)
('epoch', 53, 'train_loss:', 4.480034729242325, 'val_loss:', 1.115848606824875)
('epoch', 54, 'train_loss:', 4.4628651154041288, 'val_loss:', 1.115946135520935)
('epoch', 55, 'train_loss:', 4.4420764780044557, 'val_loss:', 1.1100358843803406)
('epoch', 56, 'train_loss:', 4.4185013687610626, 'val_loss:', 1.1045469236373902)
('epoch', 57, 'train_loss:', 4.4063318157196045, 'val_loss:', 1.1005008232593536)
('epoch', 58, 'train_loss:', 4.382938734292984, 'val_loss:', 1.0946577703952789)
('epoch', 59, 'train_loss:', 4.3616084623336793, 'val_loss:', 1.0891899538040162)
('epoch', 60, 'train_loss:', 4.353945425748825, 'val_loss:', 1.0856858265399933)
('epoch', 61, 'train_loss:', 4.3288298821449276, 'val_loss:', 1.0803678667545318)
('epoch', 62, 'train_loss:', 4.3134205913543697, 'val_loss:', 1.0815158784389496)
('epoch', 63, 'train_loss:', 4.3049354720115662, 'val_loss:', 1.0770162916183472)
('epoch', 64, 'train_loss:', 4.2881377756595613, 'val_loss:', 1.0709433031082154)
('epoch', 65, 'train_loss:', 4.2748138749599454, 'val_loss:', 1.0628949081897736)
('epoch', 66, 'train_loss:', 4.2541070735454563, 'val_loss:', 1.0672240030765534)
('epoch', 67, 'train_loss:', 4.248422235250473, 'val_loss:', 1.0598300564289094)
('epoch', 68, 'train_loss:', 4.2251561093330388, 'val_loss:', 1.0574504661560058)
('epoch', 69, 'train_loss:', 4.2170203351974491, 'val_loss:', 1.0560190224647521)
('epoch', 70, 'train_loss:', 4.2016561627388, 'val_loss:', 1.0507830584049225)
('epoch', 71, 'train_loss:', 4.1871437871456143, 'val_loss:', 1.0459038138389587)
('epoch', 72, 'train_loss:', 4.1732791733741763, 'val_loss:', 1.0468167996406554)
('epoch', 73, 'train_loss:', 4.1670213544368746, 'val_loss:', 1.0395341730117797)
('epoch', 74, 'train_loss:', 4.148590134382248, 'val_loss:', 1.0372822737693788)
('epoch', 75, 'train_loss:', 4.1321435964107511, 'val_loss:', 1.0331362557411194)
('epoch', 76, 'train_loss:', 4.1199873530864712, 'val_loss:', 1.0302826619148255)
('epoch', 77, 'train_loss:', 4.1142645967006679, 'val_loss:', 1.0297677850723266)
('epoch', 78, 'train_loss:', 4.1030561017990115, 'val_loss:', 1.0266065502166748)
('epoch', 79, 'train_loss:', 4.0932863688468935, 'val_loss:', 1.0214103269577026)
('epoch', 80, 'train_loss:', 4.0816419076919557, 'val_loss:', 1.0219950926303865)
('epoch', 81, 'train_loss:', 4.0734829080104831, 'val_loss:', 1.0174652254581451)
('epoch', 82, 'train_loss:', 4.0562529671192173, 'val_loss:', 1.014516988992691)
('epoch', 83, 'train_loss:', 4.0506968808174131, 'val_loss:', 1.011778382062912)
('epoch', 84, 'train_loss:', 4.0421367895603177, 'val_loss:', 1.0124403178691863)
('epoch', 85, 'train_loss:', 4.0318798553943633, 'val_loss:', 1.0119311463832856)
('epoch', 86, 'train_loss:', 4.0160223162174224, 'val_loss:', 1.0089716482162476)
('epoch', 87, 'train_loss:', 4.0085652017593381, 'val_loss:', 1.0028999197483062)
('epoch', 88, 'train_loss:', 4.005423413515091, 'val_loss:', 1.0031314480304718)
('epoch', 89, 'train_loss:', 3.9933915853500368, 'val_loss:', 1.0017673373222351)
('epoch', 90, 'train_loss:', 3.9836476683616637, 'val_loss:', 0.99784539580345155)
('epoch', 91, 'train_loss:', 3.9723858928680418, 'val_loss:', 0.99452317714691163)
('epoch', 92, 'train_loss:', 3.9616737818717955, 'val_loss:', 0.99795982360839841)
('epoch', 93, 'train_loss:', 3.9575900125503538, 'val_loss:', 0.99154991865158082)
('epoch', 94, 'train_loss:', 3.953261214494705, 'val_loss:', 0.9893291234970093)
('epoch', 95, 'train_loss:', 3.9470559513568877, 'val_loss:', 0.99014058113098147)
('epoch', 96, 'train_loss:', 3.9383156466484071, 'val_loss:', 0.98997944951057437)
('epoch', 97, 'train_loss:', 3.9204497611522675, 'val_loss:', 0.98484080910682681)
('epoch', 98, 'train_loss:', 3.9219176065921784, 'val_loss:', 0.98285270094871524)
('epoch', 99, 'train_loss:', 3.9098256456851961, 'val_loss:', 0.98228963255882262)
('epoch', 100, 'train_loss:', 3.9007653498649599, 'val_loss:', 0.98007830858230593)
('epoch', 101, 'train_loss:', 3.8992606973648072, 'val_loss:', 0.97792177438735961)
('epoch', 102, 'train_loss:', 3.8929671573638918, 'val_loss:', 0.97587415099143981)
('epoch', 103, 'train_loss:', 3.887487050294876, 'val_loss:', 0.97457863211631779)
('epoch', 104, 'train_loss:', 3.8785051143169404, 'val_loss:', 0.976472909450531)
('epoch', 105, 'train_loss:', 3.8687034821510315, 'val_loss:', 0.97232913494110107)
('epoch', 106, 'train_loss:', 3.8630445444583894, 'val_loss:', 0.96824889421463012)
('epoch', 107, 'train_loss:', 3.8577806580066683, 'val_loss:', 0.96546669244766237)
('epoch', 108, 'train_loss:', 3.8441560351848603, 'val_loss:', 0.96680031538009648)
('epoch', 109, 'train_loss:', 3.8477809250354769, 'val_loss:', 0.96584845423698429)
('epoch', 110, 'train_loss:', 3.8449293565750122, 'val_loss:', 0.9643260097503662)
('epoch', 111, 'train_loss:', 3.8341419017314911, 'val_loss:', 0.96240410685539246)
('epoch', 112, 'train_loss:', 3.8262827932834624, 'val_loss:', 0.96325652599334721)
('epoch', 113, 'train_loss:', 3.8187550151348115, 'val_loss:', 0.96115289330482478)
('epoch', 114, 'train_loss:', 3.8153182315826415, 'val_loss:', 0.95983355998992925)
('epoch', 115, 'train_loss:', 3.8072591185569764, 'val_loss:', 0.95813883066177363)
('epoch', 116, 'train_loss:', 3.7967231488227844, 'val_loss:', 0.95242369055747989)
('epoch', 117, 'train_loss:', 3.8035715425014498, 'val_loss:', 0.95460076570510866)
('epoch', 118, 'train_loss:', 3.7877720332145692, 'val_loss:', 0.95227097034454344)
('epoch', 119, 'train_loss:', 3.7817334151268005, 'val_loss:', 0.94834822654724116)
('epoch', 120, 'train_loss:', 3.777159984111786, 'val_loss:', 0.94898491621017456)
('epoch', 121, 'train_loss:', 3.7745715308189394, 'val_loss:', 0.95079082012176519)
('epoch', 122, 'train_loss:', 3.772108143568039, 'val_loss:', 0.94466061592102046)
('epoch', 123, 'train_loss:', 3.7642538547515869, 'val_loss:', 0.9428929674625397)
('epoch', 124, 'train_loss:', 3.7576095819473267, 'val_loss:', 0.94491476774215699)
('epoch', 125, 'train_loss:', 3.7545183587074278, 'val_loss:', 0.94622619509696959)
('epoch', 126, 'train_loss:', 3.748797115087509, 'val_loss:', 0.94065887093544009)
('epoch', 127, 'train_loss:', 3.7449195933341981, 'val_loss:', 0.94752974748611452)
('epoch', 128, 'train_loss:', 3.7347068619728088, 'val_loss:', 0.94184263706207272)
('epoch', 129, 'train_loss:', 3.732083787918091, 'val_loss:', 0.94251105189323425)
('epoch', 130, 'train_loss:', 3.7299213147163393, 'val_loss:', 0.93911083459854128)
('epoch', 131, 'train_loss:', 3.7272542119026184, 'val_loss:', 0.93687493681907652)
('epoch', 132, 'train_loss:', 3.7157952964305876, 'val_loss:', 0.93564112424850465)
('epoch', 133, 'train_loss:', 3.7153788745403289, 'val_loss:', 0.9363935053348541)
('epoch', 134, 'train_loss:', 3.707096016407013, 'val_loss:', 0.93541179776191707)
('epoch', 135, 'train_loss:', 3.7066222894191743, 'val_loss:', 0.9314954090118408)
('epoch', 136, 'train_loss:', 3.6955348217487334, 'val_loss:', 0.93359392523765561)
('epoch', 137, 'train_loss:', 3.7021401131153109, 'val_loss:', 0.93298594951629643)
('epoch', 138, 'train_loss:', 3.6990740871429444, 'val_loss:', 0.93048766613006595)
('epoch', 139, 'train_loss:', 3.6847899401187898, 'val_loss:', 0.93065267682075503)
('epoch', 140, 'train_loss:', 3.6870332396030427, 'val_loss:', 0.92612296462059018)
('epoch', 141, 'train_loss:', 3.6737802338600161, 'val_loss:', 0.92667647242546081)
('epoch', 142, 'train_loss:', 3.6745000994205474, 'val_loss:', 0.93007782816886897)
('epoch', 143, 'train_loss:', 3.6720325362682344, 'val_loss:', 0.92580543398857118)
('epoch', 144, 'train_loss:', 3.6679482722282408, 'val_loss:', 0.92484052658081051)
('epoch', 145, 'train_loss:', 3.6686783885955809, 'val_loss:', 0.92617510676383974)
('epoch', 146, 'train_loss:', 3.6604927682876589, 'val_loss:', 0.92411879420280452)
('epoch', 147, 'train_loss:', 3.6522799360752107, 'val_loss:', 0.92599106788635255)
('epoch', 148, 'train_loss:', 3.6631143200397491, 'val_loss:', 0.92007148504257197)
('epoch', 149, 'train_loss:', 3.6526519358158112, 'val_loss:', 0.92216469407081603)
('epoch', 150, 'train_loss:', 3.6485712981224059, 'val_loss:', 0.92177545666694638)
('epoch', 151, 'train_loss:', 3.641840646266937, 'val_loss:', 0.9217501139640808)
('epoch', 152, 'train_loss:', 3.6332757306098937, 'val_loss:', 0.92181824564933779)
('epoch', 153, 'train_loss:', 3.6385026371479032, 'val_loss:', 0.91690976977348326)
('epoch', 154, 'train_loss:', 3.6361773836612703, 'val_loss:', 0.91574571371078495)
('epoch', 155, 'train_loss:', 3.6290962839126588, 'val_loss:', 0.91653078556060796)
('epoch', 156, 'train_loss:', 3.6269833207130433, 'val_loss:', 0.9167140316963196)
('epoch', 157, 'train_loss:', 3.6201194751262666, 'val_loss:', 0.91466475367546085)
('epoch', 158, 'train_loss:', 3.621979213953018, 'val_loss:', 0.91570943713188169)
('epoch', 159, 'train_loss:', 3.6153687942028045, 'val_loss:', 0.91364614248275755)
('epoch', 160, 'train_loss:', 3.6051743769645692, 'val_loss:', 0.91406696796417242)
('epoch', 161, 'train_loss:', 3.6091485500335692, 'val_loss:', 0.9079645657539368)
('epoch', 162, 'train_loss:', 3.6018730425834655, 'val_loss:', 0.91241222500801089)
('epoch', 163, 'train_loss:', 3.6080581796169282, 'val_loss:', 0.91219671368598942)
('epoch', 164, 'train_loss:', 3.5998249256610872, 'val_loss:', 0.91105068683624268)
('epoch', 165, 'train_loss:', 3.5970160186290743, 'val_loss:', 0.9094051718711853)
('epoch', 166, 'train_loss:', 3.5909642815589904, 'val_loss:', 0.90993844151496883)
('epoch', 167, 'train_loss:', 3.5868127930164335, 'val_loss:', 0.90532941222190855)
('epoch', 168, 'train_loss:', 3.5887093997001647, 'val_loss:', 0.90352669239044192)
('epoch', 169, 'train_loss:', 3.5857356822490694, 'val_loss:', 0.90753404259681703)
('epoch', 170, 'train_loss:', 3.5704901123046877, 'val_loss:', 0.90785764575004579)
('epoch', 171, 'train_loss:', 3.577767881155014, 'val_loss:', 0.90794991731643682)
('epoch', 172, 'train_loss:', 3.5816868948936462, 'val_loss:', 0.89975983619689937)
('epoch', 173, 'train_loss:', 3.5691676688194276, 'val_loss:', 0.90436378359794611)
('epoch', 174, 'train_loss:', 3.5702586758136747, 'val_loss:', 0.90183440923690794)
('epoch', 175, 'train_loss:', 3.5633159697055818, 'val_loss:', 0.90164890170097356)
('epoch', 176, 'train_loss:', 3.5676473534107207, 'val_loss:', 0.90000800728797914)
('epoch', 177, 'train_loss:', 3.5591701948642731, 'val_loss:', 0.89856371164321902)
('epoch', 178, 'train_loss:', 3.5580819284915925, 'val_loss:', 0.89724723815917973)
('epoch', 179, 'train_loss:', 3.5543002080917359, 'val_loss:', 0.90057982444763185)
('epoch', 180, 'train_loss:', 3.5483270597457888, 'val_loss:', 0.89976542115211489)
('epoch', 181, 'train_loss:', 3.5468494355678559, 'val_loss:', 0.90166526675224301)
('epoch', 182, 'train_loss:', 3.5526119613647462, 'val_loss:', 0.90101721525192258)
('epoch', 183, 'train_loss:', 3.5418397343158721, 'val_loss:', 0.89682422161102293)
('epoch', 184, 'train_loss:', 3.5467615747451782, 'val_loss:', 0.89697951316833491)
('epoch', 185, 'train_loss:', 3.5413723373413086, 'val_loss:', 0.89583431005477909)
('epoch', 186, 'train_loss:', 3.5338478887081148, 'val_loss:', 0.89370491266250607)
('epoch', 187, 'train_loss:', 3.5324303710460665, 'val_loss:', 0.8944634330272675)
('epoch', 188, 'train_loss:', 3.5271402871608735, 'val_loss:', 0.89677319407463074)
('epoch', 189, 'train_loss:', 3.5310455954074857, 'val_loss:', 0.89371622323989863)
('epoch', 190, 'train_loss:', 3.5277894532680509, 'val_loss:', 0.89158305764198298)
('epoch', 191, 'train_loss:', 3.5205103898048402, 'val_loss:', 0.89303103327751154)
('epoch', 192, 'train_loss:', 3.5232163310050963, 'val_loss:', 0.89325658917427064)
('epoch', 193, 'train_loss:', 3.5235181152820587, 'val_loss:', 0.89217951536178586)
('epoch', 194, 'train_loss:', 3.5198694849014283, 'val_loss:', 0.89306156277656557)
('epoch', 195, 'train_loss:', 3.5135717821121215, 'val_loss:', 0.88843728423118595)
('epoch', 196, 'train_loss:', 3.5132544028759001, 'val_loss:', 0.89252021431922912)
('epoch', 197, 'train_loss:', 3.510374196767807, 'val_loss:', 0.89461852550506593)
('epoch', 198, 'train_loss:', 3.5140280151367187, 'val_loss:', 0.89201693415641781)
('epoch', 199, 'train_loss:', 3.5040697848796842, 'val_loss:', 0.89389966130256648)
('epoch', 200, 'train_loss:', 3.5023143768310545, 'val_loss:', 0.88712261676788329)
('epoch', 201, 'train_loss:', 3.5061013865470887, 'val_loss:', 0.89065750002861022)
('epoch', 202, 'train_loss:', 3.5041548943519594, 'val_loss:', 0.89052899241447447)
('epoch', 203, 'train_loss:', 3.4866966927051544, 'val_loss:', 0.88987472891807551)
('epoch', 204, 'train_loss:', 3.4919413459300994, 'val_loss:', 0.88884324073791499)
('epoch', 205, 'train_loss:', 3.4880282604694366, 'val_loss:', 0.8899897038936615)
('epoch', 206, 'train_loss:', 3.4989721536636353, 'val_loss:', 0.88736274123191838)
('epoch', 207, 'train_loss:', 3.4890176272392273, 'val_loss:', 0.8871260595321655)
('epoch', 208, 'train_loss:', 3.4956193614006041, 'val_loss:', 0.88555939793586735)
('epoch', 209, 'train_loss:', 3.4806022703647614, 'val_loss:', 0.8854142928123474)
('epoch', 210, 'train_loss:', 3.4802256321907041, 'val_loss:', 0.88408095717430113)
('epoch', 211, 'train_loss:', 3.4806681430339812, 'val_loss:', 0.88704707622528078)
('epoch', 212, 'train_loss:', 3.4773775351047518, 'val_loss:', 0.88623219490051275)
('epoch', 213, 'train_loss:', 3.4758044528961181, 'val_loss:', 0.88473408699035649)
('epoch', 214, 'train_loss:', 3.4704986119270327, 'val_loss:', 0.88478060841560369)
('epoch', 215, 'train_loss:', 3.4714828145504, 'val_loss:', 0.88009102702140807)
('epoch', 216, 'train_loss:', 3.463592941761017, 'val_loss:', 0.88067309617996214)
('epoch', 217, 'train_loss:', 3.4637802469730379, 'val_loss:', 0.88311541676521299)
('epoch', 218, 'train_loss:', 3.4677624833583831, 'val_loss:', 0.88832971453666687)
('epoch', 219, 'train_loss:', 3.4684862411022186, 'val_loss:', 0.88268429636955259)
('epoch', 220, 'train_loss:', 3.4630417859554292, 'val_loss:', 0.88001562237739561)
('epoch', 221, 'train_loss:', 3.4595112669467927, 'val_loss:', 0.88094787240028383)
('epoch', 222, 'train_loss:', 3.4524967396259307, 'val_loss:', 0.88199434399604792)
('epoch', 223, 'train_loss:', 3.4532895135879516, 'val_loss:', 0.87933687925338744)
('epoch', 224, 'train_loss:', 3.458898581266403, 'val_loss:', 0.87975880742073054)
('epoch', 225, 'train_loss:', 3.4538023149967194, 'val_loss:', 0.88023457169532771)
('epoch', 226, 'train_loss:', 3.4467122042179108, 'val_loss:', 0.88019090771675113)
('epoch', 227, 'train_loss:', 3.4483170032501222, 'val_loss:', 0.87923122882843019)
('epoch', 228, 'train_loss:', 3.4500496876239777, 'val_loss:', 0.88158388614654537)
('epoch', 229, 'train_loss:', 3.444939967393875, 'val_loss:', 0.8741496205329895)
('epoch', 230, 'train_loss:', 3.4436729800701142, 'val_loss:', 0.87461724042892453)
('epoch', 231, 'train_loss:', 3.4420542871952056, 'val_loss:', 0.87660789489746094)
('epoch', 232, 'train_loss:', 3.4430310225486753, 'val_loss:', 0.87771719098091128)
('epoch', 233, 'train_loss:', 3.4373311305046084, 'val_loss:', 0.87630373239517212)
('epoch', 234, 'train_loss:', 3.4324733924865725, 'val_loss:', 0.87622612357139584)
('epoch', 235, 'train_loss:', 3.4320726156234742, 'val_loss:', 0.8733459341526032)
('epoch', 236, 'train_loss:', 3.430150067806244, 'val_loss:', 0.87768395543098454)
('epoch', 237, 'train_loss:', 3.4320900523662567, 'val_loss:', 0.8739353406429291)
('epoch', 238, 'train_loss:', 3.4267087554931641, 'val_loss:', 0.87559687852859502)
('epoch', 239, 'train_loss:', 3.426863478422165, 'val_loss:', 0.87726542472839353)
('epoch', 240, 'train_loss:', 3.4297754907608033, 'val_loss:', 0.87473134517669682)
('epoch', 241, 'train_loss:', 3.41775617480278, 'val_loss:', 0.87381929874420161)
('epoch', 242, 'train_loss:', 3.4247388851642611, 'val_loss:', 0.87355318188667297)
('epoch', 243, 'train_loss:', 3.4183893394470215, 'val_loss:', 0.87363883137702947)
('epoch', 244, 'train_loss:', 3.4154608154296877, 'val_loss:', 0.87159941673278807)
('epoch', 245, 'train_loss:', 3.4238603854179384, 'val_loss:', 0.8715977919101715)
('epoch', 246, 'train_loss:', 3.4160865867137908, 'val_loss:', 0.87051084637641907)
('epoch', 247, 'train_loss:', 3.413889173269272, 'val_loss:', 0.87364784955978392)
('epoch', 248, 'train_loss:', 3.4101762282848358, 'val_loss:', 0.86891672968864442)
('epoch', 249, 'train_loss:', 3.410082335472107, 'val_loss:', 0.87059464812278753)
('epoch', 250, 'train_loss:', 3.4064773046970367, 'val_loss:', 0.8666932022571564)
('epoch', 251, 'train_loss:', 3.4089189684391021, 'val_loss:', 0.87253227829933167)
('epoch', 252, 'train_loss:', 3.4087970435619352, 'val_loss:', 0.8696662056446075)
('epoch', 253, 'train_loss:', 3.4059666192531584, 'val_loss:', 0.86737966179847714)
('epoch', 254, 'train_loss:', 3.3972660076618193, 'val_loss:', 0.87052879929542537)
('epoch', 255, 'train_loss:', 3.3971363592147825, 'val_loss:', 0.8701774442195892)
('epoch', 256, 'train_loss:', 3.3974323153495787, 'val_loss:', 0.86580590844154359)
('epoch', 257, 'train_loss:', 3.394792228937149, 'val_loss:', 0.86686810493469235)
('epoch', 258, 'train_loss:', 3.3986102068424224, 'val_loss:', 0.86866505384445192)
('epoch', 259, 'train_loss:', 3.4018026530742644, 'val_loss:', 0.86690165162086485)
('epoch', 260, 'train_loss:', 3.4006475400924683, 'val_loss:', 0.86749716877937322)
('epoch', 261, 'train_loss:', 3.3856625247001646, 'val_loss:', 0.86721211075782778)
('epoch', 262, 'train_loss:', 3.3922824501991271, 'val_loss:', 0.86901214122772219)
('epoch', 263, 'train_loss:', 3.3859845924377443, 'val_loss:', 0.86369885921478273)
('epoch', 264, 'train_loss:', 3.3878613150119783, 'val_loss:', 0.86568730473518374)
('epoch', 265, 'train_loss:', 3.3843088483810426, 'val_loss:', 0.86679106593132016)
('epoch', 266, 'train_loss:', 3.3848649644851685, 'val_loss:', 0.86426547408103938)
('epoch', 267, 'train_loss:', 3.3778888726234437, 'val_loss:', 0.86357195973396306)
('epoch', 268, 'train_loss:', 3.3834736156463623, 'val_loss:', 0.86303317904472354)
('epoch', 269, 'train_loss:', 3.3868407785892485, 'val_loss:', 0.86564592957496644)
('epoch', 270, 'train_loss:', 3.3825451111793519, 'val_loss:', 0.86343277573585508)
('epoch', 271, 'train_loss:', 3.3790199017524718, 'val_loss:', 0.86482025504112248)
('epoch', 272, 'train_loss:', 3.3699403476715086, 'val_loss:', 0.86440665125846861)
('epoch', 273, 'train_loss:', 3.3734446144104004, 'val_loss:', 0.86363313555717469)
('epoch', 274, 'train_loss:', 3.3757117652893065, 'val_loss:', 0.8621528816223144)
('epoch', 275, 'train_loss:', 3.3757919979095461, 'val_loss:', 0.86435297489166263)
('epoch', 276, 'train_loss:', 3.3659934997558594, 'val_loss:', 0.85987552881240847)
('epoch', 277, 'train_loss:', 3.3668558192253113, 'val_loss:', 0.86267451286315922)
('epoch', 278, 'train_loss:', 3.370349098443985, 'val_loss:', 0.86269660353660582)
('epoch', 279, 'train_loss:', 3.3625628733634949, 'val_loss:', 0.86431638002395628)
('epoch', 280, 'train_loss:', 3.3594909131526949, 'val_loss:', 0.864763001203537)
('epoch', 281, 'train_loss:', 3.3588742661476134, 'val_loss:', 0.86246193051338194)
('epoch', 282, 'train_loss:', 3.3580675077438356, 'val_loss:', 0.86174935102462769)
('epoch', 283, 'train_loss:', 3.3542711102962492, 'val_loss:', 0.86105990886688233)
('epoch', 284, 'train_loss:', 3.3505343294143675, 'val_loss:', 0.86278948903083796)
('epoch', 285, 'train_loss:', 3.349860507249832, 'val_loss:', 0.85995721578598028)
('epoch', 286, 'train_loss:', 3.3558977830410002, 'val_loss:', 0.86391261816024778)
('epoch', 287, 'train_loss:', 3.3535241711139681, 'val_loss:', 0.86228605747222897)
('epoch', 288, 'train_loss:', 3.3508547925949097, 'val_loss:', 0.85969914317131046)
('epoch', 289, 'train_loss:', 3.3534088242053985, 'val_loss:', 0.86052023172378544)
('epoch', 290, 'train_loss:', 3.3529836285114287, 'val_loss:', 0.85953797459602355)
('epoch', 291, 'train_loss:', 3.3474335014820098, 'val_loss:', 0.85921215057373046)
('epoch', 292, 'train_loss:', 3.3407558941841127, 'val_loss:', 0.86050059080123897)
('epoch', 293, 'train_loss:', 3.3501807796955108, 'val_loss:', 0.86403237104415898)
('epoch', 294, 'train_loss:', 3.3425732696056367, 'val_loss:', 0.85914912819862366)
('epoch', 295, 'train_loss:', 3.3457067370414735, 'val_loss:', 0.86107898950576778)
('epoch', 296, 'train_loss:', 3.3493467700481414, 'val_loss:', 0.85984166622161862)
('epoch', 297, 'train_loss:', 3.3411982607841493, 'val_loss:', 0.8593774807453155)
('epoch', 298, 'train_loss:', 3.341126185655594, 'val_loss:', 0.85787784099578857)
('epoch', 299, 'train_loss:', 3.3380350327491759, 'val_loss:', 0.85409514069557191)
('epoch', 300, 'train_loss:', 3.3334900331497193, 'val_loss:', 0.8556186437606812)
('epoch', 301, 'train_loss:', 3.3352310371398928, 'val_loss:', 0.85541563630104067)
('epoch', 302, 'train_loss:', 3.3338088691234589, 'val_loss:', 0.85463440179824834)
('epoch', 303, 'train_loss:', 3.334159724712372, 'val_loss:', 0.85857856988906855)
('epoch', 304, 'train_loss:', 3.3339158654212953, 'val_loss:', 0.85743485808372499)
('epoch', 305, 'train_loss:', 3.3307585930824279, 'val_loss:', 0.85493076682090763)
('epoch', 306, 'train_loss:', 3.3290557432174683, 'val_loss:', 0.85450050234794617)
('epoch', 307, 'train_loss:', 3.3323994505405428, 'val_loss:', 0.85642862081527715)
('epoch', 308, 'train_loss:', 3.3246751093864439, 'val_loss:', 0.85432993650436406)
('epoch', 309, 'train_loss:', 3.3304409992694857, 'val_loss:', 0.85273581504821783)
('epoch', 310, 'train_loss:', 3.3244088292121887, 'val_loss:', 0.85659844160079956)
('epoch', 311, 'train_loss:', 3.3271312105655668, 'val_loss:', 0.85657536625862118)
('epoch', 312, 'train_loss:', 3.3203953301906584, 'val_loss:', 0.85429579257965083)
('epoch', 313, 'train_loss:', 3.3190811276435852, 'val_loss:', 0.85515606522560117)
('epoch', 314, 'train_loss:', 3.3164501833915709, 'val_loss:', 0.8551704025268555)
('epoch', 315, 'train_loss:', 3.3281715381145478, 'val_loss:', 0.85476099014282225)
('epoch', 316, 'train_loss:', 3.3126736354827879, 'val_loss:', 0.85458149075508116)
('epoch', 317, 'train_loss:', 3.3175853991508486, 'val_loss:', 0.85797734141349791)
('epoch', 318, 'train_loss:', 3.316046566963196, 'val_loss:', 0.8529399454593658)
('epoch', 319, 'train_loss:', 3.3149770355224608, 'val_loss:', 0.85416478753089908)
('epoch', 320, 'train_loss:', 3.3158077645301818, 'val_loss:', 0.85250236749649044)
('epoch', 321, 'train_loss:', 3.313468713760376, 'val_loss:', 0.85540746212005614)
('epoch', 322, 'train_loss:', 3.3101577651500702, 'val_loss:', 0.85290441274642947)
('epoch', 323, 'train_loss:', 3.3055905675888062, 'val_loss:', 0.85078779578208918)
('epoch', 324, 'train_loss:', 3.3086013889312742, 'val_loss:', 0.85448011636734011)
('epoch', 325, 'train_loss:', 3.3054400634765626, 'val_loss:', 0.85220236897468571)
('epoch', 326, 'train_loss:', 3.3085109472274778, 'val_loss:', 0.85469196081161503)
('epoch', 327, 'train_loss:', 3.3019836390018464, 'val_loss:', 0.84820116281509395)
('epoch', 328, 'train_loss:', 3.3026625370979308, 'val_loss:', 0.84923543691635128)
('epoch', 329, 'train_loss:', 3.2970597398281098, 'val_loss:', 0.85109369039535521)
('epoch', 330, 'train_loss:', 3.3001341485977171, 'val_loss:', 0.85276582002639767)
('epoch', 331, 'train_loss:', 3.3013789582252504, 'val_loss:', 0.84845804095268251)
('epoch', 332, 'train_loss:', 3.2973580002784728, 'val_loss:', 0.84989169955253596)
('epoch', 333, 'train_loss:', 3.2988205766677856, 'val_loss:', 0.85126075387001032)
('epoch', 334, 'train_loss:', 3.2929232597351072, 'val_loss:', 0.8549532604217529)
('epoch', 335, 'train_loss:', 3.2949835836887358, 'val_loss:', 0.84729312419891356)
('epoch', 336, 'train_loss:', 3.2952467131614687, 'val_loss:', 0.85363178610801693)
('epoch', 337, 'train_loss:', 3.2933092510700224, 'val_loss:', 0.8464658677577972)
('epoch', 338, 'train_loss:', 3.296033743619919, 'val_loss:', 0.85038216829299929)
('epoch', 339, 'train_loss:', 3.2952256584167481, 'val_loss:', 0.85102967500686644)
('epoch', 340, 'train_loss:', 3.2864789736270903, 'val_loss:', 0.8504278433322906)
('epoch', 341, 'train_loss:', 3.289682924747467, 'val_loss:', 0.85141117572784419)
('epoch', 342, 'train_loss:', 3.2881684482097624, 'val_loss:', 0.84924008846282961)
('epoch', 343, 'train_loss:', 3.2826962816715239, 'val_loss:', 0.84957828164100646)
('epoch', 344, 'train_loss:', 3.2848575234413149, 'val_loss:', 0.84810091972351076)
('epoch', 345, 'train_loss:', 3.2855933940410615, 'val_loss:', 0.84737275719642635)
('epoch', 346, 'train_loss:', 3.2798969709873198, 'val_loss:', 0.84535233259201048)
('epoch', 347, 'train_loss:', 3.2828298902511595, 'val_loss:', 0.84502896428108221)
('epoch', 348, 'train_loss:', 3.285218348503113, 'val_loss:', 0.84844708323478701)
('epoch', 349, 'train_loss:', 3.2814625155925752, 'val_loss:', 0.85005972266197205)
('epoch', 350, 'train_loss:', 3.2794458603858949, 'val_loss:', 0.84664035797119142)
('epoch', 351, 'train_loss:', 3.277214925289154, 'val_loss:', 0.84915205597877508)
('epoch', 352, 'train_loss:', 3.2797268986701966, 'val_loss:', 0.84818616151809689)
('epoch', 353, 'train_loss:', 3.2798868882656098, 'val_loss:', 0.85002192258834841)
('epoch', 354, 'train_loss:', 3.2723214173316957, 'val_loss:', 0.84672718644142153)
('epoch', 355, 'train_loss:', 3.2758439552783964, 'val_loss:', 0.84658956527709961)
('epoch', 356, 'train_loss:', 3.2682852780818941, 'val_loss:', 0.84416774749755863)
('epoch', 357, 'train_loss:', 3.2715476965904235, 'val_loss:', 0.84647731423377992)
('epoch', 358, 'train_loss:', 3.2721812915802002, 'val_loss:', 0.84874455213546751)
('epoch', 359, 'train_loss:', 3.2640217602252961, 'val_loss:', 0.84254160404205325)
('epoch', 360, 'train_loss:', 3.269898988008499, 'val_loss:', 0.84844977140426636)
('epoch', 361, 'train_loss:', 3.2687761139869691, 'val_loss:', 0.84420556664466861)
('epoch', 362, 'train_loss:', 3.2661813366413117, 'val_loss:', 0.84440312623977665)
('epoch', 363, 'train_loss:', 3.263587851524353, 'val_loss:', 0.84486935734748836)
('epoch', 364, 'train_loss:', 3.257548863887787, 'val_loss:', 0.84773011922836305)
('epoch', 365, 'train_loss:', 3.2621686792373659, 'val_loss:', 0.84532701015472411)
('epoch', 366, 'train_loss:', 3.2617634487152101, 'val_loss:', 0.84254246950149536)
('epoch', 367, 'train_loss:', 3.2632815170288088, 'val_loss:', 0.84519122004508973)
('epoch', 368, 'train_loss:', 3.2655303335189818, 'val_loss:', 0.8464696753025055)
('epoch', 369, 'train_loss:', 3.2617570638656614, 'val_loss:', 0.84880194425582889)
('epoch', 370, 'train_loss:', 3.2624452483654021, 'val_loss:', 0.84392837524414066)
('epoch', 371, 'train_loss:', 3.2625400960445403, 'val_loss:', 0.84587124586105344)
('epoch', 372, 'train_loss:', 3.2537535929679873, 'val_loss:', 0.84304403543472295)
('epoch', 373, 'train_loss:', 3.2564852166175844, 'val_loss:', 0.84359344959259031)
('epoch', 374, 'train_loss:', 3.2550226449966431, 'val_loss:', 0.84142273306846616)
('epoch', 375, 'train_loss:', 3.2548088777065276, 'val_loss:', 0.84408406615257259)
('epoch', 376, 'train_loss:', 3.2527580392360687, 'val_loss:', 0.84814522504806522)
('epoch', 377, 'train_loss:', 3.2585857677459718, 'val_loss:', 0.84198154449462892)
('epoch', 378, 'train_loss:', 3.2473310887813569, 'val_loss:', 0.84435493469238276)
('epoch', 379, 'train_loss:', 3.2511588132381437, 'val_loss:', 0.84457975387573248)
('epoch', 380, 'train_loss:', 3.2482887685298918, 'val_loss:', 0.84253061175346378)
('epoch', 381, 'train_loss:', 3.2486306178569793, 'val_loss:', 0.84362033843994144)
('epoch', 382, 'train_loss:', 3.2530846548080445, 'val_loss:', 0.84339751124382023)
('epoch', 383, 'train_loss:', 3.2497300350666047, 'val_loss:', 0.84075920820236205)
('epoch', 384, 'train_loss:', 3.2431367874145507, 'val_loss:', 0.84024738550186162)
('epoch', 385, 'train_loss:', 3.2464850151538851, 'val_loss:', 0.84176447629928586)
('epoch', 386, 'train_loss:', 3.2471006155014037, 'val_loss:', 0.84187523961067201)
('epoch', 387, 'train_loss:', 3.2495126295089722, 'val_loss:', 0.84075446844100954)
('epoch', 388, 'train_loss:', 3.2451764345169067, 'val_loss:', 0.83934793949127195)
('epoch', 389, 'train_loss:', 3.2455720710754394, 'val_loss:', 0.84288363099098207)
('epoch', 390, 'train_loss:', 3.2383567512035372, 'val_loss:', 0.84239045858383177)
('epoch', 391, 'train_loss:', 3.2411552357673643, 'val_loss:', 0.8412648046016693)
('epoch', 392, 'train_loss:', 3.2438457262516023, 'val_loss:', 0.84246510982513423)
('epoch', 393, 'train_loss:', 3.2445366013050081, 'val_loss:', 0.84108789086341862)
('epoch', 394, 'train_loss:', 3.2342180621623995, 'val_loss:', 0.84037237882614135)
('epoch', 395, 'train_loss:', 3.2401972818374634, 'val_loss:', 0.84072779655456542)
('epoch', 396, 'train_loss:', 3.2325940203666685, 'val_loss:', 0.84143305659294132)
('epoch', 397, 'train_loss:', 3.234194449186325, 'val_loss:', 0.84084713459014893)
('epoch', 398, 'train_loss:', 3.2349960660934447, 'val_loss:', 0.83993176579475404)
('epoch', 399, 'train_loss:', 3.2343724226951598, 'val_loss:', 0.83826582074165346)
('epoch', 400, 'train_loss:', 3.2349047625064848, 'val_loss:', 0.83819720864295955)
('epoch', 401, 'train_loss:', 3.2278165137767791, 'val_loss:', 0.84036113262176515)
('epoch', 402, 'train_loss:', 3.2298122715950011, 'val_loss:', 0.83828897476196285)
('epoch', 403, 'train_loss:', 3.227959842681885, 'val_loss:', 0.83902555823326108)
('epoch', 404, 'train_loss:', 3.2258765864372254, 'val_loss:', 0.83941278100013728)
('epoch', 405, 'train_loss:', 3.2234639942646028, 'val_loss:', 0.83937390804290768)
('epoch', 406, 'train_loss:', 3.2274700343608855, 'val_loss:', 0.84033062219619747)
('epoch', 407, 'train_loss:', 3.2216018187999724, 'val_loss:', 0.83915367245674133)
('epoch', 408, 'train_loss:', 3.2309936916828157, 'val_loss:', 0.8378162014484406)
('epoch', 409, 'train_loss:', 3.2248587369918824, 'val_loss:', 0.84058498501777645)
('epoch', 410, 'train_loss:', 3.2224025118350981, 'val_loss:', 0.83583225846290587)
('epoch', 411, 'train_loss:', 3.2186178517341615, 'val_loss:', 0.84072331666946409)
('epoch', 412, 'train_loss:', 3.2153602266311645, 'val_loss:', 0.83700974225997926)
('epoch', 413, 'train_loss:', 3.2275562834739686, 'val_loss:', 0.8362177062034607)
('epoch', 414, 'train_loss:', 3.2209691870212556, 'val_loss:', 0.83817687511444094)
('epoch', 415, 'train_loss:', 3.2190920066833497, 'val_loss:', 0.84018921852111816)
('epoch', 416, 'train_loss:', 3.2200655078887941, 'val_loss:', 0.83513936042785641)
('epoch', 417, 'train_loss:', 3.2128255915641786, 'val_loss:', 0.8385599827766419)
('epoch', 418, 'train_loss:', 3.2179471576213836, 'val_loss:', 0.83752601623535161)
('epoch', 419, 'train_loss:', 3.2169206273555755, 'val_loss:', 0.83789687395095824)
('epoch', 420, 'train_loss:', 3.2173224771022797, 'val_loss:', 0.83976744890213018)
('epoch', 421, 'train_loss:', 3.2130407011508941, 'val_loss:', 0.83546201229095463)
('epoch', 422, 'train_loss:', 3.2106667959690096, 'val_loss:', 0.83709654331207273)
('epoch', 423, 'train_loss:', 3.2123510384559633, 'val_loss:', 0.83846942782402034)
('epoch', 424, 'train_loss:', 3.2097012531757354, 'val_loss:', 0.8370092678070068)
('epoch', 425, 'train_loss:', 3.2059724640846254, 'val_loss:', 0.83701637387275696)
('epoch', 426, 'train_loss:', 3.2067531681060792, 'val_loss:', 0.83405371427536013)
('epoch', 427, 'train_loss:', 3.2079622554779053, 'val_loss:', 0.83429118752479559)
('epoch', 428, 'train_loss:', 3.2069552266597747, 'val_loss:', 0.83872626185417176)
('epoch', 429, 'train_loss:', 3.2102498495578766, 'val_loss:', 0.83709232807159428)
('epoch', 430, 'train_loss:', 3.2044517052173616, 'val_loss:', 0.83998449563980104)
('epoch', 431, 'train_loss:', 3.2060656428337095, 'val_loss:', 0.83716548442840577)
('epoch', 432, 'train_loss:', 3.2092713189125059, 'val_loss:', 0.83820753931999203)
('epoch', 433, 'train_loss:', 3.2031137025356293, 'val_loss:', 0.83847171902656559)
('epoch', 434, 'train_loss:', 3.2019649410247801, 'val_loss:', 0.83789022088050846)
('epoch', 435, 'train_loss:', 3.2009977352619172, 'val_loss:', 0.83762401342391968)
('epoch', 436, 'train_loss:', 3.2004584133625031, 'val_loss:', 0.83649627089500422)
('epoch', 437, 'train_loss:', 3.2040930712223052, 'val_loss:', 0.83919919371604923)
('epoch', 438, 'train_loss:', 3.1954095804691316, 'val_loss:', 0.83754848361015322)
('epoch', 439, 'train_loss:', 3.2006112694740296, 'val_loss:', 0.83616948962211612)
('epoch', 440, 'train_loss:', 3.2040194332599641, 'val_loss:', 0.83814076662063597)
('epoch', 441, 'train_loss:', 3.1948267889022826, 'val_loss:', 0.83466933250427244)
('epoch', 442, 'train_loss:', 3.2005902767181396, 'val_loss:', 0.83607204437255855)
('epoch', 443, 'train_loss:', 3.1971947097778322, 'val_loss:', 0.83590021252632141)
('epoch', 444, 'train_loss:', 3.1934795498847963, 'val_loss:', 0.83374468803405766)
('epoch', 445, 'train_loss:', 3.1984326946735382, 'val_loss:', 0.83989838480949397)
('epoch', 446, 'train_loss:', 3.1921278464794161, 'val_loss:', 0.83085049390792842)
('epoch', 447, 'train_loss:', 3.1925962555408476, 'val_loss:', 0.83239308595657346)
('epoch', 448, 'train_loss:', 3.1938377499580382, 'val_loss:', 0.83660671710968015)
('epoch', 449, 'train_loss:', 3.1900274419784544, 'val_loss:', 0.83745909214019776)
('epoch', 450, 'train_loss:', 3.1906699132919312, 'val_loss:', 0.83669126272201533)
('epoch', 451, 'train_loss:', 3.187977476119995, 'val_loss:', 0.83340092778205876)
('epoch', 452, 'train_loss:', 3.186980081796646, 'val_loss:', 0.8338964915275574)
('epoch', 453, 'train_loss:', 3.1876112926006317, 'val_loss:', 0.8385919642448425)
('epoch', 454, 'train_loss:', 3.1887685573101043, 'val_loss:', 0.84222754120826726)
('epoch', 455, 'train_loss:', 3.1782472825050352, 'val_loss:', 0.83254078745841975)
('epoch', 456, 'train_loss:', 3.1873247504234312, 'val_loss:', 0.83432664871215823)
('epoch', 457, 'train_loss:', 3.1871721971035005, 'val_loss:', 0.83516877174377446)
('epoch', 458, 'train_loss:', 3.1822086536884306, 'val_loss:', 0.83602092862129207)
('epoch', 459, 'train_loss:', 3.1833152544498442, 'val_loss:', 0.83226911425590511)
('epoch', 460, 'train_loss:', 3.182774579524994, 'val_loss:', 0.83174671649932863)
('epoch', 461, 'train_loss:', 3.183142064809799, 'val_loss:', 0.83548204898834233)
('epoch', 462, 'train_loss:', 3.1830994892120361, 'val_loss:', 0.83243258595466618)
('epoch', 463, 'train_loss:', 3.1789492356777189, 'val_loss:', 0.8309678041934967)
('epoch', 464, 'train_loss:', 3.1763763439655306, 'val_loss:', 0.83289870142936706)
('epoch', 465, 'train_loss:', 3.1780263161659241, 'val_loss:', 0.8313922679424286)
('epoch', 466, 'train_loss:', 3.1793525362014772, 'val_loss:', 0.83343876004219053)
('epoch', 467, 'train_loss:', 3.1766543078422544, 'val_loss:', 0.83107827305793758)
('epoch', 468, 'train_loss:', 3.1763399708271027, 'val_loss:', 0.83066183209419253)
('epoch', 469, 'train_loss:', 3.1747447443008423, 'val_loss:', 0.8326403546333313)
('epoch', 470, 'train_loss:', 3.1707101058959961, 'val_loss:', 0.83050909519195559)
('epoch', 471, 'train_loss:', 3.1776819372177125, 'val_loss:', 0.83449669003486637)
('epoch', 472, 'train_loss:', 3.1689950811862944, 'val_loss:', 0.83434241652488705)
('epoch', 473, 'train_loss:', 3.1738320803642273, 'val_loss:', 0.83131844043731684)
('epoch', 474, 'train_loss:', 3.1671785604953766, 'val_loss:', 0.83508994460105901)
('epoch', 475, 'train_loss:', 3.1753932094573973, 'val_loss:', 0.83105024337768552)
('epoch', 476, 'train_loss:', 3.1689864015579223, 'val_loss:', 0.83235838055610656)
('epoch', 477, 'train_loss:', 3.1713019907474518, 'val_loss:', 0.83105931043624881)
('epoch', 478, 'train_loss:', 3.1708103346824648, 'val_loss:', 0.82898528456687925)
('epoch', 479, 'train_loss:', 3.1667416274547575, 'val_loss:', 0.83256420969963074)
('epoch', 480, 'train_loss:', 3.163595675230026, 'val_loss:', 0.83060362219810491)
('epoch', 481, 'train_loss:', 3.1638916504383086, 'val_loss:', 0.83100655794143674)
('epoch', 482, 'train_loss:', 3.1678017461299897, 'val_loss:', 0.82939767956733701)
('epoch', 483, 'train_loss:', 3.1663061428070067, 'val_loss:', 0.83275782227516171)
('epoch', 484, 'train_loss:', 3.1642140352725985, 'val_loss:', 0.83181340694427486)
('epoch', 485, 'train_loss:', 3.1648086965084077, 'val_loss:', 0.83065462231636045)
('epoch', 486, 'train_loss:', 3.164517002105713, 'val_loss:', 0.83068833351135252)
('epoch', 487, 'train_loss:', 3.1628808951377869, 'val_loss:', 0.83201551675796503)
('epoch', 488, 'train_loss:', 3.1603736007213592, 'val_loss:', 0.83129348278045656)
('epoch', 489, 'train_loss:', 3.1621386802196501, 'val_loss:', 0.82902262210845945)
('epoch', 490, 'train_loss:', 3.1585078704357148, 'val_loss:', 0.83227639436721801)
('epoch', 491, 'train_loss:', 3.1593811500072477, 'val_loss:', 0.82997505664825444)
('epoch', 492, 'train_loss:', 3.1623576724529268, 'val_loss:', 0.83063612103462214)
('epoch', 493, 'train_loss:', 3.1603769743442536, 'val_loss:', 0.83210503101348876)
('epoch', 494, 'train_loss:', 3.1590428042411802, 'val_loss:', 0.82834885001182557)
('epoch', 495, 'train_loss:', 3.1529167664051054, 'val_loss:', 0.83256292581558222)
('epoch', 496, 'train_loss:', 3.1599110889434816, 'val_loss:', 0.83024977326393123)
('epoch', 497, 'train_loss:', 3.154531066417694, 'val_loss:', 0.83142184495925908)
('epoch', 498, 'train_loss:', 3.1545840132236482, 'val_loss:', 0.83027533292770384)
('epoch', 499, 'train_loss:', 3.1522150504589082, 'val_loss:', 0.82696552753448482)
('epoch', 500, 'train_loss:', 3.1541270411014555, 'val_loss:', 0.82879519581794736)
('epoch', 501, 'train_loss:', 3.1526037859916687, 'val_loss:', 0.82866336107254024)
('epoch', 502, 'train_loss:', 3.1511636209487914, 'val_loss:', 0.83129285931587216)
('epoch', 503, 'train_loss:', 3.1481969523429871, 'val_loss:', 0.83063934206962586)
('epoch', 504, 'train_loss:', 3.14738955616951, 'val_loss:', 0.82960905432701115)
('epoch', 505, 'train_loss:', 3.1525905883312224, 'val_loss:', 0.83132536292076109)
('epoch', 506, 'train_loss:', 3.1491596937179565, 'val_loss:', 0.82616899132728572)
('epoch', 507, 'train_loss:', 3.1454019963741304, 'val_loss:', 0.8290933454036713)
('epoch', 508, 'train_loss:', 3.1483382749557496, 'val_loss:', 0.8275550818443298)
('epoch', 509, 'train_loss:', 3.1493965733051299, 'val_loss:', 0.82921437978744506)
('epoch', 510, 'train_loss:', 3.1528886473178863, 'val_loss:', 0.82604509472846988)
('epoch', 511, 'train_loss:', 3.1449255549907686, 'val_loss:', 0.8256784224510193)
('epoch', 512, 'train_loss:', 3.1421216893196107, 'val_loss:', 0.82784799337387083)
('epoch', 513, 'train_loss:', 3.1487787091732025, 'val_loss:', 0.83242764711380002)
('epoch', 514, 'train_loss:', 3.1453546929359435, 'val_loss:', 0.83168659090995789)
('epoch', 515, 'train_loss:', 3.1368875837326051, 'val_loss:', 0.83184931039810184)
('epoch', 516, 'train_loss:', 3.1410869133472441, 'val_loss:', 0.82691349267959591)
('epoch', 517, 'train_loss:', 3.1403343069553373, 'val_loss:', 0.82915982246398923)
('epoch', 518, 'train_loss:', 3.1407171964645384, 'val_loss:', 0.83014102697372438)
('epoch', 519, 'train_loss:', 3.1454529547691346, 'val_loss:', 0.83207423448562623)
('epoch', 520, 'train_loss:', 3.1432411599159242, 'val_loss:', 0.82725639343261714)
('epoch', 521, 'train_loss:', 3.1420081877708435, 'val_loss:', 0.8280776131153107)
('epoch', 522, 'train_loss:', 3.1331865978240967, 'val_loss:', 0.82662179112434386)
('epoch', 523, 'train_loss:', 3.1425261664390565, 'val_loss:', 0.83203888416290284)
('epoch', 524, 'train_loss:', 3.1404821228981019, 'val_loss:', 0.83127481460571284)
('epoch', 525, 'train_loss:', 3.1362147998809813, 'val_loss:', 0.83136037707328792)
('epoch', 526, 'train_loss:', 3.1384027171134949, 'val_loss:', 0.82928054332733159)
('epoch', 527, 'train_loss:', 3.1376887488365175, 'val_loss:', 0.82934664011001591)
('epoch', 528, 'train_loss:', 3.1361934411525727, 'val_loss:', 0.82543699979782104)
('epoch', 529, 'train_loss:', 3.1327884304523468, 'val_loss:', 0.82699046611785887)
('epoch', 530, 'train_loss:', 3.1309707593917846, 'val_loss:', 0.82740369558334348)
('epoch', 531, 'train_loss:', 3.1346555256843569, 'val_loss:', 0.82797903776168824)
('epoch', 532, 'train_loss:', 3.1361967325210571, 'val_loss:', 0.8249361765384674)
('epoch', 533, 'train_loss:', 3.1359115171432497, 'val_loss:', 0.82857922077178958)
('epoch', 534, 'train_loss:', 3.1295271551609041, 'val_loss:', 0.82522873044013978)
('epoch', 535, 'train_loss:', 3.1347245657444001, 'val_loss:', 0.82634330034255976)
('epoch', 536, 'train_loss:', 3.128737071752548, 'val_loss:', 0.82973483681678772)
('epoch', 537, 'train_loss:', 3.1322655093669893, 'val_loss:', 0.82611994981765746)
('epoch', 538, 'train_loss:', 3.1334230625629425, 'val_loss:', 0.83136140108108525)
('epoch', 539, 'train_loss:', 3.127295800447464, 'val_loss:', 0.82715555906295779)
('epoch', 540, 'train_loss:', 3.1234197068214415, 'val_loss:', 0.82828299045562748)
('epoch', 541, 'train_loss:', 3.1268719172477724, 'val_loss:', 0.8247158133983612)
('epoch', 542, 'train_loss:', 3.1225147700309752, 'val_loss:', 0.8246353185176849)
('epoch', 543, 'train_loss:', 3.1280803620815276, 'val_loss:', 0.82680794715881345)
('epoch', 544, 'train_loss:', 3.1238242614269258, 'val_loss:', 0.82588137745857237)
('epoch', 545, 'train_loss:', 3.1275511097908022, 'val_loss:', 0.8293046462535858)
('epoch', 546, 'train_loss:', 3.1237705516815186, 'val_loss:', 0.82438600540161133)
('epoch', 547, 'train_loss:', 3.1216707122325897, 'val_loss:', 0.82527006626129151)
('epoch', 548, 'train_loss:', 3.1195924043655396, 'val_loss:', 0.82905115604400637)
('epoch', 549, 'train_loss:', 3.1233695602416991, 'val_loss:', 0.82658859848976141)
('epoch', 550, 'train_loss:', 3.1210155963897703, 'val_loss:', 0.8282452082633972)
('epoch', 551, 'train_loss:', 3.1197007024288177, 'val_loss:', 0.8264467132091522)
('epoch', 552, 'train_loss:', 3.1157243347167967, 'val_loss:', 0.82569724917411802)
('epoch', 553, 'train_loss:', 3.1171475875377657, 'val_loss:', 0.82559058666229246)
('epoch', 554, 'train_loss:', 3.1131561183929444, 'val_loss:', 0.82686059355735775)
('epoch', 555, 'train_loss:', 3.118194156885147, 'val_loss:', 0.82535219430923457)
('epoch', 556, 'train_loss:', 3.1195381021499635, 'val_loss:', 0.82347602963447575)
('epoch', 557, 'train_loss:', 3.1156480479240418, 'val_loss:', 0.82650028705596923)
('epoch', 558, 'train_loss:', 3.1140325570106508, 'val_loss:', 0.82771139621734624)
('epoch', 559, 'train_loss:', 3.1161915957927704, 'val_loss:', 0.8263549554347992)
('epoch', 560, 'train_loss:', 3.1178218269348146, 'val_loss:', 0.82189147472381596)
('epoch', 561, 'train_loss:', 3.1096819949150087, 'val_loss:', 0.82767644286155706)
('epoch', 562, 'train_loss:', 3.1157423996925355, 'val_loss:', 0.82445409774780276)
('epoch', 563, 'train_loss:', 3.1130402719974519, 'val_loss:', 0.8233563375473022)
('epoch', 564, 'train_loss:', 3.109355241060257, 'val_loss:', 0.82623944520950321)
('epoch', 565, 'train_loss:', 3.111234529018402, 'val_loss:', 0.82617708683013913)
('epoch', 566, 'train_loss:', 3.1157369375228883, 'val_loss:', 0.82403867602348324)
('epoch', 567, 'train_loss:', 3.1089200365543364, 'val_loss:', 0.82555420041084293)
('epoch', 568, 'train_loss:', 3.1108481037616729, 'val_loss:', 0.82413725018501283)
('epoch', 569, 'train_loss:', 3.1089523077011108, 'val_loss:', 0.8245387828350067)
('epoch', 570, 'train_loss:', 3.1096444559097289, 'val_loss:', 0.82462296247482303)
('epoch', 571, 'train_loss:', 3.1081680548191071, 'val_loss:', 0.82675511121749878)
('epoch', 572, 'train_loss:', 3.1047936344146727, 'val_loss:', 0.82480306386947633)
('epoch', 573, 'train_loss:', 3.108174809217453, 'val_loss:', 0.82172316908836363)
('epoch', 574, 'train_loss:', 3.1021957147121428, 'val_loss:', 0.82639948248863215)
('epoch', 575, 'train_loss:', 3.1026836109161375, 'val_loss:', 0.82661646485328677)
('epoch', 576, 'train_loss:', 3.1046587979793547, 'val_loss:', 0.82461608290672306)
('epoch', 577, 'train_loss:', 3.1043212497234345, 'val_loss:', 0.82080922603607176)
('epoch', 578, 'train_loss:', 3.1037062239646911, 'val_loss:', 0.82454910159111028)
('epoch', 579, 'train_loss:', 3.1026679992675783, 'val_loss:', 0.82377161622047423)
('epoch', 580, 'train_loss:', 3.1038989400863648, 'val_loss:', 0.82344618558883664)
('epoch', 581, 'train_loss:', 3.0979632985591889, 'val_loss:', 0.82337082505226133)
('epoch', 582, 'train_loss:', 3.0993452811241151, 'val_loss:', 0.82288388371467591)
('epoch', 583, 'train_loss:', 3.0977009022235871, 'val_loss:', 0.82278468370437619)
('epoch', 584, 'train_loss:', 3.0998523449897766, 'val_loss:', 0.82589019417762755)
('epoch', 585, 'train_loss:', 3.0976510334014891, 'val_loss:', 0.82637857317924501)
('epoch', 586, 'train_loss:', 3.0962981879711151, 'val_loss:', 0.822998046875)
('epoch', 587, 'train_loss:', 3.0941796994209287, 'val_loss:', 0.8272889506816864)
('epoch', 588, 'train_loss:', 3.0972413706779478, 'val_loss:', 0.82356225848197939)
('epoch', 589, 'train_loss:', 3.0931542277336121, 'val_loss:', 0.82176376461982725)
('epoch', 590, 'train_loss:', 3.0989984142780305, 'val_loss:', 0.82295721888542173)
('epoch', 591, 'train_loss:', 3.0963826990127563, 'val_loss:', 0.82498120665550234)
('epoch', 592, 'train_loss:', 3.0996969413757323, 'val_loss:', 0.82220244050025937)
('epoch', 593, 'train_loss:', 3.1015837061405183, 'val_loss:', 0.82207193493843078)
('epoch', 594, 'train_loss:', 3.0950830924510955, 'val_loss:', 0.82224233984947204)
('epoch', 595, 'train_loss:', 3.0904760003089904, 'val_loss:', 0.82483903646469114)
('epoch', 596, 'train_loss:', 3.0922989273071289, 'val_loss:', 0.82290346145629878)
('epoch', 597, 'train_loss:', 3.0932974684238435, 'val_loss:', 0.8222653448581696)
('epoch', 598, 'train_loss:', 3.090539560317993, 'val_loss:', 0.82401356339454646)
('epoch', 599, 'train_loss:', 3.0921273910999298, 'val_loss:', 0.82575972795486452)

