(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=0 python lstm_all_variants.py -t -d paulg -v normal
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'paulg', 'train': True, 'variant': 'normal', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2280 get requests, put_count=2268 evicted_count=1000 eviction_rate=0.440917 and unsatisfied allocation rate=0.487719
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3416 get requests, put_count=3394 evicted_count=1000 eviction_rate=0.294638 and unsatisfied allocation rate=0.305913
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
('epoch', 0, 'train_loss:', 8.5415132308006285, 'val_loss:', 1.9096146464347838)
('epoch', 1, 'train_loss:', 7.2821996283531192, 'val_loss:', 1.7226690673828124)
('epoch', 2, 'train_loss:', 6.6824462795257569, 'val_loss:', 1.6373434996604919)
('epoch', 3, 'train_loss:', 6.4136382246017458, 'val_loss:', 1.5846223306655884)
('epoch', 4, 'train_loss:', 6.22801778793335, 'val_loss:', 1.529003508090973)
('epoch', 5, 'train_loss:', 6.0385672807693478, 'val_loss:', 1.4868950104713441)
('epoch', 6, 'train_loss:', 5.8351127958297733, 'val_loss:', 1.4358118629455567)
('epoch', 7, 'train_loss:', 5.6394424247741703, 'val_loss:', 1.3838497042655944)
('epoch', 8, 'train_loss:', 5.462232468128204, 'val_loss:', 1.3441567516326904)
('epoch', 9, 'train_loss:', 5.3095278286933896, 'val_loss:', 1.3153356289863587)
('epoch', 10, 'train_loss:', 5.1824510002136233, 'val_loss:', 1.2801622366905212)
('epoch', 11, 'train_loss:', 5.052001891136169, 'val_loss:', 1.2486281645298005)
('epoch', 12, 'train_loss:', 4.94180121421814, 'val_loss:', 1.2294123649597168)
('epoch', 13, 'train_loss:', 4.8403500950336458, 'val_loss:', 1.2011174750328064)
('epoch', 14, 'train_loss:', 4.7403982686996464, 'val_loss:', 1.169990929365158)
('epoch', 15, 'train_loss:', 4.6612580263614651, 'val_loss:', 1.1549830484390258)
('epoch', 16, 'train_loss:', 4.5764374291896823, 'val_loss:', 1.1304600441455841)
('epoch', 17, 'train_loss:', 4.5005621755123135, 'val_loss:', 1.1127638363838195)
('epoch', 18, 'train_loss:', 4.4410457992553711, 'val_loss:', 1.0995994186401368)
('epoch', 19, 'train_loss:', 4.380130853652954, 'val_loss:', 1.0841857004165649)
('epoch', 20, 'train_loss:', 4.3148128950595854, 'val_loss:', 1.0753244829177857)
('epoch', 21, 'train_loss:', 4.2688001835346219, 'val_loss:', 1.0580998051166535)
('epoch', 22, 'train_loss:', 4.2173422026634215, 'val_loss:', 1.0527281618118287)
('epoch', 23, 'train_loss:', 4.1704291832447051, 'val_loss:', 1.0415003967285157)
('epoch', 24, 'train_loss:', 4.132127403020859, 'val_loss:', 1.0303695809841156)
('epoch', 25, 'train_loss:', 4.0941356015205379, 'val_loss:', 1.0265840494632721)
('epoch', 26, 'train_loss:', 4.0492520022392275, 'val_loss:', 1.0161689269542693)
('epoch', 27, 'train_loss:', 4.025846112966537, 'val_loss:', 1.0054398024082183)
('epoch', 28, 'train_loss:', 3.9822468841075898, 'val_loss:', 0.99669055461883549)
('epoch', 29, 'train_loss:', 3.955240662097931, 'val_loss:', 0.99293521523475647)
('epoch', 30, 'train_loss:', 3.9303374874591825, 'val_loss:', 0.9837312483787537)
('epoch', 31, 'train_loss:', 3.8960868954658507, 'val_loss:', 0.98009251236915584)
('epoch', 32, 'train_loss:', 3.8743103492259978, 'val_loss:', 0.97093091726303105)
('epoch', 33, 'train_loss:', 3.8525269854068758, 'val_loss:', 0.96532193660736088)
('epoch', 34, 'train_loss:', 3.8294257175922395, 'val_loss:', 0.96945672273635863)
('epoch', 35, 'train_loss:', 3.8117454874515535, 'val_loss:', 0.95966739296913151)
('epoch', 36, 'train_loss:', 3.7917113089561463, 'val_loss:', 0.95531772255897518)
('epoch', 37, 'train_loss:', 3.7701362347602845, 'val_loss:', 0.94921154379844663)
('epoch', 38, 'train_loss:', 3.753266727924347, 'val_loss:', 0.94274105310440059)
('epoch', 39, 'train_loss:', 3.7380120694637298, 'val_loss:', 0.94376462936401362)
('epoch', 40, 'train_loss:', 3.7178208696842194, 'val_loss:', 0.9405959022045135)
('epoch', 41, 'train_loss:', 3.6968644607067107, 'val_loss:', 0.93630083203315739)
('epoch', 42, 'train_loss:', 3.6897546291351317, 'val_loss:', 0.92900284647941589)
('epoch', 43, 'train_loss:', 3.6705728352069853, 'val_loss:', 0.92513539552688595)
('epoch', 44, 'train_loss:', 3.6576051855087282, 'val_loss:', 0.92619570136070251)
('epoch', 45, 'train_loss:', 3.645479451417923, 'val_loss:', 0.921730637550354)
('epoch', 46, 'train_loss:', 3.6327803122997282, 'val_loss:', 0.91416346907615664)
('epoch', 47, 'train_loss:', 3.619670864343643, 'val_loss:', 0.91564119338989258)
('epoch', 48, 'train_loss:', 3.6037431597709655, 'val_loss:', 0.91271327018737791)
('epoch', 49, 'train_loss:', 3.5909411907196045, 'val_loss:', 0.91434582591056823)
('epoch', 50, 'train_loss:', 3.5832179808616638, 'val_loss:', 0.90785272240638737)
('epoch', 51, 'train_loss:', 3.5719464874267577, 'val_loss:', 0.90551592588424679)
('epoch', 52, 'train_loss:', 3.562215129137039, 'val_loss:', 0.90508781671524052)
('epoch', 53, 'train_loss:', 3.5487220108509065, 'val_loss:', 0.90678979039192198)
('epoch', 54, 'train_loss:', 3.5362709438800812, 'val_loss:', 0.90068701386451722)
('epoch', 55, 'train_loss:', 3.5338175535202025, 'val_loss:', 0.90200135827064509)
('epoch', 56, 'train_loss:', 3.5144392561912539, 'val_loss:', 0.89818610668182375)
('epoch', 57, 'train_loss:', 3.5097519099712371, 'val_loss:', 0.89423722505569458)
('epoch', 58, 'train_loss:', 3.5051485311985018, 'val_loss:', 0.89588547348976133)
('epoch', 59, 'train_loss:', 3.4913668537139895, 'val_loss:', 0.8926503312587738)
('epoch', 60, 'train_loss:', 3.4844390726089478, 'val_loss:', 0.88981475591659542)
('epoch', 61, 'train_loss:', 3.4803179299831388, 'val_loss:', 0.89077587723731999)
('epoch', 62, 'train_loss:', 3.4672855269908904, 'val_loss:', 0.88144853711128235)
('epoch', 63, 'train_loss:', 3.4608237850666046, 'val_loss:', 0.88694483995437623)
('epoch', 64, 'train_loss:', 3.4544395089149473, 'val_loss:', 0.88505876898765568)
('epoch', 65, 'train_loss:', 3.445350309610367, 'val_loss:', 0.88454414725303654)
('epoch', 66, 'train_loss:', 3.4388415133953094, 'val_loss:', 0.88304014801979069)
('epoch', 67, 'train_loss:', 3.4354631686210633, 'val_loss:', 0.88315875053405757)
('epoch', 68, 'train_loss:', 3.4269763755798341, 'val_loss:', 0.87752813816070552)
('epoch', 69, 'train_loss:', 3.4191498613357543, 'val_loss:', 0.87780745029449458)
('epoch', 70, 'train_loss:', 3.408264273405075, 'val_loss:', 0.87336675643920902)
('epoch', 71, 'train_loss:', 3.4044897818565367, 'val_loss:', 0.87187309503555299)
('epoch', 72, 'train_loss:', 3.3976839423179626, 'val_loss:', 0.87212653636932369)
('epoch', 73, 'train_loss:', 3.3926393592357638, 'val_loss:', 0.87667689800262449)
('epoch', 74, 'train_loss:', 3.3926875329017641, 'val_loss:', 0.87334310054779052)
('epoch', 75, 'train_loss:', 3.3796087515354158, 'val_loss:', 0.86815008163452145)
('epoch', 76, 'train_loss:', 3.3764785587787629, 'val_loss:', 0.87356061697006226)
('epoch', 77, 'train_loss:', 3.3711345064640046, 'val_loss:', 0.87093083381652836)
('epoch', 78, 'train_loss:', 3.364028126001358, 'val_loss:', 0.87161792993545528)
('epoch', 79, 'train_loss:', 3.3606052792072294, 'val_loss:', 0.86787549734115599)
('epoch', 80, 'train_loss:', 3.3535596990585326, 'val_loss:', 0.86622660040855404)
('epoch', 81, 'train_loss:', 3.3432583320140838, 'val_loss:', 0.86609467387199401)
('epoch', 82, 'train_loss:', 3.3461533868312836, 'val_loss:', 0.86093938112258916)
('epoch', 83, 'train_loss:', 3.3272893869876863, 'val_loss:', 0.86383818864822393)
('epoch', 84, 'train_loss:', 3.3321229457855224, 'val_loss:', 0.86541045665740968)
('epoch', 85, 'train_loss:', 3.3257045459747316, 'val_loss:', 0.8618038415908813)
('epoch', 86, 'train_loss:', 3.3205029368400574, 'val_loss:', 0.86513909101486208)
('epoch', 87, 'train_loss:', 3.3139057660102846, 'val_loss:', 0.85763168334960938)
('epoch', 88, 'train_loss:', 3.307499178647995, 'val_loss:', 0.86189919710159302)
('epoch', 89, 'train_loss:', 3.3055121123790743, 'val_loss:', 0.86505957245826726)
('epoch', 90, 'train_loss:', 3.2998840844631196, 'val_loss:', 0.86451190948486323)
('epoch', 91, 'train_loss:', 3.2926291000843046, 'val_loss:', 0.85917366385459903)
('epoch', 92, 'train_loss:', 3.2894131338596342, 'val_loss:', 0.85736081123352048)
('epoch', 93, 'train_loss:', 3.2792239677906037, 'val_loss:', 0.85486954092979428)
('epoch', 94, 'train_loss:', 3.2817579674720765, 'val_loss:', 0.85607834100723268)
('epoch', 95, 'train_loss:', 3.2833540344238283, 'val_loss:', 0.85773438334465024)
('epoch', 96, 'train_loss:', 3.2684420228004454, 'val_loss:', 0.856432957649231)
('epoch', 97, 'train_loss:', 3.2701880848407745, 'val_loss:', 0.85212214946746823)
('epoch', 98, 'train_loss:', 3.2635782527923585, 'val_loss:', 0.85195469021797177)
('epoch', 99, 'train_loss:', 3.2573863387107851, 'val_loss:', 0.8546845602989197)
('epoch', 100, 'train_loss:', 3.2598120045661925, 'val_loss:', 0.85814279437065122)
('epoch', 101, 'train_loss:', 3.2558960831165313, 'val_loss:', 0.8502751386165619)
('epoch', 102, 'train_loss:', 3.2463002359867095, 'val_loss:', 0.85518198251724242)
('epoch', 103, 'train_loss:', 3.2397222113609314, 'val_loss:', 0.85338644266128538)
('epoch', 104, 'train_loss:', 3.2429160523414611, 'val_loss:', 0.85281120896339413)
('epoch', 105, 'train_loss:', 3.2397618389129637, 'val_loss:', 0.85148287296295166)
('epoch', 106, 'train_loss:', 3.2348704886436463, 'val_loss:', 0.85566164135932921)
('epoch', 107, 'train_loss:', 3.2264419901371002, 'val_loss:', 0.84909871101379397)
('epoch', 108, 'train_loss:', 3.2239368700981141, 'val_loss:', 0.84895602226257327)
('epoch', 109, 'train_loss:', 3.2247106993198393, 'val_loss:', 0.84979576706886295)
('epoch', 110, 'train_loss:', 3.2209003293514251, 'val_loss:', 0.84488948822021481)
('epoch', 111, 'train_loss:', 3.2129685461521147, 'val_loss:', 0.84586977958679199)
('epoch', 112, 'train_loss:', 3.2172216153144837, 'val_loss:', 0.846499674320221)
('epoch', 113, 'train_loss:', 3.2086215639114379, 'val_loss:', 0.84356020808219911)
('epoch', 114, 'train_loss:', 3.2035449409484862, 'val_loss:', 0.84738485217094417)
('epoch', 115, 'train_loss:', 3.1992179405689241, 'val_loss:', 0.84352095127105708)
('epoch', 116, 'train_loss:', 3.1996520245075226, 'val_loss:', 0.84473199963569645)
('epoch', 117, 'train_loss:', 3.1944518554210664, 'val_loss:', 0.84717444300651545)
('epoch', 118, 'train_loss:', 3.1915245449542997, 'val_loss:', 0.84530492901802068)
('epoch', 119, 'train_loss:', 3.1889551293849947, 'val_loss:', 0.8446386539936066)
('epoch', 120, 'train_loss:', 3.1904216003417969, 'val_loss:', 0.84239371538162233)
('epoch', 121, 'train_loss:', 3.1808488106727602, 'val_loss:', 0.84473585724830624)
('epoch', 122, 'train_loss:', 3.1878380477428436, 'val_loss:', 0.84532233595848083)
('epoch', 123, 'train_loss:', 3.1709335696697236, 'val_loss:', 0.84337701797485354)
('epoch', 124, 'train_loss:', 3.1731356263160704, 'val_loss:', 0.84420347213745117)
('epoch', 125, 'train_loss:', 3.1708854365348818, 'val_loss:', 0.84227281689643863)
('epoch', 126, 'train_loss:', 3.1686618983745576, 'val_loss:', 0.84308229446411131)
('epoch', 127, 'train_loss:', 3.1647981762886048, 'val_loss:', 0.84518487572669987)
('epoch', 128, 'train_loss:', 3.1539816796779632, 'val_loss:', 0.84099396705627438)
('epoch', 129, 'train_loss:', 3.1556529390811918, 'val_loss:', 0.84091990113258364)
('epoch', 130, 'train_loss:', 3.1491131770610807, 'val_loss:', 0.84447857022285466)
('epoch', 131, 'train_loss:', 3.1500658643245698, 'val_loss:', 0.84123291611671447)
('epoch', 132, 'train_loss:', 3.1535024797916411, 'val_loss:', 0.84200746536254878)
('epoch', 133, 'train_loss:', 3.1445395982265474, 'val_loss:', 0.83855726838111877)
('epoch', 134, 'train_loss:', 3.1388586497306825, 'val_loss:', 0.83895909309387207)
('epoch', 135, 'train_loss:', 3.1412157392501832, 'val_loss:', 0.84129625916481021)
('epoch', 136, 'train_loss:', 3.1345542228221892, 'val_loss:', 0.84291451096534731)
('epoch', 137, 'train_loss:', 3.1367968642711639, 'val_loss:', 0.83975775837898259)
('epoch', 138, 'train_loss:', 3.1233177852630614, 'val_loss:', 0.84010778546333309)
('epoch', 139, 'train_loss:', 3.1244551622867585, 'val_loss:', 0.84010603427886965)
('epoch', 140, 'train_loss:', 3.1168978321552276, 'val_loss:', 0.84126664280891417)
('epoch', 141, 'train_loss:', 3.1243041336536406, 'val_loss:', 0.84092617750167842)
('epoch', 142, 'train_loss:', 3.1166753780841829, 'val_loss:', 0.83940170049667362)
('epoch', 143, 'train_loss:', 3.1110770368576048, 'val_loss:', 0.84183061718940733)
('epoch', 144, 'train_loss:', 3.1104315006732941, 'val_loss:', 0.83880654931068421)
('epoch', 145, 'train_loss:', 3.1041502797603608, 'val_loss:', 0.84030121803283686)
('epoch', 146, 'train_loss:', 3.1056207418441772, 'val_loss:', 0.83819818854331973)
('epoch', 147, 'train_loss:', 3.1017133665084837, 'val_loss:', 0.83513573408126829)
('epoch', 148, 'train_loss:', 3.0989237689971922, 'val_loss:', 0.83791609048843385)
('epoch', 149, 'train_loss:', 3.099959831237793, 'val_loss:', 0.83710927724838258)
('epoch', 150, 'train_loss:', 3.0981241047382353, 'val_loss:', 0.84234584212303165)
('epoch', 151, 'train_loss:', 3.0897320187091828, 'val_loss:', 0.83566660642623902)
('epoch', 152, 'train_loss:', 3.0900914764404295, 'val_loss:', 0.83480094552040096)
('epoch', 153, 'train_loss:', 3.0876846003532408, 'val_loss:', 0.84067731261253353)
('epoch', 154, 'train_loss:', 3.081195602416992, 'val_loss:', 0.8375867962837219)
('epoch', 155, 'train_loss:', 3.08122101187706, 'val_loss:', 0.84003661990165712)
('epoch', 156, 'train_loss:', 3.0818630671501159, 'val_loss:', 0.8367995023727417)
('epoch', 157, 'train_loss:', 3.0743927776813509, 'val_loss:', 0.8378753662109375)
('epoch', 158, 'train_loss:', 3.0720129203796387, 'val_loss:', 0.83640695691108702)
('epoch', 159, 'train_loss:', 3.0700876891613005, 'val_loss:', 0.83713235616683956)
('epoch', 160, 'train_loss:', 3.0704099369049072, 'val_loss:', 0.83599352002143856)
('epoch', 161, 'train_loss:', 3.0688169050216674, 'val_loss:', 0.83710489749908445)
('epoch', 162, 'train_loss:', 3.0601335215568541, 'val_loss:', 0.8398805475234985)
('epoch', 163, 'train_loss:', 3.0639597392082214, 'val_loss:', 0.83596590638160706)
('epoch', 164, 'train_loss:', 3.0517640471458436, 'val_loss:', 0.83835605502128596)
('epoch', 165, 'train_loss:', 3.0564885079860686, 'val_loss:', 0.83906336665153503)
('epoch', 166, 'train_loss:', 3.0525223100185395, 'val_loss:', 0.83606083035469059)
('epoch', 167, 'train_loss:', 3.0460350322723388, 'val_loss:', 0.8368466508388519)
('epoch', 168, 'train_loss:', 3.0492231774330141, 'val_loss:', 0.83633759737014768)
('epoch', 169, 'train_loss:', 3.0467513990402222, 'val_loss:', 0.83656624197959895)
('epoch', 170, 'train_loss:', 3.0396955597400663, 'val_loss:', 0.83763837337493896)
('epoch', 171, 'train_loss:', 3.0359288489818574, 'val_loss:', 0.83524300456047063)
('epoch', 172, 'train_loss:', 3.034589352607727, 'val_loss:', 0.84051463127136228)
('epoch', 173, 'train_loss:', 3.0295942354202272, 'val_loss:', 0.83404808759689331)
('epoch', 174, 'train_loss:', 3.0251993274688722, 'val_loss:', 0.83952055215835575)
('epoch', 175, 'train_loss:', 3.0304208886623383, 'val_loss:', 0.83763584613800046)
('epoch', 176, 'train_loss:', 3.022700072526932, 'val_loss:', 0.840457193851471)
('epoch', 177, 'train_loss:', 3.0232148993015291, 'val_loss:', 0.83704268097877499)
('epoch', 178, 'train_loss:', 3.0193791782855985, 'val_loss:', 0.83408450961112979)
('epoch', 179, 'train_loss:', 3.0168481922149657, 'val_loss:', 0.83705590367317195)
('epoch', 180, 'train_loss:', 3.0146499025821685, 'val_loss:', 0.83676596164703365)
('epoch', 181, 'train_loss:', 3.009019920825958, 'val_loss:', 0.83391105532646181)
('epoch', 182, 'train_loss:', 3.0123954153060915, 'val_loss:', 0.83896580219268801)
('epoch', 183, 'train_loss:', 3.0104571104049684, 'val_loss:', 0.834550222158432)
('epoch', 184, 'train_loss:', 3.003735203742981, 'val_loss:', 0.83828948378562929)
('epoch', 185, 'train_loss:', 3.0033880996704103, 'val_loss:', 0.84071351408958439)
('epoch', 186, 'train_loss:', 2.9977538239955903, 'val_loss:', 0.83781325936317441)
('epoch', 187, 'train_loss:', 2.9984874320030213, 'val_loss:', 0.83853039026260379)
('epoch', 188, 'train_loss:', 3.0033014941215517, 'val_loss:', 0.8381536686420441)
('epoch', 189, 'train_loss:', 2.9921360695362091, 'val_loss:', 0.83942764163017269)
('epoch', 190, 'train_loss:', 2.9901623296737672, 'val_loss:', 0.8386373853683472)
('epoch', 191, 'train_loss:', 2.9957144641876221, 'val_loss:', 0.83563041567802432)
('epoch', 192, 'train_loss:', 2.9902452099323273, 'val_loss:', 0.84075575590133667)
('epoch', 193, 'train_loss:', 2.9830473685264587, 'val_loss:', 0.83666713595390318)
('epoch', 194, 'train_loss:', 2.9770174849033357, 'val_loss:', 0.83604835152626034)
('epoch', 195, 'train_loss:', 2.9815355372428893, 'val_loss:', 0.83959646463394166)
('epoch', 196, 'train_loss:', 2.9811681151390075, 'val_loss:', 0.83445898890495296)
('epoch', 197, 'train_loss:', 2.973573614358902, 'val_loss:', 0.84145878791809081)
('epoch', 198, 'train_loss:', 2.971530170440674, 'val_loss:', 0.83778368115425106)
('epoch', 199, 'train_loss:', 2.9628638219833374, 'val_loss:', 0.84030353903770449)
('epoch', 200, 'train_loss:', 2.9672993445396423, 'val_loss:', 0.83937851071357727)
('epoch', 201, 'train_loss:', 2.967321949005127, 'val_loss:', 0.83903496503829955)
('epoch', 202, 'train_loss:', 2.9599514031410217, 'val_loss:', 0.84023754358291625)
('epoch', 203, 'train_loss:', 2.9668168222904203, 'val_loss:', 0.84204299211502076)
('epoch', 204, 'train_loss:', 2.9593702256679535, 'val_loss:', 0.83993107318878168)
('epoch', 205, 'train_loss:', 2.9571701991558075, 'val_loss:', 0.83874000072479249)
('epoch', 206, 'train_loss:', 2.954740800857544, 'val_loss:', 0.84010635495185848)
('epoch', 207, 'train_loss:', 2.9492837250232697, 'val_loss:', 0.83738235354423518)
('epoch', 208, 'train_loss:', 2.9517091131210327, 'val_loss:', 0.83967759251594543)
('epoch', 209, 'train_loss:', 2.9517756819725038, 'val_loss:', 0.83940137505531309)
('epoch', 210, 'train_loss:', 2.9426006162166596, 'val_loss:', 0.83974916577339176)
('epoch', 211, 'train_loss:', 2.9393386280536653, 'val_loss:', 0.8423054695129395)
('epoch', 212, 'train_loss:', 2.938388921022415, 'val_loss:', 0.83971309185028076)
('epoch', 213, 'train_loss:', 2.9415875494480135, 'val_loss:', 0.84054864883422853)
('epoch', 214, 'train_loss:', 2.9390234136581421, 'val_loss:', 0.8423143935203552)
('epoch', 215, 'train_loss:', 2.9339952015876771, 'val_loss:', 0.84429745078086849)
('epoch', 216, 'train_loss:', 2.9267927348613738, 'val_loss:', 0.84821312546730043)
('epoch', 217, 'train_loss:', 2.9270107698440553, 'val_loss:', 0.8380433130264282)
('epoch', 218, 'train_loss:', 2.933966338634491, 'val_loss:', 0.84096807003021246)
('epoch', 219, 'train_loss:', 2.9264041686058047, 'val_loss:', 0.83826200127601624)
('epoch', 220, 'train_loss:', 2.9205540835857393, 'val_loss:', 0.84351454496383671)
('epoch', 221, 'train_loss:', 2.9202342414855957, 'val_loss:', 0.84044440150260924)
('epoch', 222, 'train_loss:', 2.9161423087120055, 'val_loss:', 0.84188884973526001)
('epoch', 223, 'train_loss:', 2.9187094485759735, 'val_loss:', 0.8422803616523743)
('epoch', 224, 'train_loss:', 2.9128837490081789, 'val_loss:', 0.84470508575439451)
('epoch', 225, 'train_loss:', 2.9118649911880494, 'val_loss:', 0.84067455053329465)
('epoch', 226, 'train_loss:', 2.914224056005478, 'val_loss:', 0.84616753578186032)
('epoch', 227, 'train_loss:', 2.9040733087062836, 'val_loss:', 0.84276933908462526)
('epoch', 228, 'train_loss:', 2.9077629303932189, 'val_loss:', 0.8435167002677918)
('epoch', 229, 'train_loss:', 2.9040503692626953, 'val_loss:', 0.84131554365158079)
('epoch', 230, 'train_loss:', 2.9057513809204103, 'val_loss:', 0.8463940715789795)
('epoch', 231, 'train_loss:', 2.8966609096527098, 'val_loss:', 0.84590159296989442)
('epoch', 232, 'train_loss:', 2.8965200150012969, 'val_loss:', 0.84443669319152836)
('epoch', 233, 'train_loss:', 2.8965400755405426, 'val_loss:', 0.84717370629310607)
('epoch', 234, 'train_loss:', 2.8887889122962953, 'val_loss:', 0.84359184026718137)
('epoch', 235, 'train_loss:', 2.8874381768703459, 'val_loss:', 0.84398290872573856)
('epoch', 236, 'train_loss:', 2.8874779605865477, 'val_loss:', 0.8464460527896881)
('epoch', 237, 'train_loss:', 2.8834124243259431, 'val_loss:', 0.84601023912429807)
('epoch', 238, 'train_loss:', 2.8903799688816072, 'val_loss:', 0.84826670527458192)
('epoch', 239, 'train_loss:', 2.8821182107925414, 'val_loss:', 0.84776728034019466)
('epoch', 240, 'train_loss:', 2.8814677333831789, 'val_loss:', 0.84892825722694398)
('epoch', 241, 'train_loss:', 2.8793697738647461, 'val_loss:', 0.85125368595123296)
('epoch', 242, 'train_loss:', 2.875002089738846, 'val_loss:', 0.84732314944267273)
('epoch', 243, 'train_loss:', 2.873682873249054, 'val_loss:', 0.85031360626220698)
('epoch', 244, 'train_loss:', 2.8755381262302397, 'val_loss:', 0.84810926318168645)
('epoch', 245, 'train_loss:', 2.8685351574420928, 'val_loss:', 0.84980397343635561)
('epoch', 246, 'train_loss:', 2.8659919095039368, 'val_loss:', 0.84481592178344722)
('epoch', 247, 'train_loss:', 2.8610339736938477, 'val_loss:', 0.85033456206321711)
('epoch', 248, 'train_loss:', 2.8627287125587464, 'val_loss:', 0.84785809874534612)
('epoch', 249, 'train_loss:', 2.8576616966724395, 'val_loss:', 0.85081775665283199)
('epoch', 250, 'train_loss:', 2.8543821263313292, 'val_loss:', 0.8496095216274262)
('epoch', 251, 'train_loss:', 2.8580712878704073, 'val_loss:', 0.85384895086288448)
('epoch', 252, 'train_loss:', 2.85424920797348, 'val_loss:', 0.84970761537551875)
('epoch', 253, 'train_loss:', 2.8538243186473848, 'val_loss:', 0.85271825790405276)
('epoch', 254, 'train_loss:', 2.8503122425079344, 'val_loss:', 0.84954221963882448)
('epoch', 255, 'train_loss:', 2.8470662021636963, 'val_loss:', 0.84355144858360287)
('epoch', 256, 'train_loss:', 2.8471480023860933, 'val_loss:', 0.84963304877281187)
('epoch', 257, 'train_loss:', 2.8447505843639376, 'val_loss:', 0.84658136606216428)
('epoch', 258, 'train_loss:', 2.8434277153015137, 'val_loss:', 0.85046133279800418)
('epoch', 259, 'train_loss:', 2.8446307516098024, 'val_loss:', 0.85049986720085147)
('epoch', 260, 'train_loss:', 2.8398902928829193, 'val_loss:', 0.85180956482887271)
('epoch', 261, 'train_loss:', 2.8363588988780974, 'val_loss:', 0.85281358838081356)
('epoch', 262, 'train_loss:', 2.8342171347141267, 'val_loss:', 0.85078462600708005)
('epoch', 263, 'train_loss:', 2.8308430552482604, 'val_loss:', 0.8537515985965729)
('epoch', 264, 'train_loss:', 2.8287410390377046, 'val_loss:', 0.85943282485008243)
('epoch', 265, 'train_loss:', 2.8261715698242189, 'val_loss:', 0.85358923792839048)
('epoch', 266, 'train_loss:', 2.8267198288440705, 'val_loss:', 0.85635462403297424)
('epoch', 267, 'train_loss:', 2.8214576327800751, 'val_loss:', 0.85182216405868527)
('epoch', 268, 'train_loss:', 2.8182486152648925, 'val_loss:', 0.85865608572959895)
('epoch', 269, 'train_loss:', 2.8130446529388426, 'val_loss:', 0.85686129570007319)
('epoch', 270, 'train_loss:', 2.8175289011001587, 'val_loss:', 0.8580769610404968)
('epoch', 271, 'train_loss:', 2.8134153091907503, 'val_loss:', 0.86084537982940679)
('epoch', 272, 'train_loss:', 2.8097744584083557, 'val_loss:', 0.85660365819931028)
('epoch', 273, 'train_loss:', 2.8103553414344788, 'val_loss:', 0.85591863512992861)
('epoch', 274, 'train_loss:', 2.8084870290756228, 'val_loss:', 0.85705010414123539)
('epoch', 275, 'train_loss:', 2.8038405346870423, 'val_loss:', 0.85786448240280155)
('epoch', 276, 'train_loss:', 2.7985205042362211, 'val_loss:', 0.85723253607749939)
('epoch', 277, 'train_loss:', 2.8033656489849093, 'val_loss:', 0.85796460866928104)
('epoch', 278, 'train_loss:', 2.7987545347213745, 'val_loss:', 0.8573642206192017)
('epoch', 279, 'train_loss:', 2.7971337771415712, 'val_loss:', 0.8624017715454102)
('epoch', 280, 'train_loss:', 2.7951786589622496, 'val_loss:', 0.85840640664100643)
('epoch', 281, 'train_loss:', 2.7912822353839872, 'val_loss:', 0.85776885151863103)
('epoch', 282, 'train_loss:', 2.7903777444362641, 'val_loss:', 0.86244354009628299)
('epoch', 283, 'train_loss:', 2.7870318400859833, 'val_loss:', 0.85968004941940313)
('epoch', 284, 'train_loss:', 2.7842153584957123, 'val_loss:', 0.85994899272918701)
('epoch', 285, 'train_loss:', 2.7863668072223664, 'val_loss:', 0.85723237156867982)
('epoch', 286, 'train_loss:', 2.7786968433856964, 'val_loss:', 0.86599925279617307)
('epoch', 287, 'train_loss:', 2.7714979434013367, 'val_loss:', 0.86381607174873354)
('epoch', 288, 'train_loss:', 2.7812185990810394, 'val_loss:', 0.86711452960968016)
('epoch', 289, 'train_loss:', 2.7778426265716551, 'val_loss:', 0.86477178215980532)
('epoch', 290, 'train_loss:', 2.772885876893997, 'val_loss:', 0.86317668437957762)
('epoch', 291, 'train_loss:', 2.7740938353538511, 'val_loss:', 0.86112478137016302)
('epoch', 292, 'train_loss:', 2.7688703262805938, 'val_loss:', 0.86343698978424077)
('epoch', 293, 'train_loss:', 2.7666801559925078, 'val_loss:', 0.86315027117729182)
('epoch', 294, 'train_loss:', 2.7682066750526428, 'val_loss:', 0.86801055908203129)
('epoch', 295, 'train_loss:', 2.7642645907402037, 'val_loss:', 0.86777668476104741)
('epoch', 296, 'train_loss:', 2.7594261109828948, 'val_loss:', 0.86857770323753358)
('epoch', 297, 'train_loss:', 2.7554650712013244, 'val_loss:', 0.87045118570327762)
('epoch', 298, 'train_loss:', 2.7545626676082611, 'val_loss:', 0.8700980544090271)
('epoch', 299, 'train_loss:', 2.754787585735321, 'val_loss:', 0.8656065440177918)
('epoch', 300, 'train_loss:', 2.7532690358161926, 'val_loss:', 0.8677123737335205)
('epoch', 301, 'train_loss:', 2.749546890258789, 'val_loss:', 0.86836149454116818)
('epoch', 302, 'train_loss:', 2.7474769043922422, 'val_loss:', 0.86818029999732971)
('epoch', 303, 'train_loss:', 2.7443651056289671, 'val_loss:', 0.86690238714218137)
('epoch', 304, 'train_loss:', 2.7446292662620544, 'val_loss:', 0.87307435393333432)
('epoch', 305, 'train_loss:', 2.7389878714084626, 'val_loss:', 0.86938947796821597)
('epoch', 306, 'train_loss:', 2.7435404980182647, 'val_loss:', 0.86841607451438907)
('epoch', 307, 'train_loss:', 2.7320795691013338, 'val_loss:', 0.87422264814376827)
('epoch', 308, 'train_loss:', 2.7304513180255889, 'val_loss:', 0.87003974080085755)
('epoch', 309, 'train_loss:', 2.7353939652442931, 'val_loss:', 0.87684444665908812)
('epoch', 310, 'train_loss:', 2.7322041881084442, 'val_loss:', 0.87023748755455022)
('epoch', 311, 'train_loss:', 2.730612349510193, 'val_loss:', 0.87240523576736451)
('epoch', 312, 'train_loss:', 2.724336564540863, 'val_loss:', 0.87391106843948363)
('epoch', 313, 'train_loss:', 2.7235862195491789, 'val_loss:', 0.87578450918197637)
('epoch', 314, 'train_loss:', 2.7259009039402007, 'val_loss:', 0.87485664010047914)
('epoch', 315, 'train_loss:', 2.7220165991783141, 'val_loss:', 0.87565964221954351)
('epoch', 316, 'train_loss:', 2.7245626914501191, 'val_loss:', 0.87494316220283508)
('epoch', 317, 'train_loss:', 2.7191955816745756, 'val_loss:', 0.88188555955886838)
('epoch', 318, 'train_loss:', 2.7162210488319398, 'val_loss:', 0.87901906013488773)
('epoch', 319, 'train_loss:', 2.7120989394187927, 'val_loss:', 0.87998008131980898)
('epoch', 320, 'train_loss:', 2.7131762683391569, 'val_loss:', 0.88070161223411558)
('epoch', 321, 'train_loss:', 2.7086794841289521, 'val_loss:', 0.87936724305152891)
('epoch', 322, 'train_loss:', 2.7101502561569215, 'val_loss:', 0.87802771091461185)
('epoch', 323, 'train_loss:', 2.7061647510528566, 'val_loss:', 0.8773606312274933)
('epoch', 324, 'train_loss:', 2.7000034248828886, 'val_loss:', 0.87703199148178101)
('epoch', 325, 'train_loss:', 2.7016975808143617, 'val_loss:', 0.87664021611213683)
('epoch', 326, 'train_loss:', 2.7024719977378844, 'val_loss:', 0.88078120231628421)
('epoch', 327, 'train_loss:', 2.69613986492157, 'val_loss:', 0.8802633786201477)
('epoch', 328, 'train_loss:', 2.6896557843685152, 'val_loss:', 0.87985194206237793)
('epoch', 329, 'train_loss:', 2.6884748208522797, 'val_loss:', 0.8859774208068848)
('epoch', 330, 'train_loss:', 2.6889380848407747, 'val_loss:', 0.88430034518241885)
('epoch', 331, 'train_loss:', 2.6900013458728789, 'val_loss:', 0.88455600857734684)
('epoch', 332, 'train_loss:', 2.6868235552310944, 'val_loss:', 0.88395242452621459)
('epoch', 333, 'train_loss:', 2.6841298830509186, 'val_loss:', 0.88907495141029358)
('epoch', 334, 'train_loss:', 2.681574672460556, 'val_loss:', 0.88544763565063478)
('epoch', 335, 'train_loss:', 2.6793642795085906, 'val_loss:', 0.88704703688621522)
('epoch', 336, 'train_loss:', 2.6872859394550321, 'val_loss:', 0.88812176227569584)
('epoch', 337, 'train_loss:', 2.6767825961112974, 'val_loss:', 0.88874823808670045)
('epoch', 338, 'train_loss:', 2.6744572472572328, 'val_loss:', 0.88812377572059631)
('epoch', 339, 'train_loss:', 2.6714720046520233, 'val_loss:', 0.88819824218749999)
('epoch', 340, 'train_loss:', 2.6697421061992643, 'val_loss:', 0.88860299468040471)
('epoch', 341, 'train_loss:', 2.6669401693344117, 'val_loss:', 0.88741514921188358)
('epoch', 342, 'train_loss:', 2.6614088404178617, 'val_loss:', 0.89190488100051879)
('epoch', 343, 'train_loss:', 2.6697467386722566, 'val_loss:', 0.88884268999099736)
('epoch', 344, 'train_loss:', 2.6662806093692781, 'val_loss:', 0.89299997687339783)
('epoch', 345, 'train_loss:', 2.6631092047691345, 'val_loss:', 0.89087626814842225)
('epoch', 346, 'train_loss:', 2.6587497854232787, 'val_loss:', 0.8921963667869568)
('epoch', 347, 'train_loss:', 2.6590197622776031, 'val_loss:', 0.89200574040412905)
('epoch', 348, 'train_loss:', 2.6594156932830813, 'val_loss:', 0.89295701026916507)
('epoch', 349, 'train_loss:', 2.6528363144397735, 'val_loss:', 0.89450709939002992)
('epoch', 350, 'train_loss:', 2.6523325145244598, 'val_loss:', 0.89261212229728704)
('epoch', 351, 'train_loss:', 2.6522841835021973, 'val_loss:', 0.89543029189109802)
('epoch', 352, 'train_loss:', 2.646500722169876, 'val_loss:', 0.89398950457572934)
('epoch', 353, 'train_loss:', 2.6452738451957702, 'val_loss:', 0.89763050079345708)
('epoch', 354, 'train_loss:', 2.6421632754802702, 'val_loss:', 0.8954279708862305)
('epoch', 355, 'train_loss:', 2.6361129009723663, 'val_loss:', 0.89775395274162295)
('epoch', 356, 'train_loss:', 2.6411334717273713, 'val_loss:', 0.89695793628692622)
('epoch', 357, 'train_loss:', 2.6364078748226167, 'val_loss:', 0.89900287151336666)
('epoch', 358, 'train_loss:', 2.6417356538772583, 'val_loss:', 0.899142245054245)
('epoch', 359, 'train_loss:', 2.6320907950401304, 'val_loss:', 0.89469980716705322)
('epoch', 360, 'train_loss:', 2.6353468143939973, 'val_loss:', 0.90081690192222597)
('epoch', 361, 'train_loss:', 2.6320526576042176, 'val_loss:', 0.89673008322715764)
('epoch', 362, 'train_loss:', 2.6229414463043215, 'val_loss:', 0.90090212464332575)
('epoch', 363, 'train_loss:', 2.6257242274284365, 'val_loss:', 0.89615261197090146)
('epoch', 364, 'train_loss:', 2.6250754833221435, 'val_loss:', 0.90161290407180783)
('epoch', 365, 'train_loss:', 2.6241193521022796, 'val_loss:', 0.90322853326797481)
('epoch', 366, 'train_loss:', 2.6188200390338898, 'val_loss:', 0.89877214670181271)
('epoch', 367, 'train_loss:', 2.616624792814255, 'val_loss:', 0.90376873373985289)
('epoch', 368, 'train_loss:', 2.6129346656799317, 'val_loss:', 0.90577107667922974)
('epoch', 369, 'train_loss:', 2.6150026047229766, 'val_loss:', 0.90210240364074712)
('epoch', 370, 'train_loss:', 2.615466775894165, 'val_loss:', 0.90140765547752377)
('epoch', 371, 'train_loss:', 2.6063726556301119, 'val_loss:', 0.90458593726158143)
('epoch', 372, 'train_loss:', 2.6075962495803835, 'val_loss:', 0.90600401759147642)
('epoch', 373, 'train_loss:', 2.6073316061496734, 'val_loss:', 0.90629364848136906)
('epoch', 374, 'train_loss:', 2.6041504633426666, 'val_loss:', 0.9051597058773041)
('epoch', 375, 'train_loss:', 2.6028733861446383, 'val_loss:', 0.90588727951049808)
('epoch', 376, 'train_loss:', 2.6018093097209931, 'val_loss:', 0.90814975976943968)
('epoch', 377, 'train_loss:', 2.6041884630918504, 'val_loss:', 0.91247631669044493)
('epoch', 378, 'train_loss:', 2.5983858883380888, 'val_loss:', 0.90992424368858338)
('epoch', 379, 'train_loss:', 2.5899491000175474, 'val_loss:', 0.90917546510696412)
('epoch', 380, 'train_loss:', 2.587199944257736, 'val_loss:', 0.91332958221435545)
('epoch', 381, 'train_loss:', 2.5922460329532622, 'val_loss:', 0.90933880448341364)
('epoch', 382, 'train_loss:', 2.5891180676221848, 'val_loss:', 0.9107137084007263)
('epoch', 383, 'train_loss:', 2.5863517355918884, 'val_loss:', 0.9084118402004242)
('epoch', 384, 'train_loss:', 2.5870237743854521, 'val_loss:', 0.9123172116279602)
('epoch', 385, 'train_loss:', 2.5820942735671997, 'val_loss:', 0.90972099661827088)
('epoch', 386, 'train_loss:', 2.5790270924568177, 'val_loss:', 0.91524048924446111)
('epoch', 387, 'train_loss:', 2.5814015400409698, 'val_loss:', 0.91094310760498043)
('epoch', 388, 'train_loss:', 2.5761066198349001, 'val_loss:', 0.91480657815933231)
('epoch', 389, 'train_loss:', 2.5766563153266908, 'val_loss:', 0.90978759169578549)
('epoch', 390, 'train_loss:', 2.5750707322359085, 'val_loss:', 0.91169695138931273)
('epoch', 391, 'train_loss:', 2.5707529783248901, 'val_loss:', 0.91430183768272399)
('epoch', 392, 'train_loss:', 2.5677341216802598, 'val_loss:', 0.91516986370086673)
('epoch', 393, 'train_loss:', 2.5669328647851946, 'val_loss:', 0.91778233051300051)
('epoch', 394, 'train_loss:', 2.5671923905611038, 'val_loss:', 0.91519570231437686)
('epoch', 395, 'train_loss:', 2.5631359428167344, 'val_loss:', 0.92176309823989866)
('epoch', 396, 'train_loss:', 2.5615482538938523, 'val_loss:', 0.91661937952041628)
('epoch', 397, 'train_loss:', 2.5676026117801665, 'val_loss:', 0.91621313929557802)
('epoch', 398, 'train_loss:', 2.5644168776273726, 'val_loss:', 0.91805071473121647)
('epoch', 399, 'train_loss:', 2.557413719892502, 'val_loss:', 0.91708871126174929)
('epoch', 400, 'train_loss:', 2.5607468032836915, 'val_loss:', 0.92014070391654967)
('epoch', 401, 'train_loss:', 2.5572530734539032, 'val_loss:', 0.9173942255973816)
('epoch', 402, 'train_loss:', 2.5494105148315431, 'val_loss:', 0.91855121612548829)
('epoch', 403, 'train_loss:', 2.5504175484180451, 'val_loss:', 0.92210045218467718)
('epoch', 404, 'train_loss:', 2.5443437230587005, 'val_loss:', 0.92618589758872982)
('epoch', 405, 'train_loss:', 2.5429115641117095, 'val_loss:', 0.91981126904487609)
('epoch', 406, 'train_loss:', 2.5449107670783997, 'val_loss:', 0.92363343715667723)
('epoch', 407, 'train_loss:', 2.5405972719192507, 'val_loss:', 0.92237979292869565)
('epoch', 408, 'train_loss:', 2.5432610136270521, 'val_loss:', 0.92449538946151733)
('epoch', 409, 'train_loss:', 2.539413560628891, 'val_loss:', 0.92405818343162538)
('epoch', 410, 'train_loss:', 2.5364590108394625, 'val_loss:', 0.92320681095123291)
('epoch', 411, 'train_loss:', 2.5379307508468627, 'val_loss:', 0.92453442931175234)
('epoch', 412, 'train_loss:', 2.5306196790933608, 'val_loss:', 0.92362262368202208)
('epoch', 413, 'train_loss:', 2.5325599783658981, 'val_loss:', 0.93139969825744628)
('epoch', 414, 'train_loss:', 2.5269749212265014, 'val_loss:', 0.93005872964859004)
('epoch', 415, 'train_loss:', 2.5270771956443787, 'val_loss:', 0.92991622328758239)
('epoch', 416, 'train_loss:', 2.5279021096229553, 'val_loss:', 0.93264795184135441)
('epoch', 417, 'train_loss:', 2.5267528873682021, 'val_loss:', 0.92882046580314637)
('epoch', 418, 'train_loss:', 2.5250671738386155, 'val_loss:', 0.9312793469429016)
('epoch', 419, 'train_loss:', 2.519394593834877, 'val_loss:', 0.93317409753799441)
('epoch', 420, 'train_loss:', 2.5161293214559555, 'val_loss:', 0.93232195854187017)
('epoch', 421, 'train_loss:', 2.5178150683641434, 'val_loss:', 0.93125580072402958)
('epoch', 422, 'train_loss:', 2.5138336962461469, 'val_loss:', 0.93544813632965085)
('epoch', 423, 'train_loss:', 2.5141678470373154, 'val_loss:', 0.92933318972587586)
('epoch', 424, 'train_loss:', 2.508534106016159, 'val_loss:', 0.93349442481994627)
('epoch', 425, 'train_loss:', 2.5117779260873796, 'val_loss:', 0.9355831372737885)
('epoch', 426, 'train_loss:', 2.5121659988164904, 'val_loss:', 0.93059267997741701)
('epoch', 427, 'train_loss:', 2.5089620047807695, 'val_loss:', 0.93853637933731082)
('epoch', 428, 'train_loss:', 2.5080129820108414, 'val_loss:', 0.93885796308517455)
('epoch', 429, 'train_loss:', 2.49725099503994, 'val_loss:', 0.9390607643127441)
('epoch', 430, 'train_loss:', 2.5067588871717454, 'val_loss:', 0.93633738279342649)
('epoch', 431, 'train_loss:', 2.5027215254306792, 'val_loss:', 0.94120291233062747)
('epoch', 432, 'train_loss:', 2.500228415131569, 'val_loss:', 0.94314396381378174)
('epoch', 433, 'train_loss:', 2.4970089972019194, 'val_loss:', 0.94109355807304385)
('epoch', 434, 'train_loss:', 2.4959523761272431, 'val_loss:', 0.93882685899734497)
('epoch', 435, 'train_loss:', 2.4907519835233689, 'val_loss:', 0.93858337044715878)
('epoch', 436, 'train_loss:', 2.4914760577678678, 'val_loss:', 0.94338322639465333)
('epoch', 437, 'train_loss:', 2.4910674077272414, 'val_loss:', 0.93498659729957578)
('epoch', 438, 'train_loss:', 2.4848206919431686, 'val_loss:', 0.94112497806549067)
('epoch', 439, 'train_loss:', 2.482630025744438, 'val_loss:', 0.93714832067489628)
('epoch', 440, 'train_loss:', 2.4880072945356368, 'val_loss:', 0.94152535080909727)
('epoch', 441, 'train_loss:', 2.485746010541916, 'val_loss:', 0.9425396931171417)
('epoch', 442, 'train_loss:', 2.4828554666042328, 'val_loss:', 0.94401077151298518)
('epoch', 443, 'train_loss:', 2.4781066298484804, 'val_loss:', 0.94531776547431945)
('epoch', 444, 'train_loss:', 2.4792624729871751, 'val_loss:', 0.94424142718315129)
('epoch', 445, 'train_loss:', 2.4754306519031526, 'val_loss:', 0.94575044631958005)
('epoch', 446, 'train_loss:', 2.4725820928812028, 'val_loss:', 0.94179129362106329)
('epoch', 447, 'train_loss:', 2.4718572402000429, 'val_loss:', 0.94486444473266606)
('epoch', 448, 'train_loss:', 2.4766455692052842, 'val_loss:', 0.94053465485572818)
('epoch', 449, 'train_loss:', 2.4688035982847212, 'val_loss:', 0.95020029783248905)
('epoch', 450, 'train_loss:', 2.4733842664957049, 'val_loss:', 0.94401260972023016)
('epoch', 451, 'train_loss:', 2.4696449929475786, 'val_loss:', 0.9484129416942596)
('epoch', 452, 'train_loss:', 2.471686924099922, 'val_loss:', 0.94801691293716428)
('epoch', 453, 'train_loss:', 2.4619994574785231, 'val_loss:', 0.94893205165863037)
('epoch', 454, 'train_loss:', 2.4616649699211122, 'val_loss:', 0.95109167695045471)
('epoch', 455, 'train_loss:', 2.4665600603818891, 'val_loss:', 0.9539639699459076)
('epoch', 456, 'train_loss:', 2.4605299782752992, 'val_loss:', 0.9497721207141876)
('epoch', 457, 'train_loss:', 2.4569170045852662, 'val_loss:', 0.95221684932708739)
('epoch', 458, 'train_loss:', 2.4557053393125532, 'val_loss:', 0.95336243510246277)
('epoch', 459, 'train_loss:', 2.4530802887678145, 'val_loss:', 0.95253301143646241)
('epoch', 460, 'train_loss:', 2.4571048861742018, 'val_loss:', 0.95179825782775884)
('epoch', 461, 'train_loss:', 2.4571211451292037, 'val_loss:', 0.94918674468994135)
('epoch', 462, 'train_loss:', 2.4460225373506548, 'val_loss:', 0.9519668841361999)
('epoch', 463, 'train_loss:', 2.4442198807001114, 'val_loss:', 0.95205901503562929)
('epoch', 464, 'train_loss:', 2.4420403128862382, 'val_loss:', 0.95480550646781925)
('epoch', 465, 'train_loss:', 2.4442241674661638, 'val_loss:', 0.95503241777420045)
('epoch', 466, 'train_loss:', 2.4436672115325928, 'val_loss:', 0.9546697735786438)
('epoch', 467, 'train_loss:', 2.445797209739685, 'val_loss:', 0.96131437778472906)
('epoch', 468, 'train_loss:', 2.4410502636432647, 'val_loss:', 0.95905086278915408)
('epoch', 469, 'train_loss:', 2.4352773493528366, 'val_loss:', 0.95905811905860905)
('epoch', 470, 'train_loss:', 2.4364897972345352, 'val_loss:', 0.96111952543258672)
('epoch', 471, 'train_loss:', 2.4341467148065568, 'val_loss:', 0.95731095671653743)
('epoch', 472, 'train_loss:', 2.4353585106134417, 'val_loss:', 0.95914630055427552)
('epoch', 473, 'train_loss:', 2.4392630654573439, 'val_loss:', 0.95953724861145018)
('epoch', 474, 'train_loss:', 2.4275133883953095, 'val_loss:', 0.9576392209529877)
('epoch', 475, 'train_loss:', 2.4301816493272783, 'val_loss:', 0.96066109895706175)
('epoch', 476, 'train_loss:', 2.4280480265617372, 'val_loss:', 0.9633790862560272)
('epoch', 477, 'train_loss:', 2.4252647477388383, 'val_loss:', 0.9583381462097168)
('epoch', 478, 'train_loss:', 2.4256203073263167, 'val_loss:', 0.96886230587959288)
('epoch', 479, 'train_loss:', 2.4216173446178435, 'val_loss:', 0.96250841736793513)
('epoch', 480, 'train_loss:', 2.4219872772693636, 'val_loss:', 0.96008117675781246)
('epoch', 481, 'train_loss:', 2.4152486127614976, 'val_loss:', 0.96725538015365597)
('epoch', 482, 'train_loss:', 2.4198648762702941, 'val_loss:', 0.96641727328300475)
('epoch', 483, 'train_loss:', 2.4174280917644499, 'val_loss:', 0.96441800713539128)
('epoch', 484, 'train_loss:', 2.4195655739307402, 'val_loss:', 0.97309768676757813)
('epoch', 485, 'train_loss:', 2.4096703201532366, 'val_loss:', 0.96334079265594486)
('epoch', 486, 'train_loss:', 2.413361831307411, 'val_loss:', 0.96692514300346377)
('epoch', 487, 'train_loss:', 2.409088877439499, 'val_loss:', 0.96484853863716125)
('epoch', 488, 'train_loss:', 2.4133961993455886, 'val_loss:', 0.97150849223136904)
('epoch', 489, 'train_loss:', 2.4093770027160644, 'val_loss:', 0.97111369490623478)
('epoch', 490, 'train_loss:', 2.4080505859851837, 'val_loss:', 0.96601550579071049)
('epoch', 491, 'train_loss:', 2.4061544287204741, 'val_loss:', 0.97050514698028567)
('epoch', 492, 'train_loss:', 2.4032139074802399, 'val_loss:', 0.97161566615104678)
('epoch', 493, 'train_loss:', 2.4003202414512632, 'val_loss:', 0.97050552487373354)
('epoch', 494, 'train_loss:', 2.3975352799892424, 'val_loss:', 0.96711908698081972)
('epoch', 495, 'train_loss:', 2.4014372509717941, 'val_loss:', 0.97631418585777285)
('epoch', 496, 'train_loss:', 2.3994259190559388, 'val_loss:', 0.97434192657470708)
('epoch', 497, 'train_loss:', 2.395233690738678, 'val_loss:', 0.97489188194274901)
('epoch', 498, 'train_loss:', 2.3936109453439713, 'val_loss:', 0.97164471149444576)
('epoch', 499, 'train_loss:', 2.3891598677635191, 'val_loss:', 0.9720163011550903)
('epoch', 500, 'train_loss:', 2.3910406869649887, 'val_loss:', 0.98152051568031307)
('epoch', 501, 'train_loss:', 2.3878676915168762, 'val_loss:', 0.97419275522232052)
('epoch', 502, 'train_loss:', 2.3931320327520371, 'val_loss:', 0.97248873949050907)
('epoch', 503, 'train_loss:', 2.3859792536497118, 'val_loss:', 0.97120946049690249)
('epoch', 504, 'train_loss:', 2.3810042393207551, 'val_loss:', 0.97851030111312864)
('epoch', 505, 'train_loss:', 2.3792838788032533, 'val_loss:', 0.97344705581665036)
('epoch', 506, 'train_loss:', 2.3827696239948271, 'val_loss:', 0.97708711624145506)
('epoch', 507, 'train_loss:', 2.377657243013382, 'val_loss:', 0.97982125043869017)
('epoch', 508, 'train_loss:', 2.3789804029464721, 'val_loss:', 0.98337248921394349)
('epoch', 509, 'train_loss:', 2.3843377864360811, 'val_loss:', 0.98650147199630733)
('epoch', 510, 'train_loss:', 2.3791700464487078, 'val_loss:', 0.97605655908584599)
('epoch', 511, 'train_loss:', 2.3736558711528777, 'val_loss:', 0.97984599113464355)
('epoch', 512, 'train_loss:', 2.3694997805356981, 'val_loss:', 0.98270901560783386)
('epoch', 513, 'train_loss:', 2.3721780776977539, 'val_loss:', 0.98139867663383484)
('epoch', 514, 'train_loss:', 2.3666347515583039, 'val_loss:', 0.97962992548942562)
('epoch', 515, 'train_loss:', 2.368087291121483, 'val_loss:', 0.98185198783874517)
('epoch', 516, 'train_loss:', 2.3663517612218858, 'val_loss:', 0.98366950392723085)
('epoch', 517, 'train_loss:', 2.3641520708799364, 'val_loss:', 0.97953522443771357)
('epoch', 518, 'train_loss:', 2.3650896912813186, 'val_loss:', 0.98158254861831662)
('epoch', 519, 'train_loss:', 2.358935645222664, 'val_loss:', 0.98450239300727849)
('epoch', 520, 'train_loss:', 2.3625150358676912, 'val_loss:', 0.98593609333038335)
('epoch', 521, 'train_loss:', 2.3536622744798659, 'val_loss:', 0.98959155559539791)
('epoch', 522, 'train_loss:', 2.359834412932396, 'val_loss:', 0.987723001241684)
('epoch', 523, 'train_loss:', 2.3588471668958664, 'val_loss:', 0.99222672820091251)
('epoch', 524, 'train_loss:', 2.3537800395488739, 'val_loss:', 0.98466632962226863)
('epoch', 525, 'train_loss:', 2.3530827337503433, 'val_loss:', 0.98453154921531683)
('epoch', 526, 'train_loss:', 2.3566542840003968, 'val_loss:', 0.9881617319583893)
('epoch', 527, 'train_loss:', 2.3491581839323046, 'val_loss:', 0.98954327940940856)
('epoch', 528, 'train_loss:', 2.3563684314489364, 'val_loss:', 0.99397083044052126)
('epoch', 529, 'train_loss:', 2.3496299326419829, 'val_loss:', 0.98889233589172365)
('epoch', 530, 'train_loss:', 2.3384677511453629, 'val_loss:', 0.98757902503013606)
('epoch', 531, 'train_loss:', 2.348213556408882, 'val_loss:', 0.99127677917480472)
('epoch', 532, 'train_loss:', 2.3508911776542663, 'val_loss:', 0.98968607664108277)
('epoch', 533, 'train_loss:', 2.3438357341289522, 'val_loss:', 0.98985985755920414)
('epoch', 534, 'train_loss:', 2.3452347534894944, 'val_loss:', 0.98850221753120426)
('epoch', 535, 'train_loss:', 2.3420793008804321, 'val_loss:', 0.99216292977333065)
('epoch', 536, 'train_loss:', 2.3369018632173537, 'val_loss:', 0.99514076113700867)
('epoch', 537, 'train_loss:', 2.3445555257797239, 'val_loss:', 0.99389917016029361)
('epoch', 538, 'train_loss:', 2.3326560163497927, 'val_loss:', 0.99694906115531923)
('epoch', 539, 'train_loss:', 2.3319719082117079, 'val_loss:', 0.99245396733283997)
('epoch', 540, 'train_loss:', 2.3292190647125244, 'val_loss:', 1.0066803562641145)
('epoch', 541, 'train_loss:', 2.3348490339517594, 'val_loss:', 0.99415676474571224)
('epoch', 542, 'train_loss:', 2.3287240570783614, 'val_loss:', 0.99512364506721496)
('epoch', 543, 'train_loss:', 2.3265428042411802, 'val_loss:', 0.99811915993690492)
('epoch', 544, 'train_loss:', 2.3263963079452514, 'val_loss:', 0.9990971040725708)
('epoch', 545, 'train_loss:', 2.3306510710716246, 'val_loss:', 0.99711894750595098)
('epoch', 546, 'train_loss:', 2.3299833780527113, 'val_loss:', 0.9949815154075623)
('epoch', 547, 'train_loss:', 2.3258260124921799, 'val_loss:', 0.99969594240188597)
('epoch', 548, 'train_loss:', 2.3240422785282133, 'val_loss:', 1.0005406689643861)
('epoch', 549, 'train_loss:', 2.3216224098205567, 'val_loss:', 0.9962404346466065)
('epoch', 550, 'train_loss:', 2.31796717941761, 'val_loss:', 1.0003593838214875)
('epoch', 551, 'train_loss:', 2.314461001753807, 'val_loss:', 0.9992891919612884)
('epoch', 552, 'train_loss:', 2.3132099860906603, 'val_loss:', 0.99778629779815675)
('epoch', 553, 'train_loss:', 2.321248595714569, 'val_loss:', 1.0008275663852693)
('epoch', 554, 'train_loss:', 2.3131096017360688, 'val_loss:', 1.0041413021087646)
('epoch', 555, 'train_loss:', 2.3135169512033462, 'val_loss:', 1.0035999631881714)
('epoch', 556, 'train_loss:', 2.3161728423833847, 'val_loss:', 1.0003169536590577)
('epoch', 557, 'train_loss:', 2.3119793230295183, 'val_loss:', 1.0031628358364104)
('epoch', 558, 'train_loss:', 2.3048562198877334, 'val_loss:', 1.0064281582832337)
('epoch', 559, 'train_loss:', 2.3097282671928405, 'val_loss:', 1.0052495718002319)
('epoch', 560, 'train_loss:', 2.3062784177064897, 'val_loss:', 1.0089669156074523)
('epoch', 561, 'train_loss:', 2.3054811024665831, 'val_loss:', 1.0061302876472473)
('epoch', 562, 'train_loss:', 2.3034195423126222, 'val_loss:', 1.0075077545642852)
('epoch', 563, 'train_loss:', 2.3028566652536391, 'val_loss:', 1.0056692087650299)
('epoch', 564, 'train_loss:', 2.3031915014982225, 'val_loss:', 1.0059903943538666)
('epoch', 565, 'train_loss:', 2.3008916366100309, 'val_loss:', 1.0078729617595672)
('epoch', 566, 'train_loss:', 2.2981197494268417, 'val_loss:', 1.0095855927467345)
('epoch', 567, 'train_loss:', 2.295243199467659, 'val_loss:', 1.0064701235294342)
('epoch', 568, 'train_loss:', 2.2950377810001372, 'val_loss:', 1.0068544816970826)
('epoch', 569, 'train_loss:', 2.2990382331609727, 'val_loss:', 1.0061084461212157)
('epoch', 570, 'train_loss:', 2.2940830898284914, 'val_loss:', 1.0066936385631562)
('epoch', 571, 'train_loss:', 2.2900889325141907, 'val_loss:', 1.0122262394428254)
('epoch', 572, 'train_loss:', 2.2945246475934984, 'val_loss:', 1.0084398186206818)
('epoch', 573, 'train_loss:', 2.2929736459255219, 'val_loss:', 1.0169735586643218)
('epoch', 574, 'train_loss:', 2.2910196876525877, 'val_loss:', 1.0159322559833526)
('epoch', 575, 'train_loss:', 2.2884876835346222, 'val_loss:', 1.0132358241081239)
('epoch', 576, 'train_loss:', 2.2877501970529557, 'val_loss:', 1.0128050851821899)
('epoch', 577, 'train_loss:', 2.2856172060966493, 'val_loss:', 1.0175935113430024)
('epoch', 578, 'train_loss:', 2.2861901050806046, 'val_loss:', 1.0136946308612824)
('epoch', 579, 'train_loss:', 2.2883052265644075, 'val_loss:', 1.0155761170387267)
('epoch', 580, 'train_loss:', 2.2817089486122133, 'val_loss:', 1.0151325619220735)
('epoch', 581, 'train_loss:', 2.2835515397787094, 'val_loss:', 1.0171748518943786)
('epoch', 582, 'train_loss:', 2.2793198227882385, 'val_loss:', 1.0080096340179443)
('epoch', 583, 'train_loss:', 2.277152181863785, 'val_loss:', 1.0131670379638671)
('epoch', 584, 'train_loss:', 2.2750425338745117, 'val_loss:', 1.016874166727066)
('epoch', 585, 'train_loss:', 2.2764190286397934, 'val_loss:', 1.0178498792648316)
('epoch', 586, 'train_loss:', 2.2769662630558014, 'val_loss:', 1.0199226367473602)
('epoch', 587, 'train_loss:', 2.2737915718555453, 'val_loss:', 1.0107146716117859)
('epoch', 588, 'train_loss:', 2.2684501910209658, 'val_loss:', 1.0154503417015075)
('epoch', 589, 'train_loss:', 2.2687183189392091, 'val_loss:', 1.0193389534950257)
('epoch', 590, 'train_loss:', 2.265625942349434, 'val_loss:', 1.0217253255844116)
('epoch', 591, 'train_loss:', 2.2706726044416428, 'val_loss:', 1.0181033253669738)
('epoch', 592, 'train_loss:', 2.2689622670412062, 'val_loss:', 1.0197743940353394)
('epoch', 593, 'train_loss:', 2.2639552426338194, 'val_loss:', 1.022284643650055)
('epoch', 594, 'train_loss:', 2.2683783763647081, 'val_loss:', 1.0196818482875825)
('epoch', 595, 'train_loss:', 2.2633512967824938, 'val_loss:', 1.0191598570346831)
('epoch', 596, 'train_loss:', 2.2649504482746123, 'val_loss:', 1.0196902847290039)
('epoch', 597, 'train_loss:', 2.2589165180921555, 'val_loss:', 1.0204590153694153)
('epoch', 598, 'train_loss:', 2.2619089657068252, 'val_loss:', 1.0268592047691345)
('epoch', 599, 'train_loss:', 2.2552001225948333, 'val_loss:', 1.0221243679523468)
