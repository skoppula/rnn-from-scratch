(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=1 python lstm_all_variants.py -t -d paulg -v additive_no_sigmoid_relu6
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'paulg', 'train': True, 'variant': 'additive_no_sigmoid_relu6', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2676 get requests, put_count=2443 evicted_count=1000 eviction_rate=0.409333 and unsatisfied allocation rate=0.498132
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2755 get requests, put_count=2871 evicted_count=1000 eviction_rate=0.348311 and unsatisfied allocation rate=0.32922
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24354 get requests, put_count=24343 evicted_count=1000 eviction_rate=0.0410796 and unsatisfied allocation rate=0.0439353
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 0, 'train_loss:', 9.5168771338462825, 'val_loss:', 2.1361286187171937)
('epoch', 1, 'train_loss:', 8.2286033439636235, 'val_loss:', 1.9999638605117798)
('epoch', 2, 'train_loss:', 7.8394363355636596, 'val_loss:', 1.9155731225013732)
('epoch', 3, 'train_loss:', 7.4632486701011658, 'val_loss:', 1.8110254287719727)
('epoch', 4, 'train_loss:', 7.0485801005363466, 'val_loss:', 1.7229476642608643)
('epoch', 5, 'train_loss:', 6.7774120497703549, 'val_loss:', 1.6678794860839843)
('epoch', 6, 'train_loss:', 6.5998808312416077, 'val_loss:', 1.6301498270034791)
('epoch', 7, 'train_loss:', 6.4757159280776975, 'val_loss:', 1.6039348554611206)
('epoch', 8, 'train_loss:', 6.3789604306221008, 'val_loss:', 1.5830611300468445)
('epoch', 9, 'train_loss:', 6.3076132345199589, 'val_loss:', 1.5656023621559143)
('epoch', 10, 'train_loss:', 6.2347161698341367, 'val_loss:', 1.5501266479492188)
('epoch', 11, 'train_loss:', 6.1746160864830015, 'val_loss:', 1.5372332954406738)
('epoch', 12, 'train_loss:', 6.117461538314819, 'val_loss:', 1.5210622763633728)
('epoch', 13, 'train_loss:', 6.0616012001037598, 'val_loss:', 1.5088059020042419)
('epoch', 14, 'train_loss:', 6.0089455652236943, 'val_loss:', 1.4943345928192138)
('epoch', 15, 'train_loss:', 5.9531245231628418, 'val_loss:', 1.481139099597931)
('epoch', 16, 'train_loss:', 5.9088031125068667, 'val_loss:', 1.467575478553772)
('epoch', 17, 'train_loss:', 5.8622166180610655, 'val_loss:', 1.4555088543891908)
('epoch', 18, 'train_loss:', 5.8121980690956114, 'val_loss:', 1.4457044696807861)
('epoch', 19, 'train_loss:', 5.7668125939369199, 'val_loss:', 1.434360671043396)
('epoch', 20, 'train_loss:', 5.7294006896018983, 'val_loss:', 1.4236456966400146)
('epoch', 21, 'train_loss:', 5.6885440278053281, 'val_loss:', 1.4147949838638305)
('epoch', 22, 'train_loss:', 5.6505879330635072, 'val_loss:', 1.4079941749572753)
('epoch', 23, 'train_loss:', 5.6148923993110653, 'val_loss:', 1.3969624829292298)
('epoch', 24, 'train_loss:', 5.5721214818954472, 'val_loss:', 1.3875338935852051)
('epoch', 25, 'train_loss:', 5.5461935067176817, 'val_loss:', 1.3791813826560975)
('epoch', 26, 'train_loss:', 5.5075153970718382, 'val_loss:', 1.3715596508979797)
('epoch', 27, 'train_loss:', 5.4734800267219548, 'val_loss:', 1.3626675772666932)
('epoch', 28, 'train_loss:', 5.4445873475074764, 'val_loss:', 1.3544619631767274)
('epoch', 29, 'train_loss:', 5.4188431930541991, 'val_loss:', 1.3467061352729797)
('epoch', 30, 'train_loss:', 5.3804346489906312, 'val_loss:', 1.3378421759605408)
('epoch', 31, 'train_loss:', 5.3515239191055297, 'val_loss:', 1.3327583217620849)
('epoch', 32, 'train_loss:', 5.3203262281417842, 'val_loss:', 1.3248357892036438)
('epoch', 33, 'train_loss:', 5.2936337828636173, 'val_loss:', 1.3178925848007201)
('epoch', 34, 'train_loss:', 5.2587014269828796, 'val_loss:', 1.3117899489402771)
('epoch', 35, 'train_loss:', 5.2342390418052673, 'val_loss:', 1.3049227023124694)
('epoch', 36, 'train_loss:', 5.2100709581375124, 'val_loss:', 1.2982305121421813)
('epoch', 37, 'train_loss:', 5.1869501376152041, 'val_loss:', 1.2915142130851747)
('epoch', 38, 'train_loss:', 5.1600629854202271, 'val_loss:', 1.2840209293365479)
('epoch', 39, 'train_loss:', 5.1314509654045102, 'val_loss:', 1.2799951481819152)
('epoch', 40, 'train_loss:', 5.1071655535697937, 'val_loss:', 1.2692435121536254)
('epoch', 41, 'train_loss:', 5.0838589358329775, 'val_loss:', 1.2688041388988496)
('epoch', 42, 'train_loss:', 5.0607146179676059, 'val_loss:', 1.26102015376091)
('epoch', 43, 'train_loss:', 5.0357406616210936, 'val_loss:', 1.2548179566860198)
('epoch', 44, 'train_loss:', 5.0158150613307955, 'val_loss:', 1.2476232016086579)
('epoch', 45, 'train_loss:', 4.9901342880725856, 'val_loss:', 1.2417587661743164)
('epoch', 46, 'train_loss:', 4.9728499758243565, 'val_loss:', 1.2436832439899446)
('epoch', 47, 'train_loss:', 4.9474652993679049, 'val_loss:', 1.2312307262420654)
('epoch', 48, 'train_loss:', 4.9268391358852384, 'val_loss:', 1.2288916206359863)
('epoch', 49, 'train_loss:', 4.9106607115268703, 'val_loss:', 1.2253174960613251)
('epoch', 50, 'train_loss:', 4.8865301871299742, 'val_loss:', 1.2222743165493011)
('epoch', 51, 'train_loss:', 4.8679459619522092, 'val_loss:', 1.2104183256626129)
('epoch', 52, 'train_loss:', 4.8490303552150724, 'val_loss:', 1.207577772140503)
('epoch', 53, 'train_loss:', 4.83478618144989, 'val_loss:', 1.2028756129741669)
('epoch', 54, 'train_loss:', 4.8067050445079804, 'val_loss:', 1.198014998435974)
('epoch', 55, 'train_loss:', 4.7886698961257936, 'val_loss:', 1.1921538984775544)
('epoch', 56, 'train_loss:', 4.7869274985790256, 'val_loss:', 1.1938593375682831)
('epoch', 57, 'train_loss:', 4.7551830208301542, 'val_loss:', 1.1887307596206664)
('epoch', 58, 'train_loss:', 4.7424491357803342, 'val_loss:', 1.1830341756343841)
('epoch', 59, 'train_loss:', 4.7241239345073698, 'val_loss:', 1.1795190286636352)
('epoch', 60, 'train_loss:', 4.7097714030742646, 'val_loss:', 1.1729174840450287)
('epoch', 61, 'train_loss:', 4.7004115414619445, 'val_loss:', 1.1776071548461915)
('epoch', 62, 'train_loss:', 4.6776487374305722, 'val_loss:', 1.1667326068878174)
('epoch', 63, 'train_loss:', 4.6611305499076847, 'val_loss:', 1.1636234068870543)
('epoch', 64, 'train_loss:', 4.639885900020599, 'val_loss:', 1.1559801852703095)
('epoch', 65, 'train_loss:', 4.6313220715522769, 'val_loss:', 1.1559255111217499)
('epoch', 66, 'train_loss:', 4.618153731822968, 'val_loss:', 1.1494804036617279)
('epoch', 67, 'train_loss:', 4.5948031234741213, 'val_loss:', 1.1489477837085724)
('epoch', 68, 'train_loss:', 4.59214697599411, 'val_loss:', 1.1467096257209777)
('epoch', 69, 'train_loss:', 4.5759406852722169, 'val_loss:', 1.1417945301532746)
('epoch', 70, 'train_loss:', 4.5636597025394439, 'val_loss:', 1.137036908864975)
('epoch', 71, 'train_loss:', 4.5449892139434818, 'val_loss:', 1.1361814022064209)
('epoch', 72, 'train_loss:', 4.5374802434444428, 'val_loss:', 1.1341590750217438)
('epoch', 73, 'train_loss:', 4.5265342414379122, 'val_loss:', 1.1275237190723419)
('epoch', 74, 'train_loss:', 4.5039654064178469, 'val_loss:', 1.1310973751544953)
('epoch', 75, 'train_loss:', 4.4974659943580626, 'val_loss:', 1.1221984803676606)
('epoch', 76, 'train_loss:', 4.4794551873207089, 'val_loss:', 1.1200120937824249)
('epoch', 77, 'train_loss:', 4.4609449088573454, 'val_loss:', 1.1201461374759674)
('epoch', 78, 'train_loss:', 4.457401872873306, 'val_loss:', 1.1145836806297302)
('epoch', 79, 'train_loss:', 4.4499312388896941, 'val_loss:', 1.1108287942409516)
('epoch', 80, 'train_loss:', 4.4259838950633998, 'val_loss:', 1.108229888677597)
('epoch', 81, 'train_loss:', 4.4109429848194122, 'val_loss:', 1.1029011881351471)
('epoch', 82, 'train_loss:', 4.4099286413192749, 'val_loss:', 1.1017590904235839)
('epoch', 83, 'train_loss:', 4.394739134311676, 'val_loss:', 1.1010843956470489)
('epoch', 84, 'train_loss:', 4.3859130072593686, 'val_loss:', 1.0940970909595489)
('epoch', 85, 'train_loss:', 4.3698768794536589, 'val_loss:', 1.0913444924354554)
('epoch', 86, 'train_loss:', 4.3658918702602385, 'val_loss:', 1.0887190198898316)
('epoch', 87, 'train_loss:', 4.3426271069049838, 'val_loss:', 1.0913175868988036)
('epoch', 88, 'train_loss:', 4.3523689639568328, 'val_loss:', 1.0830619728565216)
('epoch', 89, 'train_loss:', 4.3336215734481813, 'val_loss:', 1.0830618155002594)
('epoch', 90, 'train_loss:', 4.3224214208126064, 'val_loss:', 1.0820070588588715)
('epoch', 91, 'train_loss:', 4.3125359165668486, 'val_loss:', 1.0821526920795441)
('epoch', 92, 'train_loss:', 4.2936409687995907, 'val_loss:', 1.0737252140045166)
('epoch', 93, 'train_loss:', 4.2858804571628575, 'val_loss:', 1.069234756231308)
('epoch', 94, 'train_loss:', 4.2775116837024685, 'val_loss:', 1.0694391787052155)
('epoch', 95, 'train_loss:', 4.2695161139965059, 'val_loss:', 1.0707605636119844)
('epoch', 96, 'train_loss:', 4.2667981410026554, 'val_loss:', 1.0704026341438293)
('epoch', 97, 'train_loss:', 4.2580863380432126, 'val_loss:', 1.0651646995544433)
('epoch', 98, 'train_loss:', 4.2489984345436094, 'val_loss:', 1.067133537530899)
('epoch', 99, 'train_loss:', 4.2452274644374848, 'val_loss:', 1.0583773088455199)
('epoch', 100, 'train_loss:', 4.2293183851242064, 'val_loss:', 1.0598790037631989)
('epoch', 101, 'train_loss:', 4.2214450180530552, 'val_loss:', 1.0553521275520326)
('epoch', 102, 'train_loss:', 4.215847630500793, 'val_loss:', 1.0593316841125489)
('epoch', 103, 'train_loss:', 4.2011416912078854, 'val_loss:', 1.0515810966491699)
('epoch', 104, 'train_loss:', 4.1904178774356842, 'val_loss:', 1.0468051004409791)
('epoch', 105, 'train_loss:', 4.1862508308887483, 'val_loss:', 1.0489458179473876)
('epoch', 106, 'train_loss:', 4.1803800165653229, 'val_loss:', 1.0471033895015716)
('epoch', 107, 'train_loss:', 4.1703617811203006, 'val_loss:', 1.0434960365295409)
('epoch', 108, 'train_loss:', 4.1662232553958889, 'val_loss:', 1.0428128743171692)
('epoch', 109, 'train_loss:', 4.1584512889385223, 'val_loss:', 1.0400761282444)
('epoch', 110, 'train_loss:', 4.1474026036262508, 'val_loss:', 1.0392208945751191)
('epoch', 111, 'train_loss:', 4.1413185381889344, 'val_loss:', 1.0355290102958679)
('epoch', 112, 'train_loss:', 4.1331518805027008, 'val_loss:', 1.0379435598850251)
('epoch', 113, 'train_loss:', 4.1274244141578675, 'val_loss:', 1.0377233445644378)
('epoch', 114, 'train_loss:', 4.1215758252143857, 'val_loss:', 1.0308351910114288)
('epoch', 115, 'train_loss:', 4.1145775604248049, 'val_loss:', 1.0318703687191009)
('epoch', 116, 'train_loss:', 4.1110605287551882, 'val_loss:', 1.027643483877182)
('epoch', 117, 'train_loss:', 4.1021897447109223, 'val_loss:', 1.0270365393161773)
('epoch', 118, 'train_loss:', 4.0986765265464786, 'val_loss:', 1.0232867431640624)
('epoch', 119, 'train_loss:', 4.0883197951316834, 'val_loss:', 1.0245980358123778)
('epoch', 120, 'train_loss:', 4.0801960754394528, 'val_loss:', 1.0252793180942534)
('epoch', 121, 'train_loss:', 4.0783760869503025, 'val_loss:', 1.0185334134101867)
('epoch', 122, 'train_loss:', 4.0628836381435391, 'val_loss:', 1.0192143535614013)
('epoch', 123, 'train_loss:', 4.0660029971599583, 'val_loss:', 1.0188519525527955)
('epoch', 124, 'train_loss:', 4.0572470343112945, 'val_loss:', 1.0192445933818817)
('epoch', 125, 'train_loss:', 4.0527843236923218, 'val_loss:', 1.0157474207878112)
('epoch', 126, 'train_loss:', 4.0524977970123288, 'val_loss:', 1.0146195638179778)
('epoch', 127, 'train_loss:', 4.0392381417751313, 'val_loss:', 1.0132952725887299)
('epoch', 128, 'train_loss:', 4.0333021259307857, 'val_loss:', 1.0113250625133514)
('epoch', 129, 'train_loss:', 4.0248613989353181, 'val_loss:', 1.0163372302055358)
('epoch', 130, 'train_loss:', 4.0261796903610225, 'val_loss:', 1.0099589538574218)
('epoch', 131, 'train_loss:', 4.0195623075962068, 'val_loss:', 1.0056251585483551)
('epoch', 132, 'train_loss:', 4.0121878290176394, 'val_loss:', 1.0064550340175629)
('epoch', 133, 'train_loss:', 4.0036052370071413, 'val_loss:', 1.006520448923111)
('epoch', 134, 'train_loss:', 4.0012682759761811, 'val_loss:', 1.0048014605045319)
('epoch', 135, 'train_loss:', 4.0005479645729061, 'val_loss:', 1.0014908492565155)
('epoch', 136, 'train_loss:', 3.9895020866394044, 'val_loss:', 1.000043090581894)
('epoch', 137, 'train_loss:', 3.9824390876293183, 'val_loss:', 0.99857331395149229)
('epoch', 138, 'train_loss:', 3.9728141927719118, 'val_loss:', 0.99982977151870722)
('epoch', 139, 'train_loss:', 3.9789147257804869, 'val_loss:', 0.99871052265167237)
('epoch', 140, 'train_loss:', 3.9691830813884734, 'val_loss:', 0.99483003854751584)
('epoch', 141, 'train_loss:', 3.9648059368133546, 'val_loss:', 0.99220873951911925)
('epoch', 142, 'train_loss:', 3.956247742176056, 'val_loss:', 0.99604138612747195)
('epoch', 143, 'train_loss:', 3.9540509212017061, 'val_loss:', 0.99089860558509824)
('epoch', 144, 'train_loss:', 3.945789531469345, 'val_loss:', 0.99203383445739746)
('epoch', 145, 'train_loss:', 3.943542733192444, 'val_loss:', 0.99187363266944883)
('epoch', 146, 'train_loss:', 3.9436051619052885, 'val_loss:', 0.98904499650001521)
('epoch', 147, 'train_loss:', 3.9353685569763184, 'val_loss:', 0.98756701827049254)
('epoch', 148, 'train_loss:', 3.9323391830921173, 'val_loss:', 0.98690145969390874)
('epoch', 149, 'train_loss:', 3.9339779353141786, 'val_loss:', 0.98691030383110045)
('epoch', 150, 'train_loss:', 3.9251940524578095, 'val_loss:', 0.98908502578735347)
('epoch', 151, 'train_loss:', 3.9171474468708039, 'val_loss:', 0.9827612209320068)
('epoch', 152, 'train_loss:', 3.9087559342384339, 'val_loss:', 0.98172362804412838)
('epoch', 153, 'train_loss:', 3.9096978068351746, 'val_loss:', 0.98162274718284603)
('epoch', 154, 'train_loss:', 3.9053860044479372, 'val_loss:', 0.97989301562309261)
('epoch', 155, 'train_loss:', 3.9047999703884124, 'val_loss:', 0.98016821622848516)
('epoch', 156, 'train_loss:', 3.8940597307682037, 'val_loss:', 0.9797117853164673)
('epoch', 157, 'train_loss:', 3.8971043956279754, 'val_loss:', 0.97565977215766908)
('epoch', 158, 'train_loss:', 3.8924230897426604, 'val_loss:', 0.98010419130325321)
('epoch', 159, 'train_loss:', 3.8893610727787018, 'val_loss:', 0.97841619610786434)
('epoch', 160, 'train_loss:', 3.8845013976097107, 'val_loss:', 0.97522106051445012)
('epoch', 161, 'train_loss:', 3.8755573725700376, 'val_loss:', 0.97537169218063358)
('epoch', 162, 'train_loss:', 3.8755255973339082, 'val_loss:', 0.97281493186950685)
('epoch', 163, 'train_loss:', 3.8626344835758211, 'val_loss:', 0.9711358773708344)
('epoch', 164, 'train_loss:', 3.8707174682617187, 'val_loss:', 0.96942231059074402)
('epoch', 165, 'train_loss:', 3.8660753691196441, 'val_loss:', 0.97288224458694461)
('epoch', 166, 'train_loss:', 3.8506158578395842, 'val_loss:', 0.96880177378654475)
('epoch', 167, 'train_loss:', 3.8567701923847197, 'val_loss:', 0.97017503142356876)
('epoch', 168, 'train_loss:', 3.8529343271255492, 'val_loss:', 0.96625240683555602)
('epoch', 169, 'train_loss:', 3.8504937112331392, 'val_loss:', 0.96910894751548771)
('epoch', 170, 'train_loss:', 3.8510659492015837, 'val_loss:', 0.96618204116821294)
('epoch', 171, 'train_loss:', 3.8426490807533265, 'val_loss:', 0.96361311674118044)
('epoch', 172, 'train_loss:', 3.8350931704044342, 'val_loss:', 0.9715068364143371)
('epoch', 173, 'train_loss:', 3.8324850821495056, 'val_loss:', 0.96116636753082274)
('epoch', 174, 'train_loss:', 3.8281859266757965, 'val_loss:', 0.96452116847038272)
('epoch', 175, 'train_loss:', 3.8246934878826142, 'val_loss:', 0.95972043037414556)
('epoch', 176, 'train_loss:', 3.8221734464168549, 'val_loss:', 0.95950280666351317)
('epoch', 177, 'train_loss:', 3.8153081750869751, 'val_loss:', 0.96078553915023801)
('epoch', 178, 'train_loss:', 3.81811700463295, 'val_loss:', 0.95579952359199527)
('epoch', 179, 'train_loss:', 3.8145747292041778, 'val_loss:', 0.9565520894527435)
('epoch', 180, 'train_loss:', 3.8145932090282439, 'val_loss:', 0.95501436352729796)
('epoch', 181, 'train_loss:', 3.8037722039222719, 'val_loss:', 0.95960195183753971)
('epoch', 182, 'train_loss:', 3.7992924916744233, 'val_loss:', 0.96048711538314824)
('epoch', 183, 'train_loss:', 3.7893744540214538, 'val_loss:', 0.95514901995658874)
('epoch', 184, 'train_loss:', 3.7959640634059908, 'val_loss:', 0.95375551700592043)
('epoch', 185, 'train_loss:', 3.7879953098297121, 'val_loss:', 0.95269521355628972)
('epoch', 186, 'train_loss:', 3.7891689074039459, 'val_loss:', 0.95302050948143002)
('epoch', 187, 'train_loss:', 3.7828028190135954, 'val_loss:', 0.95446372985839845)
('epoch', 188, 'train_loss:', 3.7875445878505705, 'val_loss:', 0.95022145986557005)
('epoch', 189, 'train_loss:', 3.7852644968032836, 'val_loss:', 0.95315748572349546)
('epoch', 190, 'train_loss:', 3.7726513290405275, 'val_loss:', 0.95181054234504703)
('epoch', 191, 'train_loss:', 3.7805592823028564, 'val_loss:', 0.94853398799896238)
('epoch', 192, 'train_loss:', 3.7642732930183409, 'val_loss:', 0.94780349493026739)
('epoch', 193, 'train_loss:', 3.7630682957172392, 'val_loss:', 0.94513175964355467)
('epoch', 194, 'train_loss:', 3.7717183780670167, 'val_loss:', 0.94744054198265071)
('epoch', 195, 'train_loss:', 3.7650835943222045, 'val_loss:', 0.94690123319625852)
('epoch', 196, 'train_loss:', 3.7597418701648713, 'val_loss:', 0.94725671648979182)
('epoch', 197, 'train_loss:', 3.7529535853862761, 'val_loss:', 0.94463784098625181)
('epoch', 198, 'train_loss:', 3.7510932755470274, 'val_loss:', 0.94369907617568971)
('epoch', 199, 'train_loss:', 3.7509529638290404, 'val_loss:', 0.9463180613517761)
('epoch', 200, 'train_loss:', 3.7436440575122831, 'val_loss:', 0.94015377998352045)
('epoch', 201, 'train_loss:', 3.7462157988548279, 'val_loss:', 0.9408962666988373)
('epoch', 202, 'train_loss:', 3.7442591941356658, 'val_loss:', 0.94011902332305908)
('epoch', 203, 'train_loss:', 3.7431891500949859, 'val_loss:', 0.94339093327522283)
('epoch', 204, 'train_loss:', 3.7393326354026795, 'val_loss:', 0.94748823165893559)
('epoch', 205, 'train_loss:', 3.7266292798519136, 'val_loss:', 0.94102772116661071)
('epoch', 206, 'train_loss:', 3.7237958204746247, 'val_loss:', 0.94323332428932194)
('epoch', 207, 'train_loss:', 3.734421148300171, 'val_loss:', 0.93610201001167292)
('epoch', 208, 'train_loss:', 3.7259569883346559, 'val_loss:', 0.93789114117622374)
('epoch', 209, 'train_loss:', 3.7183970141410829, 'val_loss:', 0.93611727237701414)
('epoch', 210, 'train_loss:', 3.7257814812660217, 'val_loss:', 0.93466829776763916)
('epoch', 211, 'train_loss:', 3.7122407209873201, 'val_loss:', 0.93615291357040409)
('epoch', 212, 'train_loss:', 3.7157243061065675, 'val_loss:', 0.93687235832214355)
('epoch', 213, 'train_loss:', 3.7034031510353089, 'val_loss:', 0.93433058381080625)
('epoch', 214, 'train_loss:', 3.7074743831157684, 'val_loss:', 0.93315335869789129)
('epoch', 215, 'train_loss:', 3.7059696900844572, 'val_loss:', 0.93567334890365605)
('epoch', 216, 'train_loss:', 3.7036957061290741, 'val_loss:', 0.93562286257743832)
('epoch', 217, 'train_loss:', 3.7012609231472013, 'val_loss:', 0.93112701058387759)
('epoch', 218, 'train_loss:', 3.6985013365745543, 'val_loss:', 0.93144689321517948)
('epoch', 219, 'train_loss:', 3.696560299396515, 'val_loss:', 0.93010111689567565)
('epoch', 220, 'train_loss:', 3.688697930574417, 'val_loss:', 0.9293253433704376)
('epoch', 221, 'train_loss:', 3.6943210422992707, 'val_loss:', 0.93351860761642458)
('epoch', 222, 'train_loss:', 3.6936985421180726, 'val_loss:', 0.93206120014190674)
('epoch', 223, 'train_loss:', 3.6858983457088472, 'val_loss:', 0.92903728008270259)
('epoch', 224, 'train_loss:', 3.6893539214134217, 'val_loss:', 0.93043540835380556)
('epoch', 225, 'train_loss:', 3.6838878512382509, 'val_loss:', 0.92949945569038395)
('epoch', 226, 'train_loss:', 3.6752788162231447, 'val_loss:', 0.92440381050109866)
('epoch', 227, 'train_loss:', 3.673710916042328, 'val_loss:', 0.9247858679294586)
('epoch', 228, 'train_loss:', 3.6710434949398039, 'val_loss:', 0.92750887513160707)
('epoch', 229, 'train_loss:', 3.6736957800388335, 'val_loss:', 0.92639506697654728)
('epoch', 230, 'train_loss:', 3.664750977754593, 'val_loss:', 0.9238393712043762)
('epoch', 231, 'train_loss:', 3.6661544632911682, 'val_loss:', 0.92455736637115482)
('epoch', 232, 'train_loss:', 3.6655195868015289, 'val_loss:', 0.92624896526336675)
('epoch', 233, 'train_loss:', 3.6600897514820101, 'val_loss:', 0.92390930294990536)
('epoch', 234, 'train_loss:', 3.6632460892200469, 'val_loss:', 0.92275835156440733)
('epoch', 235, 'train_loss:', 3.6616744017601013, 'val_loss:', 0.92486063957214359)
('epoch', 236, 'train_loss:', 3.6532832956314087, 'val_loss:', 0.92272583365440364)
('epoch', 237, 'train_loss:', 3.6514714848995209, 'val_loss:', 0.92237251281738286)
('epoch', 238, 'train_loss:', 3.6507779383659362, 'val_loss:', 0.92268279910087581)
('epoch', 239, 'train_loss:', 3.6485411918163297, 'val_loss:', 0.92335648775100709)
('epoch', 240, 'train_loss:', 3.6463567614555359, 'val_loss:', 0.92002931237220764)
('epoch', 241, 'train_loss:', 3.6440073323249815, 'val_loss:', 0.91518594622612004)
('epoch', 242, 'train_loss:', 3.6405423450469971, 'val_loss:', 0.92017662525177002)
('epoch', 243, 'train_loss:', 3.6459061014652252, 'val_loss:', 0.92009157538413999)
('epoch', 244, 'train_loss:', 3.6433662486076357, 'val_loss:', 0.91762111425399784)
('epoch', 245, 'train_loss:', 3.6390592360496523, 'val_loss:', 0.9166459131240845)
('epoch', 246, 'train_loss:', 3.6379749500751495, 'val_loss:', 0.92041752696037293)
('epoch', 247, 'train_loss:', 3.6328077471256255, 'val_loss:', 0.91834292411804197)
('epoch', 248, 'train_loss:', 3.6379013872146606, 'val_loss:', 0.91709342002868655)
('epoch', 249, 'train_loss:', 3.6267472565174104, 'val_loss:', 0.9186458098888397)
('epoch', 250, 'train_loss:', 3.6307642495632173, 'val_loss:', 0.92042552947998046)
('epoch', 251, 'train_loss:', 3.6256123220920564, 'val_loss:', 0.91352580904960634)
('epoch', 252, 'train_loss:', 3.6257795691490173, 'val_loss:', 0.91248290419578548)
('epoch', 253, 'train_loss:', 3.6221208000183105, 'val_loss:', 0.91567126989364622)
('epoch', 254, 'train_loss:', 3.6124702858924866, 'val_loss:', 0.91311306953430171)
('epoch', 255, 'train_loss:', 3.6212128019332885, 'val_loss:', 0.91312572836875916)
('epoch', 256, 'train_loss:', 3.6218852424621581, 'val_loss:', 0.91762349009513855)
('epoch', 257, 'train_loss:', 3.6195133221149445, 'val_loss:', 0.91026271700859074)
('epoch', 258, 'train_loss:', 3.6146387267112732, 'val_loss:', 0.91125116229057312)
('epoch', 259, 'train_loss:', 3.6099913215637205, 'val_loss:', 0.90872818827629087)
('epoch', 260, 'train_loss:', 3.6108253824710848, 'val_loss:', 0.91318798303604121)
('epoch', 261, 'train_loss:', 3.6089521956443789, 'val_loss:', 0.90809082627296445)
('epoch', 262, 'train_loss:', 3.6083569693565369, 'val_loss:', 0.91058938384056087)
('epoch', 263, 'train_loss:', 3.609927294254303, 'val_loss:', 0.90812240004539491)
('epoch', 264, 'train_loss:', 3.6005942308902741, 'val_loss:', 0.91292318463325506)
('epoch', 265, 'train_loss:', 3.6083371710777281, 'val_loss:', 0.9087679052352905)
('epoch', 266, 'train_loss:', 3.6022988963127136, 'val_loss:', 0.90776732206344601)
('epoch', 267, 'train_loss:', 3.5963567101955416, 'val_loss:', 0.90956127405166631)
('epoch', 268, 'train_loss:', 3.5963509035110475, 'val_loss:', 0.90881230115890499)
('epoch', 269, 'train_loss:', 3.5907084608078002, 'val_loss:', 0.91599993348121644)
('epoch', 270, 'train_loss:', 3.5912524449825285, 'val_loss:', 0.90321510553359985)
('epoch', 271, 'train_loss:', 3.5859682524204253, 'val_loss:', 0.9064165914058685)
('epoch', 272, 'train_loss:', 3.5900108516216278, 'val_loss:', 0.90629514813423162)
('epoch', 273, 'train_loss:', 3.5907557547092437, 'val_loss:', 0.90921472430229189)
('epoch', 274, 'train_loss:', 3.5826128566265107, 'val_loss:', 0.90668765664100648)
('epoch', 275, 'train_loss:', 3.5854850757122039, 'val_loss:', 0.90676039695739741)
('epoch', 276, 'train_loss:', 3.5848055922985078, 'val_loss:', 0.90479986906051635)
('epoch', 277, 'train_loss:', 3.5801942455768585, 'val_loss:', 0.90839568734169007)
('epoch', 278, 'train_loss:', 3.575787799358368, 'val_loss:', 0.90567310452461247)
('epoch', 279, 'train_loss:', 3.5698303580284119, 'val_loss:', 0.91339919567108152)
('epoch', 280, 'train_loss:', 3.5777954375743866, 'val_loss:', 0.90648460745811466)
('epoch', 281, 'train_loss:', 3.5661358058452608, 'val_loss:', 0.90475507140159606)
('epoch', 282, 'train_loss:', 3.5690897989273069, 'val_loss:', 0.90343613743782047)
('epoch', 283, 'train_loss:', 3.569012633562088, 'val_loss:', 0.90614706873893736)
('epoch', 284, 'train_loss:', 3.5668937540054322, 'val_loss:', 0.90813349366188045)
('epoch', 285, 'train_loss:', 3.5717544591426851, 'val_loss:', 0.89914695978164672)
('epoch', 286, 'train_loss:', 3.568246293067932, 'val_loss:', 0.90306379437446593)
('epoch', 287, 'train_loss:', 3.5663895773887635, 'val_loss:', 0.90026192903518676)
('epoch', 288, 'train_loss:', 3.5585812628269196, 'val_loss:', 0.9006036579608917)
('epoch', 289, 'train_loss:', 3.5602407217025758, 'val_loss:', 0.90185870647430422)
('epoch', 290, 'train_loss:', 3.5525004053115845, 'val_loss:', 0.89866459250450137)
('epoch', 291, 'train_loss:', 3.5613629722595217, 'val_loss:', 0.90050936698913575)
('epoch', 292, 'train_loss:', 3.5555029213428497, 'val_loss:', 0.90218286633491518)
('epoch', 293, 'train_loss:', 3.5526111352443697, 'val_loss:', 0.90355938673019409)
('epoch', 294, 'train_loss:', 3.5503296148777008, 'val_loss:', 0.90089317798614499)
('epoch', 295, 'train_loss:', 3.5557798361778259, 'val_loss:', 0.90441109895706173)
('epoch', 296, 'train_loss:', 3.5491745090484619, 'val_loss:', 0.90218391895294192)
('epoch', 297, 'train_loss:', 3.5528596580028533, 'val_loss:', 0.90102989792823796)
('epoch', 298, 'train_loss:', 3.5481252694129943, 'val_loss:', 0.89811615705490111)
('epoch', 299, 'train_loss:', 3.5472341346740723, 'val_loss:', 0.89969239354133601)
('epoch', 300, 'train_loss:', 3.5407700943946838, 'val_loss:', 0.89506492018699646)
('epoch', 301, 'train_loss:', 3.5464533877372744, 'val_loss:', 0.89618969321250919)
('epoch', 302, 'train_loss:', 3.5383017754554746, 'val_loss:', 0.90029372096061711)
('epoch', 303, 'train_loss:', 3.5462652170658111, 'val_loss:', 0.89846090674400325)
('epoch', 304, 'train_loss:', 3.536827368736267, 'val_loss:', 0.89853944778442385)
('epoch', 305, 'train_loss:', 3.5288691794872284, 'val_loss:', 0.89737383246421809)
('epoch', 306, 'train_loss:', 3.5341377830505372, 'val_loss:', 0.89750062346458437)
('epoch', 307, 'train_loss:', 3.5313980484008791, 'val_loss:', 0.89451692819595341)
('epoch', 308, 'train_loss:', 3.5344055581092833, 'val_loss:', 0.89634802103042599)
('epoch', 309, 'train_loss:', 3.525201528072357, 'val_loss:', 0.89741401553153988)
('epoch', 310, 'train_loss:', 3.5282219338417051, 'val_loss:', 0.8967377829551697)
('epoch', 311, 'train_loss:', 3.5341434800624847, 'val_loss:', 0.89346600890159611)
('epoch', 312, 'train_loss:', 3.5288976335525515, 'val_loss:', 0.89526959419250485)
('epoch', 313, 'train_loss:', 3.5235884654521943, 'val_loss:', 0.8959924411773682)
('epoch', 314, 'train_loss:', 3.5219739842414857, 'val_loss:', 0.89657690882682806)
('epoch', 315, 'train_loss:', 3.5245061814785004, 'val_loss:', 0.89063118338584901)
('epoch', 316, 'train_loss:', 3.5164273202419283, 'val_loss:', 0.89617764472961425)
('epoch', 317, 'train_loss:', 3.5171703362464903, 'val_loss:', 0.89217192769050602)
('epoch', 318, 'train_loss:', 3.5202913892269136, 'val_loss:', 0.89301077127456663)
('epoch', 319, 'train_loss:', 3.5146628165245057, 'val_loss:', 0.89381909728050235)
('epoch', 320, 'train_loss:', 3.5141468942165375, 'val_loss:', 0.89412058115005488)
('epoch', 321, 'train_loss:', 3.5093183946609496, 'val_loss:', 0.89396679282188418)
('epoch', 322, 'train_loss:', 3.5178679871559142, 'val_loss:', 0.89170570135116578)
('epoch', 323, 'train_loss:', 3.5072213196754456, 'val_loss:', 0.88891867637634281)
('epoch', 324, 'train_loss:', 3.5158987128734589, 'val_loss:', 0.89456997036933894)
('epoch', 325, 'train_loss:', 3.5161961555480956, 'val_loss:', 0.89210542678833005)
('epoch', 326, 'train_loss:', 3.5071713888645171, 'val_loss:', 0.88980802178382878)
('epoch', 327, 'train_loss:', 3.4999581742286683, 'val_loss:', 0.89051625370979304)
('epoch', 328, 'train_loss:', 3.5055186283588409, 'val_loss:', 0.88847266674041747)
('epoch', 329, 'train_loss:', 3.5033826017379761, 'val_loss:', 0.88872907161712644)
('epoch', 330, 'train_loss:', 3.5041850686073301, 'val_loss:', 0.88752454757690424)
('epoch', 331, 'train_loss:', 3.4982973778247834, 'val_loss:', 0.89131353974342342)
('epoch', 332, 'train_loss:', 3.4969072115421294, 'val_loss:', 0.8885020160675049)
('epoch', 333, 'train_loss:', 3.4978024232387543, 'val_loss:', 0.88732163906097417)
('epoch', 334, 'train_loss:', 3.5037869274616242, 'val_loss:', 0.89022058248519897)
('epoch', 335, 'train_loss:', 3.4904082310199738, 'val_loss:', 0.89119519233703615)
('epoch', 336, 'train_loss:', 3.4923323905467987, 'val_loss:', 0.88627972841262814)
('epoch', 337, 'train_loss:', 3.4884123599529264, 'val_loss:', 0.89056262373924255)
('epoch', 338, 'train_loss:', 3.4903884935379028, 'val_loss:', 0.88740694999694825)
('epoch', 339, 'train_loss:', 3.4915500748157502, 'val_loss:', 0.89304017782211309)
('epoch', 340, 'train_loss:', 3.4932465684413909, 'val_loss:', 0.8885314500331879)
('epoch', 341, 'train_loss:', 3.4906689012050629, 'val_loss:', 0.88422753572463986)
('epoch', 342, 'train_loss:', 3.484235792160034, 'val_loss:', 0.88641049265861516)
('epoch', 343, 'train_loss:', 3.485307478904724, 'val_loss:', 0.88481859087944026)
('epoch', 344, 'train_loss:', 3.4817870438098906, 'val_loss:', 0.88872794628143315)
('epoch', 345, 'train_loss:', 3.4860538911819456, 'val_loss:', 0.8859932732582092)
('epoch', 346, 'train_loss:', 3.4780155026912691, 'val_loss:', 0.88464566826820379)
('epoch', 347, 'train_loss:', 3.4818669390678405, 'val_loss:', 0.88794333338737486)
('epoch', 348, 'train_loss:', 3.4833472526073455, 'val_loss:', 0.88821409583091737)
('epoch', 349, 'train_loss:', 3.4784039759635927, 'val_loss:', 0.88357449889183048)
('epoch', 350, 'train_loss:', 3.4762548589706421, 'val_loss:', 0.88463180661201479)
('epoch', 351, 'train_loss:', 3.4810877418518067, 'val_loss:', 0.88419889688491826)
('epoch', 352, 'train_loss:', 3.4698244249820709, 'val_loss:', 0.88583015561103817)
('epoch', 353, 'train_loss:', 3.4712394857406617, 'val_loss:', 0.88799749612808232)
('epoch', 354, 'train_loss:', 3.4718983948230742, 'val_loss:', 0.8834038901329041)
('epoch', 355, 'train_loss:', 3.4775994122028351, 'val_loss:', 0.87888748049736021)
('epoch', 356, 'train_loss:', 3.4673829996585845, 'val_loss:', 0.88313189744949339)
('epoch', 357, 'train_loss:', 3.4706394088268282, 'val_loss:', 0.8787189650535584)
('epoch', 358, 'train_loss:', 3.4699156928062438, 'val_loss:', 0.8899418020248413)
('epoch', 359, 'train_loss:', 3.4645131015777588, 'val_loss:', 0.88168544411659244)
('epoch', 360, 'train_loss:', 3.4653478968143463, 'val_loss:', 0.88235699415206914)
('epoch', 361, 'train_loss:', 3.4612945652008058, 'val_loss:', 0.8857318615913391)
('epoch', 362, 'train_loss:', 3.462476634979248, 'val_loss:', 0.88404639482498171)
('epoch', 363, 'train_loss:', 3.4576571118831634, 'val_loss:', 0.88254794478416443)
('epoch', 364, 'train_loss:', 3.4564847004413606, 'val_loss:', 0.87973787426948546)
('epoch', 365, 'train_loss:', 3.4596642565727236, 'val_loss:', 0.88148867368698125)
('epoch', 366, 'train_loss:', 3.4640045142173768, 'val_loss:', 0.88275531530380247)
('epoch', 367, 'train_loss:', 3.4641296815872193, 'val_loss:', 0.8803224349021912)
('epoch', 368, 'train_loss:', 3.4573671042919161, 'val_loss:', 0.8785437250137329)
('epoch', 369, 'train_loss:', 3.4576863396167754, 'val_loss:', 0.8786927902698517)
('epoch', 370, 'train_loss:', 3.4556357002258302, 'val_loss:', 0.88223548889160153)
('epoch', 371, 'train_loss:', 3.4568355035781861, 'val_loss:', 0.88059935450553894)
('epoch', 372, 'train_loss:', 3.4544235920906066, 'val_loss:', 0.88118074297904969)
('epoch', 373, 'train_loss:', 3.461003396511078, 'val_loss:', 0.88201735019683836)
('epoch', 374, 'train_loss:', 3.4511993086338042, 'val_loss:', 0.87959159374237061)
('epoch', 375, 'train_loss:', 3.4510926616191866, 'val_loss:', 0.88009329199790953)
('epoch', 376, 'train_loss:', 3.4515257644653321, 'val_loss:', 0.87898876905441281)
('epoch', 377, 'train_loss:', 3.4463685691356658, 'val_loss:', 0.87753760814666748)
('epoch', 378, 'train_loss:', 3.4500303041934965, 'val_loss:', 0.87910093545913692)
('epoch', 379, 'train_loss:', 3.4535326206684114, 'val_loss:', 0.87398056387901302)
('epoch', 380, 'train_loss:', 3.4441788661479951, 'val_loss:', 0.88168658256530763)
('epoch', 381, 'train_loss:', 3.4431598222255708, 'val_loss:', 0.87607064604759222)
('epoch', 382, 'train_loss:', 3.4417182564735413, 'val_loss:', 0.88056598067283631)
('epoch', 383, 'train_loss:', 3.4443691682815554, 'val_loss:', 0.87564352750778196)
('epoch', 384, 'train_loss:', 3.4423370480537416, 'val_loss:', 0.87952724218368528)
('epoch', 385, 'train_loss:', 3.441112457513809, 'val_loss:', 0.87604029536247252)
('epoch', 386, 'train_loss:', 3.4390045964717864, 'val_loss:', 0.87674865841865535)
('epoch', 387, 'train_loss:', 3.4381782984733582, 'val_loss:', 0.876085352897644)
('epoch', 388, 'train_loss:', 3.4357198679447176, 'val_loss:', 0.87676733255386352)
('epoch', 389, 'train_loss:', 3.441565283536911, 'val_loss:', 0.87646107077598567)
('epoch', 390, 'train_loss:', 3.4378165876865387, 'val_loss:', 0.87132673740386968)
('epoch', 391, 'train_loss:', 3.4313104593753816, 'val_loss:', 0.87713186502456664)
('epoch', 392, 'train_loss:', 3.4313679385185241, 'val_loss:', 0.87498873949050904)
('epoch', 393, 'train_loss:', 3.4289092314243317, 'val_loss:', 0.87291317224502563)
('epoch', 394, 'train_loss:', 3.4304959821701049, 'val_loss:', 0.87633387565612797)
('epoch', 395, 'train_loss:', 3.4337360906600951, 'val_loss:', 0.8764371585845947)
('epoch', 396, 'train_loss:', 3.4278696537017823, 'val_loss:', 0.8749830007553101)
('epoch', 397, 'train_loss:', 3.4296965813636779, 'val_loss:', 0.87571606397628787)
('epoch', 398, 'train_loss:', 3.4308635604381563, 'val_loss:', 0.87279592633247371)
('epoch', 399, 'train_loss:', 3.4306883227825167, 'val_loss:', 0.87462075948715212)
('epoch', 400, 'train_loss:', 3.4249517667293548, 'val_loss:', 0.8737607789039612)
('epoch', 401, 'train_loss:', 3.4231951594352723, 'val_loss:', 0.87271756529808042)
('epoch', 402, 'train_loss:', 3.4283277380466459, 'val_loss:', 0.87665575623512271)
('epoch', 403, 'train_loss:', 3.4254772174358368, 'val_loss:', 0.87443497776985168)
('epoch', 404, 'train_loss:', 3.4206914448738099, 'val_loss:', 0.87254088640213012)
('epoch', 405, 'train_loss:', 3.422994763851166, 'val_loss:', 0.8711842942237854)
('epoch', 406, 'train_loss:', 3.4144893574714659, 'val_loss:', 0.87334133982658391)
('epoch', 407, 'train_loss:', 3.4211505985260011, 'val_loss:', 0.87196881771087642)
('epoch', 408, 'train_loss:', 3.4207800674438475, 'val_loss:', 0.87204347610473631)
('epoch', 409, 'train_loss:', 3.416754125356674, 'val_loss:', 0.87046970844268801)
('epoch', 410, 'train_loss:', 3.4136933863162993, 'val_loss:', 0.87250925779342647)
('epoch', 411, 'train_loss:', 3.413232513666153, 'val_loss:', 0.8735824513435364)
('epoch', 412, 'train_loss:', 3.4114255869388579, 'val_loss:', 0.87044161438941958)
('epoch', 413, 'train_loss:', 3.4158459842205047, 'val_loss:', 0.87315994620323178)
('epoch', 414, 'train_loss:', 3.412963180541992, 'val_loss:', 0.87042474508285528)
('epoch', 415, 'train_loss:', 3.4108327448368074, 'val_loss:', 0.86905198454856869)
('epoch', 416, 'train_loss:', 3.4127269685268402, 'val_loss:', 0.87037556409835815)
('epoch', 417, 'train_loss:', 3.408288676738739, 'val_loss:', 0.87185349345207219)
('epoch', 418, 'train_loss:', 3.413284957408905, 'val_loss:', 0.87604833126068116)
('epoch', 419, 'train_loss:', 3.4089067173004151, 'val_loss:', 0.86972285509109493)
('epoch', 420, 'train_loss:', 3.4020693635940553, 'val_loss:', 0.87162895202636714)
('epoch', 421, 'train_loss:', 3.3969596147537233, 'val_loss:', 0.8719358241558075)
('epoch', 422, 'train_loss:', 3.4093400144577028, 'val_loss:', 0.87436633348464965)
('epoch', 423, 'train_loss:', 3.4050326013565062, 'val_loss:', 0.87117131829261785)
('epoch', 424, 'train_loss:', 3.4008277618885039, 'val_loss:', 0.87072606325149537)
('epoch', 425, 'train_loss:', 3.4006371557712556, 'val_loss:', 0.87335322380065916)
('epoch', 426, 'train_loss:', 3.4013005292415617, 'val_loss:', 0.87282788276672363)
('epoch', 427, 'train_loss:', 3.4034695768356324, 'val_loss:', 0.8675962448120117)
('epoch', 428, 'train_loss:', 3.3991053092479704, 'val_loss:', 0.8705684328079224)
('epoch', 429, 'train_loss:', 3.3978453505039217, 'val_loss:', 0.86928823471069339)
('epoch', 430, 'train_loss:', 3.4046759247779845, 'val_loss:', 0.87372040629386905)
('epoch', 431, 'train_loss:', 3.3948850393295289, 'val_loss:', 0.86792940020561216)
('epoch', 432, 'train_loss:', 3.4030197882652282, 'val_loss:', 0.87085479855537418)
('epoch', 433, 'train_loss:', 3.3938178837299349, 'val_loss:', 0.87292219400405879)
('epoch', 434, 'train_loss:', 3.4012385642528535, 'val_loss:', 0.87005385518074041)
('epoch', 435, 'train_loss:', 3.3939091646671296, 'val_loss:', 0.87002749919891353)
('epoch', 436, 'train_loss:', 3.3944693338871001, 'val_loss:', 0.87068763256073001)
('epoch', 437, 'train_loss:', 3.3895209646224975, 'val_loss:', 0.86913088560104368)
('epoch', 438, 'train_loss:', 3.3906406557559965, 'val_loss:', 0.87383032798767091)
('epoch', 439, 'train_loss:', 3.3907964539527895, 'val_loss:', 0.86890824198722838)
('epoch', 440, 'train_loss:', 3.3912677979469299, 'val_loss:', 0.86859299540519719)
('epoch', 441, 'train_loss:', 3.3851074039936067, 'val_loss:', 0.86357681155204769)
('epoch', 442, 'train_loss:', 3.393368738889694, 'val_loss:', 0.87031246662139894)
('epoch', 443, 'train_loss:', 3.3843056702613832, 'val_loss:', 0.86780474901199345)
('epoch', 444, 'train_loss:', 3.3840998327732086, 'val_loss:', 0.86561952829360966)
('epoch', 445, 'train_loss:', 3.3850480258464812, 'val_loss:', 0.86673740506172181)
('epoch', 446, 'train_loss:', 3.387381467819214, 'val_loss:', 0.86851829290390015)
('epoch', 447, 'train_loss:', 3.3848216259479522, 'val_loss:', 0.86302151322364806)
('epoch', 448, 'train_loss:', 3.3789736282825471, 'val_loss:', 0.8671096456050873)
('epoch', 449, 'train_loss:', 3.3805511844158174, 'val_loss:', 0.8648228144645691)
('epoch', 450, 'train_loss:', 3.3875252258777619, 'val_loss:', 0.86672555208206181)
('epoch', 451, 'train_loss:', 3.3791802430152895, 'val_loss:', 0.8649410104751587)
('epoch', 452, 'train_loss:', 3.3816497266292571, 'val_loss:', 0.86315153479576112)
('epoch', 453, 'train_loss:', 3.3851310324668886, 'val_loss:', 0.86756900072097776)
('epoch', 454, 'train_loss:', 3.3790799117088319, 'val_loss:', 0.86566994309425349)
('epoch', 455, 'train_loss:', 3.3763505280017854, 'val_loss:', 0.86738638401031498)
('epoch', 456, 'train_loss:', 3.37732377409935, 'val_loss:', 0.86534754753112797)
('epoch', 457, 'train_loss:', 3.3777795362472536, 'val_loss:', 0.86391706585884098)
('epoch', 458, 'train_loss:', 3.3765361928939819, 'val_loss:', 0.86277793526649471)
('epoch', 459, 'train_loss:', 3.3763237702846527, 'val_loss:', 0.86270252943038939)
('epoch', 460, 'train_loss:', 3.3748584878444672, 'val_loss:', 0.86408657312393189)
('epoch', 461, 'train_loss:', 3.3813239681720733, 'val_loss:', 0.86251257896423339)
('epoch', 462, 'train_loss:', 3.3696788084506988, 'val_loss:', 0.8641715312004089)
('epoch', 463, 'train_loss:', 3.3748180830478667, 'val_loss:', 0.86248321533203121)
('epoch', 464, 'train_loss:', 3.3698425447940825, 'val_loss:', 0.86210504770278928)
('epoch', 465, 'train_loss:', 3.3706245529651642, 'val_loss:', 0.86878444433212276)
('epoch', 466, 'train_loss:', 3.3702734518051147, 'val_loss:', 0.86917351007461552)
('epoch', 467, 'train_loss:', 3.3640528476238249, 'val_loss:', 0.86229333996772761)
('epoch', 468, 'train_loss:', 3.3679879331588745, 'val_loss:', 0.86425484538078312)
('epoch', 469, 'train_loss:', 3.3667662107944487, 'val_loss:', 0.86429853796958922)
('epoch', 470, 'train_loss:', 3.3679314517974852, 'val_loss:', 0.85950549960136413)
('epoch', 471, 'train_loss:', 3.3646103656291961, 'val_loss:', 0.86253368139266973)
('epoch', 472, 'train_loss:', 3.3678920066356657, 'val_loss:', 0.86238074660301212)
('epoch', 473, 'train_loss:', 3.3609694015979765, 'val_loss:', 0.8626332807540894)
('epoch', 474, 'train_loss:', 3.3630998444557192, 'val_loss:', 0.86164525628089905)
('epoch', 475, 'train_loss:', 3.3595898592472078, 'val_loss:', 0.86265013813972469)
('epoch', 476, 'train_loss:', 3.3609385168552399, 'val_loss:', 0.86134485602378841)
('epoch', 477, 'train_loss:', 3.3628057205677031, 'val_loss:', 0.8630941236019134)
('epoch', 478, 'train_loss:', 3.3559787583351137, 'val_loss:', 0.86093430638313295)
('epoch', 479, 'train_loss:', 3.3626242625713347, 'val_loss:', 0.8628762292861939)
('epoch', 480, 'train_loss:', 3.358741385936737, 'val_loss:', 0.85943362236022947)
('epoch', 481, 'train_loss:', 3.3550620234012603, 'val_loss:', 0.86512635707855223)
('epoch', 482, 'train_loss:', 3.3580272853374482, 'val_loss:', 0.85903875231742854)
('epoch', 483, 'train_loss:', 3.3567167782783507, 'val_loss:', 0.8618039298057556)
('epoch', 484, 'train_loss:', 3.3520567274093627, 'val_loss:', 0.85774038314819334)
('epoch', 485, 'train_loss:', 3.3570976090431213, 'val_loss:', 0.86028250455856325)
('epoch', 486, 'train_loss:', 3.356337366104126, 'val_loss:', 0.86071531176567073)
('epoch', 487, 'train_loss:', 3.355005805492401, 'val_loss:', 0.8611743819713592)
('epoch', 488, 'train_loss:', 3.3572368705272675, 'val_loss:', 0.86179273962974545)
('epoch', 489, 'train_loss:', 3.3491274952888488, 'val_loss:', 0.85867316722869869)
('epoch', 490, 'train_loss:', 3.3499513590335845, 'val_loss:', 0.85831735014915467)
('epoch', 491, 'train_loss:', 3.3479850947856904, 'val_loss:', 0.85935690522193908)
('epoch', 492, 'train_loss:', 3.3505379104614259, 'val_loss:', 0.86367874741554262)
('epoch', 493, 'train_loss:', 3.3510373246669771, 'val_loss:', 0.85982615828514097)
('epoch', 494, 'train_loss:', 3.3551743268966674, 'val_loss:', 0.86013978123664858)
('epoch', 495, 'train_loss:', 3.3505499207973481, 'val_loss:', 0.86177649021148683)
('epoch', 496, 'train_loss:', 3.3413431906700133, 'val_loss:', 0.85885246872901921)
('epoch', 497, 'train_loss:', 3.3482380950450898, 'val_loss:', 0.85855757594108584)
('epoch', 498, 'train_loss:', 3.3482253479957582, 'val_loss:', 0.86080599904060362)
('epoch', 499, 'train_loss:', 3.3450858139991761, 'val_loss:', 0.86076359748840336)
('epoch', 500, 'train_loss:', 3.3418910586833954, 'val_loss:', 0.85747038245201113)
('epoch', 501, 'train_loss:', 3.344961817264557, 'val_loss:', 0.86022476553916927)
('epoch', 502, 'train_loss:', 3.3416671049594879, 'val_loss:', 0.86051241159439085)
('epoch', 503, 'train_loss:', 3.3448618245124817, 'val_loss:', 0.85622003316879269)
('epoch', 504, 'train_loss:', 3.3423777675628661, 'val_loss:', 0.8585233652591705)
('epoch', 505, 'train_loss:', 3.3398349833488465, 'val_loss:', 0.85936159014701841)
('epoch', 506, 'train_loss:', 3.3411748254299165, 'val_loss:', 0.8586531841754913)
('epoch', 507, 'train_loss:', 3.3425738942623138, 'val_loss:', 0.86369714498519901)
('epoch', 508, 'train_loss:', 3.3413004064559937, 'val_loss:', 0.85592363238334657)
('epoch', 509, 'train_loss:', 3.337785938978195, 'val_loss:', 0.85715690016746526)
('epoch', 510, 'train_loss:', 3.3374241411685945, 'val_loss:', 0.85795473575592041)
('epoch', 511, 'train_loss:', 3.3401589035987853, 'val_loss:', 0.8590275704860687)
('epoch', 512, 'train_loss:', 3.3378466987609863, 'val_loss:', 0.8570724248886108)
('epoch', 513, 'train_loss:', 3.3414477801322935, 'val_loss:', 0.85975620985031131)
('epoch', 514, 'train_loss:', 3.3309101009368898, 'val_loss:', 0.856231894493103)
('epoch', 515, 'train_loss:', 3.3362670981884004, 'val_loss:', 0.85912719607353205)
('epoch', 516, 'train_loss:', 3.3278158533573152, 'val_loss:', 0.86094202995300295)
('epoch', 517, 'train_loss:', 3.3340177881717681, 'val_loss:', 0.85768246769905088)
('epoch', 518, 'train_loss:', 3.3296888065338135, 'val_loss:', 0.85369214892387391)
('epoch', 519, 'train_loss:', 3.3309159648418425, 'val_loss:', 0.85473668575286865)
('epoch', 520, 'train_loss:', 3.3296636259555816, 'val_loss:', 0.8562285721302032)
('epoch', 521, 'train_loss:', 3.3301697647571564, 'val_loss:', 0.85480693817138675)
('epoch', 522, 'train_loss:', 3.3287911295890806, 'val_loss:', 0.85764352202415461)
('epoch', 523, 'train_loss:', 3.3302294540405275, 'val_loss:', 0.8593747365474701)
('epoch', 524, 'train_loss:', 3.3312813150882721, 'val_loss:', 0.85513365864753721)
('epoch', 525, 'train_loss:', 3.3336295282840727, 'val_loss:', 0.8531305360794067)
('epoch', 526, 'train_loss:', 3.3272346067428589, 'val_loss:', 0.8548774564266205)
('epoch', 527, 'train_loss:', 3.3294035112857818, 'val_loss:', 0.86067825436592105)
('epoch', 528, 'train_loss:', 3.3251083397865298, 'val_loss:', 0.85675834298133846)
('epoch', 529, 'train_loss:', 3.3219919502735138, 'val_loss:', 0.85807277083396916)
('epoch', 530, 'train_loss:', 3.3227907752990724, 'val_loss:', 0.85863899946212774)
('epoch', 531, 'train_loss:', 3.3276657998561858, 'val_loss:', 0.85175518512725834)
('epoch', 532, 'train_loss:', 3.3227980303764344, 'val_loss:', 0.85280692696571347)
('epoch', 533, 'train_loss:', 3.3246043193340302, 'val_loss:', 0.8562437641620636)
('epoch', 534, 'train_loss:', 3.3228653538227082, 'val_loss:', 0.858497211933136)
('epoch', 535, 'train_loss:', 3.3292828524112701, 'val_loss:', 0.85690556764602666)
('epoch', 536, 'train_loss:', 3.3181650924682615, 'val_loss:', 0.85528371453285212)
('epoch', 537, 'train_loss:', 3.3134365606307985, 'val_loss:', 0.8581718945503235)
('epoch', 538, 'train_loss:', 3.3193929100036623, 'val_loss:', 0.85797017216682436)
('epoch', 539, 'train_loss:', 3.3152904570102693, 'val_loss:', 0.85463029146194458)
('epoch', 540, 'train_loss:', 3.3191429042816161, 'val_loss:', 0.85503155350685123)
('epoch', 541, 'train_loss:', 3.3180363070964813, 'val_loss:', 0.8529949140548706)
('epoch', 542, 'train_loss:', 3.3101287531852721, 'val_loss:', 0.85664441704750061)
('epoch', 543, 'train_loss:', 3.3185706007480622, 'val_loss:', 0.85744329929351804)
('epoch', 544, 'train_loss:', 3.3162827253341676, 'val_loss:', 0.85662265777587887)
('epoch', 545, 'train_loss:', 3.3173565948009491, 'val_loss:', 0.85592628240585322)
('epoch', 546, 'train_loss:', 3.3073599636554718, 'val_loss:', 0.85624460220336918)
('epoch', 547, 'train_loss:', 3.3171757113933564, 'val_loss:', 0.85465881705284119)
('epoch', 548, 'train_loss:', 3.3139138972759246, 'val_loss:', 0.85259001493453979)
('epoch', 549, 'train_loss:', 3.3062091660499573, 'val_loss:', 0.85447890877723698)
('epoch', 550, 'train_loss:', 3.3134671413898467, 'val_loss:', 0.85411515831947327)
('epoch', 551, 'train_loss:', 3.3049854218959807, 'val_loss:', 0.85749931216239927)
('epoch', 552, 'train_loss:', 3.3100153446197509, 'val_loss:', 0.85260729551315306)
('epoch', 553, 'train_loss:', 3.3094211781024931, 'val_loss:', 0.85223841309547421)
('epoch', 554, 'train_loss:', 3.3168696093559267, 'val_loss:', 0.85490147352218626)
('epoch', 555, 'train_loss:', 3.3090184569358825, 'val_loss:', 0.85456417679786678)
('epoch', 556, 'train_loss:', 3.3068638730049131, 'val_loss:', 0.85409066796302791)
('epoch', 557, 'train_loss:', 3.3068804061412811, 'val_loss:', 0.85371058344841)
('epoch', 558, 'train_loss:', 3.3095744705200194, 'val_loss:', 0.8529914331436157)
('epoch', 559, 'train_loss:', 3.3069360756874087, 'val_loss:', 0.85649894952774053)
('epoch', 560, 'train_loss:', 3.3039269042015076, 'val_loss:', 0.85082912325859072)
('epoch', 561, 'train_loss:', 3.2935718202590945, 'val_loss:', 0.85107352733612063)
('epoch', 562, 'train_loss:', 3.3041211366653442, 'val_loss:', 0.85315230369567874)
('epoch', 563, 'train_loss:', 3.3037378621101379, 'val_loss:', 0.8522710192203522)
('epoch', 564, 'train_loss:', 3.3080836343765259, 'val_loss:', 0.8553188943862915)
('epoch', 565, 'train_loss:', 3.3073810327053068, 'val_loss:', 0.85197372317314146)
('epoch', 566, 'train_loss:', 3.3027424263954162, 'val_loss:', 0.85196249723434447)
('epoch', 567, 'train_loss:', 3.3046530163288117, 'val_loss:', 0.85055439710617065)
('epoch', 568, 'train_loss:', 3.3018279325962068, 'val_loss:', 0.85296628952026365)
('epoch', 569, 'train_loss:', 3.3007142901420594, 'val_loss:', 0.85889269113540645)
('epoch', 570, 'train_loss:', 3.2968263483047484, 'val_loss:', 0.85062259554862973)
('epoch', 571, 'train_loss:', 3.3035207581520081, 'val_loss:', 0.85036128282546997)
('epoch', 572, 'train_loss:', 3.2968937730789185, 'val_loss:', 0.85154541134834294)
('epoch', 573, 'train_loss:', 3.2978840339183808, 'val_loss:', 0.85002741813659666)
('epoch', 574, 'train_loss:', 3.2966933727264403, 'val_loss:', 0.85428617000579832)
('epoch', 575, 'train_loss:', 3.2958515846729277, 'val_loss:', 0.85018341302871703)
('epoch', 576, 'train_loss:', 3.2941252100467682, 'val_loss:', 0.85099215865135192)
('epoch', 577, 'train_loss:', 3.293465483188629, 'val_loss:', 0.84963706374168391)
('epoch', 578, 'train_loss:', 3.2956065785884858, 'val_loss:', 0.84875756502151489)
('epoch', 579, 'train_loss:', 3.2910980415344238, 'val_loss:', 0.85138707160949711)
('epoch', 580, 'train_loss:', 3.2958364844322205, 'val_loss:', 0.84934061765670776)
('epoch', 581, 'train_loss:', 3.2979287719726562, 'val_loss:', 0.85262981295585627)
('epoch', 582, 'train_loss:', 3.2950713491439818, 'val_loss:', 0.85272398471832278)
('epoch', 583, 'train_loss:', 3.2902840483188629, 'val_loss:', 0.84779838204383851)
('epoch', 584, 'train_loss:', 3.2923014950752258, 'val_loss:', 0.85039293766021729)
('epoch', 585, 'train_loss:', 3.2948979485034942, 'val_loss:', 0.84776242017745973)
('epoch', 586, 'train_loss:', 3.2861292302608489, 'val_loss:', 0.84633899927139278)
('epoch', 587, 'train_loss:', 3.2908429515361788, 'val_loss:', 0.85072951912879946)
('epoch', 588, 'train_loss:', 3.2912092578411101, 'val_loss:', 0.85298810601234432)
('epoch', 589, 'train_loss:', 3.289220082759857, 'val_loss:', 0.84708853006362916)
('epoch', 590, 'train_loss:', 3.2835828971862795, 'val_loss:', 0.85139166712760928)
('epoch', 591, 'train_loss:', 3.2839017617702484, 'val_loss:', 0.84959545731544495)
('epoch', 592, 'train_loss:', 3.2869100332260133, 'val_loss:', 0.84962081670761114)
('epoch', 593, 'train_loss:', 3.2903797161579131, 'val_loss:', 0.84853921055793757)
('epoch', 594, 'train_loss:', 3.2874232518672941, 'val_loss:', 0.85112488269805908)
('epoch', 595, 'train_loss:', 3.280610581636429, 'val_loss:', 0.8486396324634552)
('epoch', 596, 'train_loss:', 3.2817281746864317, 'val_loss:', 0.84928161263465884)
('epoch', 597, 'train_loss:', 3.2837727093696594, 'val_loss:', 0.84702039599418644)
('epoch', 598, 'train_loss:', 3.2833725571632386, 'val_loss:', 0.84893903851509089)
('epoch', 599, 'train_loss:', 3.2856387984752655, 'val_loss:', 0.84792447686195371)
