(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=3 python lstm_all_variants.py -t -d paulg -v additive_no_sigmoid_linear
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'paulg', 'train': True, 'variant': 'additive_no_sigmoid_linear', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:82:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2390 get requests, put_count=2258 evicted_count=1000 eviction_rate=0.44287 and unsatisfied allocation rate=0.515481
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2436 get requests, put_count=2464 evicted_count=1000 eviction_rate=0.405844 and unsatisfied allocation rate=0.408456
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4777 get requests, put_count=4677 evicted_count=1000 eviction_rate=0.213812 and unsatisfied allocation rate=0.242621
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 0, 'train_loss:', 7.875314824581146, 'val_loss:', 1.9208697772026062)
('epoch', 1, 'train_loss:', 7.6380841374397281, 'val_loss:', 1.896616711616516)
('epoch', 2, 'train_loss:', 7.4683688116073608, 'val_loss:', 1.8269411253929138)
('epoch', 3, 'train_loss:', 7.1476985716819765, 'val_loss:', 1.7484649229049682)
('epoch', 4, 'train_loss:', 6.9088903307914737, 'val_loss:', 1.7057200169563294)
('epoch', 5, 'train_loss:', 6.7552053856849668, 'val_loss:', 1.6742940020561219)
('epoch', 6, 'train_loss:', 6.6304293465614315, 'val_loss:', 1.6481198334693909)
('epoch', 7, 'train_loss:', 6.538477041721344, 'val_loss:', 1.626075267791748)
('epoch', 8, 'train_loss:', 6.474863774776459, 'val_loss:', 1.6056515955924988)
('epoch', 9, 'train_loss:', 6.4175490021705626, 'val_loss:', 1.5996092963218689)
('epoch', 10, 'train_loss:', 6.376589040756226, 'val_loss:', 1.5851639413833618)
('epoch', 11, 'train_loss:', 6.3394858074188232, 'val_loss:', 1.5786746478080749)
('epoch', 12, 'train_loss:', 6.3043296408653262, 'val_loss:', 1.5733993721008301)
('epoch', 13, 'train_loss:', 6.2792741417884823, 'val_loss:', 1.5636577391624451)
('epoch', 14, 'train_loss:', 6.2556074357032774, 'val_loss:', 1.5576960825920105)
('epoch', 15, 'train_loss:', 6.2384049010276792, 'val_loss:', 1.5537633585929871)
('epoch', 16, 'train_loss:', 6.2185890221595761, 'val_loss:', 1.5475671792030334)
('epoch', 17, 'train_loss:', 6.2019867587089541, 'val_loss:', 1.5447227048873902)
('epoch', 18, 'train_loss:', 6.1860671234130855, 'val_loss:', 1.546538393497467)
('epoch', 19, 'train_loss:', 6.1728277754783631, 'val_loss:', 1.5405865335464477)
('epoch', 20, 'train_loss:', 6.1541315841674802, 'val_loss:', 1.5359619736671448)
('epoch', 21, 'train_loss:', 6.141615979671478, 'val_loss:', 1.5333465027809143)
('epoch', 22, 'train_loss:', 6.1270681357383729, 'val_loss:', 1.5270945072174071)
('epoch', 23, 'train_loss:', 6.1156063866615291, 'val_loss:', 1.5283867287635804)
('epoch', 24, 'train_loss:', 6.1047270727157592, 'val_loss:', 1.5197068285942077)
('epoch', 25, 'train_loss:', 6.0885057497024535, 'val_loss:', 1.5184923315048218)
('epoch', 26, 'train_loss:', 6.0750133681297305, 'val_loss:', 1.5167513298988342)
('epoch', 27, 'train_loss:', 6.0627604722976685, 'val_loss:', 1.5110598683357239)
('epoch', 28, 'train_loss:', 6.0468300271034243, 'val_loss:', 1.510125560760498)
('epoch', 29, 'train_loss:', 6.0310180878639219, 'val_loss:', 1.5051876735687255)
('epoch', 30, 'train_loss:', 6.020989158153534, 'val_loss:', 1.5008902001380919)
('epoch', 31, 'train_loss:', 5.998257639408112, 'val_loss:', 1.4993077206611634)
('epoch', 32, 'train_loss:', 5.9918203568458557, 'val_loss:', 1.4923494410514833)
('epoch', 33, 'train_loss:', 5.9752840209007267, 'val_loss:', 1.4898215961456298)
('epoch', 34, 'train_loss:', 5.9673812866210936, 'val_loss:', 1.4852146244049071)
('epoch', 35, 'train_loss:', 5.9468595099449155, 'val_loss:', 1.4790161418914796)
('epoch', 36, 'train_loss:', 5.932769362926483, 'val_loss:', 1.4801111316680908)
('epoch', 37, 'train_loss:', 5.9115127110481263, 'val_loss:', 1.4740280938148498)
('epoch', 38, 'train_loss:', 5.9045603799819943, 'val_loss:', 1.4714950156211852)
('epoch', 39, 'train_loss:', 5.8831157040596009, 'val_loss:', 1.4658587789535522)
('epoch', 40, 'train_loss:', 5.8702286958694456, 'val_loss:', 1.4629581427574159)
('epoch', 41, 'train_loss:', 5.8492822885513309, 'val_loss:', 1.4581527638435363)
('epoch', 42, 'train_loss:', 5.8348892068862916, 'val_loss:', 1.4545660972595216)
('epoch', 43, 'train_loss:', 5.823844664096832, 'val_loss:', 1.4536492943763732)
('epoch', 44, 'train_loss:', 5.8084484553337097, 'val_loss:', 1.4479763340950011)
('epoch', 45, 'train_loss:', 5.7949363994598393, 'val_loss:', 1.4460069632530212)
('epoch', 46, 'train_loss:', 5.7799641680717464, 'val_loss:', 1.4434676742553711)
('epoch', 47, 'train_loss:', 5.7658854222297666, 'val_loss:', 1.4399094247817994)
('epoch', 48, 'train_loss:', 5.7562496042251583, 'val_loss:', 1.4357993221282959)
('epoch', 49, 'train_loss:', 5.7498076343536377, 'val_loss:', 1.4325153827667236)
('epoch', 50, 'train_loss:', 5.7334190487861632, 'val_loss:', 1.4278524065017699)
('epoch', 51, 'train_loss:', 5.7213489198684693, 'val_loss:', 1.4272783017158508)
('epoch', 52, 'train_loss:', 5.7063412213325497, 'val_loss:', 1.425957851409912)
('epoch', 53, 'train_loss:', 5.6887979078292847, 'val_loss:', 1.4200783133506776)
('epoch', 54, 'train_loss:', 5.67767205953598, 'val_loss:', 1.4148267531394958)
('epoch', 55, 'train_loss:', 5.6678974008560177, 'val_loss:', 1.4133132600784302)
('epoch', 56, 'train_loss:', 5.6624326777458194, 'val_loss:', 1.4114491152763367)
('epoch', 57, 'train_loss:', 5.6433747458457946, 'val_loss:', 1.4131156325340271)
('epoch', 58, 'train_loss:', 5.6408220195770262, 'val_loss:', 1.4043865537643432)
('epoch', 59, 'train_loss:', 5.6290475702285763, 'val_loss:', 1.404130187034607)
('epoch', 60, 'train_loss:', 5.6191059184074401, 'val_loss:', 1.4016949033737183)
('epoch', 61, 'train_loss:', 5.6118272900581356, 'val_loss:', 1.4015073204040527)
('epoch', 62, 'train_loss:', 5.5982607316970823, 'val_loss:', 1.3947158193588256)
('epoch', 63, 'train_loss:', 5.5833204412460331, 'val_loss:', 1.3914783120155334)
('epoch', 64, 'train_loss:', 5.5739738106727597, 'val_loss:', 1.3975097775459289)
('epoch', 65, 'train_loss:', 5.5623887586593632, 'val_loss:', 1.3877693319320679)
('epoch', 66, 'train_loss:', 5.55553995847702, 'val_loss:', 1.3870430374145508)
('epoch', 67, 'train_loss:', 5.5452909493446354, 'val_loss:', 1.3895731854438782)
('epoch', 68, 'train_loss:', 5.53974728345871, 'val_loss:', 1.3879678845405579)
('epoch', 69, 'train_loss:', 5.5288441348075867, 'val_loss:', 1.3779136443138122)
('epoch', 70, 'train_loss:', 5.5185126399993898, 'val_loss:', 1.3794576930999756)
('epoch', 71, 'train_loss:', 5.5129845547676091, 'val_loss:', 1.3774784731864929)
('epoch', 72, 'train_loss:', 5.5079365015029911, 'val_loss:', 1.3736014556884766)
('epoch', 73, 'train_loss:', 5.4933810210227962, 'val_loss:', 1.3663601517677306)
('epoch', 74, 'train_loss:', 5.4858825850486754, 'val_loss:', 1.3735376262664796)
('epoch', 75, 'train_loss:', 5.4846620440483091, 'val_loss:', 1.3652048373222352)
('epoch', 76, 'train_loss:', 5.4733158087730409, 'val_loss:', 1.3721975851058961)
('epoch', 77, 'train_loss:', 5.4617126250267027, 'val_loss:', 1.3620425724983216)
('epoch', 78, 'train_loss:', 5.4490999460220335, 'val_loss:', 1.3607435345649719)
('epoch', 79, 'train_loss:', 5.445130054950714, 'val_loss:', 1.3607531332969665)
('epoch', 80, 'train_loss:', 5.4339660835266113, 'val_loss:', 1.3594258165359496)
('epoch', 81, 'train_loss:', 5.4296620202064512, 'val_loss:', 1.3539990067481995)
('epoch', 82, 'train_loss:', 5.4163551998138431, 'val_loss:', 1.3506043100357055)
('epoch', 83, 'train_loss:', 5.4085827779769895, 'val_loss:', 1.349703562259674)
('epoch', 84, 'train_loss:', 5.3982822847366334, 'val_loss:', 1.3459627151489257)
('epoch', 85, 'train_loss:', 5.3885406970977785, 'val_loss:', 1.3443686246871949)
('epoch', 86, 'train_loss:', 5.3751194024085995, 'val_loss:', 1.3430591440200805)
('epoch', 87, 'train_loss:', 5.3666489529609684, 'val_loss:', 1.3368734908103943)
('epoch', 88, 'train_loss:', 5.3574570155143739, 'val_loss:', 1.3411110329627991)
('epoch', 89, 'train_loss:', 5.3558162379264829, 'val_loss:', 1.3334550690650939)
('epoch', 90, 'train_loss:', 5.3403905916213992, 'val_loss:', 1.3297319412231445)
('epoch', 91, 'train_loss:', 5.3364774656295779, 'val_loss:', 1.3325056552886962)
('epoch', 92, 'train_loss:', 5.3153681182861332, 'val_loss:', 1.3301626968383788)
('epoch', 93, 'train_loss:', 5.3078540158271785, 'val_loss:', 1.3275552558898926)
('epoch', 94, 'train_loss:', 5.298368937969208, 'val_loss:', 1.3244379997253417)
('epoch', 95, 'train_loss:', 5.2921899461746218, 'val_loss:', 1.321046736240387)
('epoch', 96, 'train_loss:', 5.2764236736297612, 'val_loss:', 1.3187454342842102)
('epoch', 97, 'train_loss:', 5.2679686188697818, 'val_loss:', 1.3128108119964599)
('epoch', 98, 'train_loss:', 5.2597435474395748, 'val_loss:', 1.3164969778060913)
('epoch', 99, 'train_loss:', 5.2504065704345706, 'val_loss:', 1.3100161790847777)
('epoch', 100, 'train_loss:', 5.2403633713722231, 'val_loss:', 1.3122278928756714)
('epoch', 101, 'train_loss:', 5.2350222539901736, 'val_loss:', 1.309184229373932)
('epoch', 102, 'train_loss:', 5.2298065829277043, 'val_loss:', 1.3018583798408507)
('epoch', 103, 'train_loss:', 5.2137993025779723, 'val_loss:', 1.3019167113304138)
('epoch', 104, 'train_loss:', 5.1994552803039547, 'val_loss:', 1.2982954978942871)
('epoch', 105, 'train_loss:', 5.1947540783882138, 'val_loss:', 1.2943427991867065)
('epoch', 106, 'train_loss:', 5.1827447247505187, 'val_loss:', 1.2963883376121521)
('epoch', 107, 'train_loss:', 5.1732320952415467, 'val_loss:', 1.2947690296173096)
('epoch', 108, 'train_loss:', 5.1597732996940611, 'val_loss:', 1.2893432664871216)
('epoch', 109, 'train_loss:', 5.1556328725814815, 'val_loss:', 1.2868670654296874)
('epoch', 110, 'train_loss:', 5.1456556439399721, 'val_loss:', 1.2847029232978822)
('epoch', 111, 'train_loss:', 5.1314488828182219, 'val_loss:', 1.2806417489051818)
('epoch', 112, 'train_loss:', 5.1298064136505124, 'val_loss:', 1.28014244556427)
('epoch', 113, 'train_loss:', 5.1164018082618714, 'val_loss:', 1.2775993490219115)
('epoch', 114, 'train_loss:', 5.1073630166053769, 'val_loss:', 1.2755557894706726)
('epoch', 115, 'train_loss:', 5.0961957800388333, 'val_loss:', 1.274150116443634)
('epoch', 116, 'train_loss:', 5.0952302360534665, 'val_loss:', 1.2705100941658021)
('epoch', 117, 'train_loss:', 5.0784755730628968, 'val_loss:', 1.2698511409759521)
('epoch', 118, 'train_loss:', 5.0777778649330143, 'val_loss:', 1.2642194640636444)
('epoch', 119, 'train_loss:', 5.0552798032760622, 'val_loss:', 1.2624950456619262)
('epoch', 120, 'train_loss:', 5.0472360801696778, 'val_loss:', 1.2625101125240326)
('epoch', 121, 'train_loss:', 5.0393490660190583, 'val_loss:', 1.2594013547897338)
('epoch', 122, 'train_loss:', 5.0347838056087495, 'val_loss:', 1.2542015361785888)
('epoch', 123, 'train_loss:', 5.0306050276756284, 'val_loss:', 1.2554097545146943)
('epoch', 124, 'train_loss:', 5.0193659877777099, 'val_loss:', 1.2549307799339295)
('epoch', 125, 'train_loss:', 5.0075220000743865, 'val_loss:', 1.2522963082790375)
('epoch', 126, 'train_loss:', 5.0037700092792514, 'val_loss:', 1.2520604121685028)
('epoch', 127, 'train_loss:', 4.9967768371105192, 'val_loss:', 1.2470244252681733)
('epoch', 128, 'train_loss:', 4.9908348107337952, 'val_loss:', 1.2453582906723022)
('epoch', 129, 'train_loss:', 4.9734996533393856, 'val_loss:', 1.2475475430488587)
('epoch', 130, 'train_loss:', 4.9687504947185515, 'val_loss:', 1.2430255258083343)
('epoch', 131, 'train_loss:', 4.9579534673690793, 'val_loss:', 1.2381441807746887)
('epoch', 132, 'train_loss:', 4.9505354309082028, 'val_loss:', 1.2374072468280792)
('epoch', 133, 'train_loss:', 4.9347477900981902, 'val_loss:', 1.2347430205345153)
('epoch', 134, 'train_loss:', 4.9313391053676607, 'val_loss:', 1.2317650437355041)
('epoch', 135, 'train_loss:', 4.9241356730461119, 'val_loss:', 1.2275785624980926)
('epoch', 136, 'train_loss:', 4.9124354016780849, 'val_loss:', 1.2268022668361664)
('epoch', 137, 'train_loss:', 4.9076507914066312, 'val_loss:', 1.2267025291919709)
('epoch', 138, 'train_loss:', 4.8995383954048153, 'val_loss:', 1.2230428397655486)
('epoch', 139, 'train_loss:', 4.8872588193416595, 'val_loss:', 1.2167878675460815)
('epoch', 140, 'train_loss:', 4.8784306108951565, 'val_loss:', 1.220774563550949)
('epoch', 141, 'train_loss:', 4.8689315271377565, 'val_loss:', 1.2173980700969695)
('epoch', 142, 'train_loss:', 4.8613430964946751, 'val_loss:', 1.2142108297348022)
('epoch', 143, 'train_loss:', 4.8572083497047425, 'val_loss:', 1.2098829495906829)
('epoch', 144, 'train_loss:', 4.8464915525913241, 'val_loss:', 1.2113804757595061)
('epoch', 145, 'train_loss:', 4.8377968561649318, 'val_loss:', 1.2082812845706941)
('epoch', 146, 'train_loss:', 4.8340705788135532, 'val_loss:', 1.2070052790641785)
('epoch', 147, 'train_loss:', 4.8237743473052976, 'val_loss:', 1.2037021958827971)
('epoch', 148, 'train_loss:', 4.8159411668777468, 'val_loss:', 1.2029157507419586)
('epoch', 149, 'train_loss:', 4.8075314879417421, 'val_loss:', 1.20266184091568)
('epoch', 150, 'train_loss:', 4.800127464532852, 'val_loss:', 1.1979896676540376)
('epoch', 151, 'train_loss:', 4.7909313702583312, 'val_loss:', 1.1988009130954742)
('epoch', 152, 'train_loss:', 4.78241693854332, 'val_loss:', 1.1935700690746307)
('epoch', 153, 'train_loss:', 4.7752873480319975, 'val_loss:', 1.1950544238090515)
('epoch', 154, 'train_loss:', 4.7700693166255954, 'val_loss:', 1.1905277061462403)
('epoch', 155, 'train_loss:', 4.7591330063343049, 'val_loss:', 1.1912004673480987)
('epoch', 156, 'train_loss:', 4.7497129750251768, 'val_loss:', 1.1842908895015716)
('epoch', 157, 'train_loss:', 4.7484182405471804, 'val_loss:', 1.1843540847301484)
('epoch', 158, 'train_loss:', 4.7392726755142212, 'val_loss:', 1.1830461645126342)
('epoch', 159, 'train_loss:', 4.7314169180393222, 'val_loss:', 1.1841163253784179)
('epoch', 160, 'train_loss:', 4.7274407958984375, 'val_loss:', 1.1807196342945099)
('epoch', 161, 'train_loss:', 4.7202034139633176, 'val_loss:', 1.1800744318962098)
('epoch', 162, 'train_loss:', 4.7102180194854739, 'val_loss:', 1.179771820306778)
('epoch', 163, 'train_loss:', 4.7044720137119294, 'val_loss:', 1.1766867268085479)
('epoch', 164, 'train_loss:', 4.6970001208782195, 'val_loss:', 1.1776564121246338)
('epoch', 165, 'train_loss:', 4.6901464891433715, 'val_loss:', 1.1711978220939636)
('epoch', 166, 'train_loss:', 4.6878124403953549, 'val_loss:', 1.1694157612323761)
('epoch', 167, 'train_loss:', 4.6749269342422481, 'val_loss:', 1.1685976707935333)
('epoch', 168, 'train_loss:', 4.6690078151226047, 'val_loss:', 1.1654580712318421)
('epoch', 169, 'train_loss:', 4.6651366341114047, 'val_loss:', 1.1665097045898438)
('epoch', 170, 'train_loss:', 4.6595606350898739, 'val_loss:', 1.1635154259204865)
('epoch', 171, 'train_loss:', 4.6475554454326629, 'val_loss:', 1.1645215678215026)
('epoch', 172, 'train_loss:', 4.6423341894149779, 'val_loss:', 1.1593084537982941)
('epoch', 173, 'train_loss:', 4.6346868872642517, 'val_loss:', 1.1607433199882506)
('epoch', 174, 'train_loss:', 4.6343755829334263, 'val_loss:', 1.15573983669281)
('epoch', 175, 'train_loss:', 4.6258421909809115, 'val_loss:', 1.15810995221138)
('epoch', 176, 'train_loss:', 4.6129962968826295, 'val_loss:', 1.1548065114021302)
('epoch', 177, 'train_loss:', 4.611142182350159, 'val_loss:', 1.1514868831634522)
('epoch', 178, 'train_loss:', 4.6036149430274964, 'val_loss:', 1.1541765999794007)
('epoch', 179, 'train_loss:', 4.6015231692790985, 'val_loss:', 1.1507683777809143)
('epoch', 180, 'train_loss:', 4.593017965555191, 'val_loss:', 1.1493133258819581)
('epoch', 181, 'train_loss:', 4.5843208920955654, 'val_loss:', 1.1450062704086303)
('epoch', 182, 'train_loss:', 4.575325478315353, 'val_loss:', 1.1482225644588471)
('epoch', 183, 'train_loss:', 4.5657332575321199, 'val_loss:', 1.1436256515979766)
('epoch', 184, 'train_loss:', 4.569536737203598, 'val_loss:', 1.1425992345809937)
('epoch', 185, 'train_loss:', 4.5630140173435212, 'val_loss:', 1.1433541262149811)
('epoch', 186, 'train_loss:', 4.5521715617179872, 'val_loss:', 1.1393445956707)
('epoch', 187, 'train_loss:', 4.5482512450218202, 'val_loss:', 1.1377450871467589)
('epoch', 188, 'train_loss:', 4.5488386440277102, 'val_loss:', 1.1367768025398255)
('epoch', 189, 'train_loss:', 4.5351173639297482, 'val_loss:', 1.1348122203350066)
('epoch', 190, 'train_loss:', 4.5350003004074093, 'val_loss:', 1.1366345810890197)
('epoch', 191, 'train_loss:', 4.5286632573604582, 'val_loss:', 1.1326678347587587)
('epoch', 192, 'train_loss:', 4.5247207462787626, 'val_loss:', 1.131988822221756)
('epoch', 193, 'train_loss:', 4.5105479240417479, 'val_loss:', 1.1319518411159515)
('epoch', 194, 'train_loss:', 4.5063841807842255, 'val_loss:', 1.1286163568496703)
('epoch', 195, 'train_loss:', 4.5075817358493806, 'val_loss:', 1.1248591601848603)
('epoch', 196, 'train_loss:', 4.4966764020919801, 'val_loss:', 1.1243056297302245)
('epoch', 197, 'train_loss:', 4.4911478900909421, 'val_loss:', 1.1235112690925597)
('epoch', 198, 'train_loss:', 4.4857343888282779, 'val_loss:', 1.1245956218242645)
('epoch', 199, 'train_loss:', 4.4738792610168456, 'val_loss:', 1.1221819424629211)
('epoch', 200, 'train_loss:', 4.4712875235080718, 'val_loss:', 1.11847727060318)
('epoch', 201, 'train_loss:', 4.4690793859958653, 'val_loss:', 1.1168701314926148)
('epoch', 202, 'train_loss:', 4.4662375950813296, 'val_loss:', 1.116818175315857)
('epoch', 203, 'train_loss:', 4.45515349984169, 'val_loss:', 1.1112598717212676)
('epoch', 204, 'train_loss:', 4.4508999824523929, 'val_loss:', 1.1135424971580505)
('epoch', 205, 'train_loss:', 4.442284624576569, 'val_loss:', 1.1164558291435243)
('epoch', 206, 'train_loss:', 4.4429307806491849, 'val_loss:', 1.111072324514389)
('epoch', 207, 'train_loss:', 4.4348241889476778, 'val_loss:', 1.1088030326366425)
('epoch', 208, 'train_loss:', 4.4274304711818697, 'val_loss:', 1.1074163973331452)
('epoch', 209, 'train_loss:', 4.4225816273689267, 'val_loss:', 1.1060222208499908)
('epoch', 210, 'train_loss:', 4.423073045015335, 'val_loss:', 1.1055597400665282)
('epoch', 211, 'train_loss:', 4.4184700393676755, 'val_loss:', 1.1030541026592255)
('epoch', 212, 'train_loss:', 4.4107016670703887, 'val_loss:', 1.1036196088790893)
('epoch', 213, 'train_loss:', 4.4037514376640319, 'val_loss:', 1.1039035499095917)
('epoch', 214, 'train_loss:', 4.3989740622043607, 'val_loss:', 1.0995637524127959)
('epoch', 215, 'train_loss:', 4.3902886331081392, 'val_loss:', 1.1001083421707154)
('epoch', 216, 'train_loss:', 4.388548964262009, 'val_loss:', 1.1008704721927642)
('epoch', 217, 'train_loss:', 4.3866666460037234, 'val_loss:', 1.0969153869152068)
('epoch', 218, 'train_loss:', 4.3722094881534579, 'val_loss:', 1.0965433180332185)
('epoch', 219, 'train_loss:', 4.3777560937404632, 'val_loss:', 1.0961952126026153)
('epoch', 220, 'train_loss:', 4.3697295606136324, 'val_loss:', 1.0896016037464142)
('epoch', 221, 'train_loss:', 4.3640821075439451, 'val_loss:', 1.0895309090614318)
('epoch', 222, 'train_loss:', 4.3573805141448974, 'val_loss:', 1.0928917169570922)
('epoch', 223, 'train_loss:', 4.3496649932861331, 'val_loss:', 1.0886490476131438)
('epoch', 224, 'train_loss:', 4.3461920535564422, 'val_loss:', 1.0897677266597747)
('epoch', 225, 'train_loss:', 4.3388795053958891, 'val_loss:', 1.0869397675991059)
('epoch', 226, 'train_loss:', 4.3439225566387174, 'val_loss:', 1.0872523760795594)
('epoch', 227, 'train_loss:', 4.33559093952179, 'val_loss:', 1.0864202749729157)
('epoch', 228, 'train_loss:', 4.3218768930435179, 'val_loss:', 1.0859540629386901)
('epoch', 229, 'train_loss:', 4.3220963478088379, 'val_loss:', 1.0795355451107025)
('epoch', 230, 'train_loss:', 4.3170406651496886, 'val_loss:', 1.0840233552455902)
('epoch', 231, 'train_loss:', 4.3123689901828763, 'val_loss:', 1.0777475440502167)
('epoch', 232, 'train_loss:', 4.3053632092475889, 'val_loss:', 1.0783156490325927)
('epoch', 233, 'train_loss:', 4.3077275371551513, 'val_loss:', 1.0755000746250152)
('epoch', 234, 'train_loss:', 4.3016794681549069, 'val_loss:', 1.0767042803764344)
('epoch', 235, 'train_loss:', 4.2975252068042753, 'val_loss:', 1.0751863431930542)
('epoch', 236, 'train_loss:', 4.2970740056037906, 'val_loss:', 1.0729191100597382)
('epoch', 237, 'train_loss:', 4.2876088845729825, 'val_loss:', 1.073647404909134)
('epoch', 238, 'train_loss:', 4.2780494070053097, 'val_loss:', 1.0711613202095032)
('epoch', 239, 'train_loss:', 4.2807155513763426, 'val_loss:', 1.0715878593921662)
('epoch', 240, 'train_loss:', 4.2692701900005341, 'val_loss:', 1.0703529179096223)
('epoch', 241, 'train_loss:', 4.2757809138298031, 'val_loss:', 1.0723165464401245)
('epoch', 242, 'train_loss:', 4.2594697630405429, 'val_loss:', 1.0719777584075927)
('epoch', 243, 'train_loss:', 4.2640857756137844, 'val_loss:', 1.0666030323505402)
('epoch', 244, 'train_loss:', 4.2537618851661678, 'val_loss:', 1.0666991007328033)
('epoch', 245, 'train_loss:', 4.2536578536033627, 'val_loss:', 1.0655019307136535)
('epoch', 246, 'train_loss:', 4.2535024034976958, 'val_loss:', 1.0661037003993987)
('epoch', 247, 'train_loss:', 4.2469279587268831, 'val_loss:', 1.0620987451076507)
('epoch', 248, 'train_loss:', 4.2413687086105343, 'val_loss:', 1.0625560796260833)
('epoch', 249, 'train_loss:', 4.2322130262851712, 'val_loss:', 1.0620607924461365)
('epoch', 250, 'train_loss:', 4.2408438432216649, 'val_loss:', 1.0600239765644073)
('epoch', 251, 'train_loss:', 4.2288977003097532, 'val_loss:', 1.0622734010219574)
('epoch', 252, 'train_loss:', 4.2221370995044705, 'val_loss:', 1.0572338044643401)
('epoch', 253, 'train_loss:', 4.2205108761787411, 'val_loss:', 1.0568023872375489)
('epoch', 254, 'train_loss:', 4.2163276970386505, 'val_loss:', 1.0571424460411072)
('epoch', 255, 'train_loss:', 4.2168295335769654, 'val_loss:', 1.0555995285511017)
('epoch', 256, 'train_loss:', 4.2127883815765381, 'val_loss:', 1.0533512234687805)
('epoch', 257, 'train_loss:', 4.2040407466888432, 'val_loss:', 1.0539220845699311)
('epoch', 258, 'train_loss:', 4.1985049116611481, 'val_loss:', 1.0562563371658324)
('epoch', 259, 'train_loss:', 4.1974142587184904, 'val_loss:', 1.053538247346878)
('epoch', 260, 'train_loss:', 4.1886905789375302, 'val_loss:', 1.0500731539726258)
('epoch', 261, 'train_loss:', 4.1946958553791047, 'val_loss:', 1.0476435613632202)
('epoch', 262, 'train_loss:', 4.1879550480842589, 'val_loss:', 1.0505382180213929)
('epoch', 263, 'train_loss:', 4.1817207920551303, 'val_loss:', 1.048117115497589)
('epoch', 264, 'train_loss:', 4.1799202144145964, 'val_loss:', 1.0474015057086945)
('epoch', 265, 'train_loss:', 4.1754904687404633, 'val_loss:', 1.0476078712940216)
('epoch', 266, 'train_loss:', 4.1744435167312623, 'val_loss:', 1.0467750716209412)
('epoch', 267, 'train_loss:', 4.174694553613663, 'val_loss:', 1.0479402673244476)
('epoch', 268, 'train_loss:', 4.1651952099800109, 'val_loss:', 1.044005970954895)
('epoch', 269, 'train_loss:', 4.1616379284858702, 'val_loss:', 1.0431560957431794)
('epoch', 270, 'train_loss:', 4.1599074149131772, 'val_loss:', 1.0421796119213105)
('epoch', 271, 'train_loss:', 4.1504195928573608, 'val_loss:', 1.0428274476528168)
('epoch', 272, 'train_loss:', 4.1477697300910954, 'val_loss:', 1.0421425223350524)
('epoch', 273, 'train_loss:', 4.1463293075561527, 'val_loss:', 1.0413322985172271)
('epoch', 274, 'train_loss:', 4.1429998314380647, 'val_loss:', 1.0374259257316589)
('epoch', 275, 'train_loss:', 4.1464077484607698, 'val_loss:', 1.0376166307926178)
('epoch', 276, 'train_loss:', 4.1398756492137911, 'val_loss:', 1.0341147756576539)
('epoch', 277, 'train_loss:', 4.1290058803558347, 'val_loss:', 1.0418483912944794)
('epoch', 278, 'train_loss:', 4.1313197457790372, 'val_loss:', 1.037433215379715)
('epoch', 279, 'train_loss:', 4.1259247541427611, 'val_loss:', 1.0368303096294402)
('epoch', 280, 'train_loss:', 4.1234738516807559, 'val_loss:', 1.0356524956226349)
('epoch', 281, 'train_loss:', 4.1234331297874451, 'val_loss:', 1.0312227189540863)
('epoch', 282, 'train_loss:', 4.1189099144935604, 'val_loss:', 1.0319080805778504)
('epoch', 283, 'train_loss:', 4.1129944062232973, 'val_loss:', 1.034055576324463)
('epoch', 284, 'train_loss:', 4.1097279500961301, 'val_loss:', 1.0272261285781861)
('epoch', 285, 'train_loss:', 4.108189902305603, 'val_loss:', 1.0338906097412108)
('epoch', 286, 'train_loss:', 4.1063398551940917, 'val_loss:', 1.027653592824936)
('epoch', 287, 'train_loss:', 4.099289177656174, 'val_loss:', 1.0296994233131409)
('epoch', 288, 'train_loss:', 4.0957406270504002, 'val_loss:', 1.027327766418457)
('epoch', 289, 'train_loss:', 4.0940197229385378, 'val_loss:', 1.0269840121269227)
('epoch', 290, 'train_loss:', 4.0956329905986788, 'val_loss:', 1.0305614590644836)
('epoch', 291, 'train_loss:', 4.0932651901245114, 'val_loss:', 1.0263612127304078)
('epoch', 292, 'train_loss:', 4.0887119090557098, 'val_loss:', 1.0234541475772858)
('epoch', 293, 'train_loss:', 4.0865217804908749, 'val_loss:', 1.0255687928199768)
('epoch', 294, 'train_loss:', 4.0805937099456786, 'val_loss:', 1.0237360358238221)
('epoch', 295, 'train_loss:', 4.0787312626838688, 'val_loss:', 1.0221596336364747)
('epoch', 296, 'train_loss:', 4.0695838832855227, 'val_loss:', 1.0227324616909028)
('epoch', 297, 'train_loss:', 4.0722015750408174, 'val_loss:', 1.0211012947559357)
('epoch', 298, 'train_loss:', 4.0668472504615787, 'val_loss:', 1.019972026348114)
('epoch', 299, 'train_loss:', 4.0586339521408084, 'val_loss:', 1.0222768032550811)
('epoch', 300, 'train_loss:', 4.0639237987995145, 'val_loss:', 1.022589055299759)
('epoch', 301, 'train_loss:', 4.063180545568466, 'val_loss:', 1.019547986984253)
('epoch', 302, 'train_loss:', 4.0529403734207152, 'val_loss:', 1.0155206847190856)
('epoch', 303, 'train_loss:', 4.0496708071231842, 'val_loss:', 1.0161862802505492)
('epoch', 304, 'train_loss:', 4.048123826980591, 'val_loss:', 1.019559873342514)
('epoch', 305, 'train_loss:', 4.0434479439258579, 'val_loss:', 1.0135247206687927)
('epoch', 306, 'train_loss:', 4.0413773322105406, 'val_loss:', 1.0147013926506043)
('epoch', 307, 'train_loss:', 4.0407949137687682, 'val_loss:', 1.0123905348777771)
('epoch', 308, 'train_loss:', 4.043166545629501, 'val_loss:', 1.0129345214366914)
('epoch', 309, 'train_loss:', 4.0361154961586001, 'val_loss:', 1.0147425746917724)
('epoch', 310, 'train_loss:', 4.0326893174648282, 'val_loss:', 1.0130742406845092)
('epoch', 311, 'train_loss:', 4.0293926537036899, 'val_loss:', 1.0087745189666748)
('epoch', 312, 'train_loss:', 4.0327247071266177, 'val_loss:', 1.0136928153038025)
('epoch', 313, 'train_loss:', 4.0262380135059352, 'val_loss:', 1.0066862988471985)
('epoch', 314, 'train_loss:', 4.0240465891361232, 'val_loss:', 1.0108675730228425)
('epoch', 315, 'train_loss:', 4.0181086659431458, 'val_loss:', 1.0089049017429352)
('epoch', 316, 'train_loss:', 4.0126620459556577, 'val_loss:', 1.007226368188858)
('epoch', 317, 'train_loss:', 4.0118120276927947, 'val_loss:', 1.009176162481308)
('epoch', 318, 'train_loss:', 4.0067017853260039, 'val_loss:', 1.0061145401000977)
('epoch', 319, 'train_loss:', 4.0074707579612729, 'val_loss:', 1.0078105878829957)
('epoch', 320, 'train_loss:', 4.0000635695457456, 'val_loss:', 1.0027720987796784)
('epoch', 321, 'train_loss:', 4.0027674221992493, 'val_loss:', 1.0059517788887025)
('epoch', 322, 'train_loss:', 4.0027276921272277, 'val_loss:', 1.0074056577682495)
('epoch', 323, 'train_loss:', 3.9979717803001402, 'val_loss:', 1.0072316515445709)
('epoch', 324, 'train_loss:', 3.9960097634792326, 'val_loss:', 1.0026687359809876)
('epoch', 325, 'train_loss:', 3.9892311871051787, 'val_loss:', 1.0039659607410432)
('epoch', 326, 'train_loss:', 3.9841888272762298, 'val_loss:', 1.002078549861908)
('epoch', 327, 'train_loss:', 3.9881721770763399, 'val_loss:', 1.0012342154979705)
('epoch', 328, 'train_loss:', 3.9811914026737214, 'val_loss:', 0.99894846916198732)
('epoch', 329, 'train_loss:', 3.9775546586513517, 'val_loss:', 0.99813606739044192)
('epoch', 330, 'train_loss:', 3.9757627260684969, 'val_loss:', 1.0016639280319213)
('epoch', 331, 'train_loss:', 3.9772685837745665, 'val_loss:', 0.99929942846298214)
('epoch', 332, 'train_loss:', 3.9688486456871033, 'val_loss:', 0.99910420775413511)
('epoch', 333, 'train_loss:', 3.962899330854416, 'val_loss:', 1.0015040016174317)
('epoch', 334, 'train_loss:', 3.9704622662067415, 'val_loss:', 0.99871482372283937)
('epoch', 335, 'train_loss:', 3.9666730284690859, 'val_loss:', 0.99317447185516361)
('epoch', 336, 'train_loss:', 3.9648540532588958, 'val_loss:', 0.99414244413375852)
('epoch', 337, 'train_loss:', 3.9567606508731843, 'val_loss:', 0.99587503910064701)
('epoch', 338, 'train_loss:', 3.9591471040248871, 'val_loss:', 0.99419181704521176)
('epoch', 339, 'train_loss:', 3.9536364352703095, 'val_loss:', 0.99058067798614502)
('epoch', 340, 'train_loss:', 3.9519124603271485, 'val_loss:', 0.99414943695068358)
('epoch', 341, 'train_loss:', 3.952245181798935, 'val_loss:', 0.99378648161888128)
('epoch', 342, 'train_loss:', 3.9481629824638365, 'val_loss:', 0.98924074411392215)
('epoch', 343, 'train_loss:', 3.9468151056766509, 'val_loss:', 0.99310295701026918)
('epoch', 344, 'train_loss:', 3.9393930435180664, 'val_loss:', 0.99482737779617314)
('epoch', 345, 'train_loss:', 3.9339779055118562, 'val_loss:', 0.99022644877433774)
('epoch', 346, 'train_loss:', 3.9356947839260101, 'val_loss:', 0.99361047744750974)
('epoch', 347, 'train_loss:', 3.9379912936687469, 'val_loss:', 0.98944370269775395)
('epoch', 348, 'train_loss:', 3.9392643094062807, 'val_loss:', 0.98896777272224423)
('epoch', 349, 'train_loss:', 3.928853952884674, 'val_loss:', 0.98803385496139529)
('epoch', 350, 'train_loss:', 3.927076585292816, 'val_loss:', 0.98656260609626767)
('epoch', 351, 'train_loss:', 3.9296749341487884, 'val_loss:', 0.98600432038307195)
('epoch', 352, 'train_loss:', 3.9285893881320955, 'val_loss:', 0.98642290234565733)
('epoch', 353, 'train_loss:', 3.9235894787311554, 'val_loss:', 0.99022167086601254)
('epoch', 354, 'train_loss:', 3.9165001952648164, 'val_loss:', 0.98095089197158813)
('epoch', 355, 'train_loss:', 3.9110964834690094, 'val_loss:', 0.98267317175865176)
('epoch', 356, 'train_loss:', 3.9165608465671538, 'val_loss:', 0.98673481464385981)
('epoch', 357, 'train_loss:', 3.9111209058761598, 'val_loss:', 0.98448991537094122)
('epoch', 358, 'train_loss:', 3.9108327937126162, 'val_loss:', 0.98535351157188411)
('epoch', 359, 'train_loss:', 3.9137915396690368, 'val_loss:', 0.98082067370414738)
('epoch', 360, 'train_loss:', 3.9079167401790618, 'val_loss:', 0.98138941645622257)
('epoch', 361, 'train_loss:', 3.9102715897560119, 'val_loss:', 0.98031257510185243)
('epoch', 362, 'train_loss:', 3.9036934101581573, 'val_loss:', 0.98090867877006527)
('epoch', 363, 'train_loss:', 3.894319257736206, 'val_loss:', 0.97743387460708619)
('epoch', 364, 'train_loss:', 3.8897035968303681, 'val_loss:', 0.9824052858352661)
('epoch', 365, 'train_loss:', 3.8960049009323119, 'val_loss:', 0.9803560137748718)
('epoch', 366, 'train_loss:', 3.8885933291912078, 'val_loss:', 0.97817926049232484)
('epoch', 367, 'train_loss:', 3.8870653498172758, 'val_loss:', 0.98159543633460999)
('epoch', 368, 'train_loss:', 3.8858127713203432, 'val_loss:', 0.98048799276351928)
('epoch', 369, 'train_loss:', 3.8874125099182129, 'val_loss:', 0.97802830815315245)
('epoch', 370, 'train_loss:', 3.8838667356967926, 'val_loss:', 0.97784964919090267)
('epoch', 371, 'train_loss:', 3.8795209085941313, 'val_loss:', 0.97875753164291379)
('epoch', 372, 'train_loss:', 3.8754906785488128, 'val_loss:', 0.97692260026931765)
('epoch', 373, 'train_loss:', 3.8797843980789186, 'val_loss:', 0.97590600371360781)
('epoch', 374, 'train_loss:', 3.8723879837989807, 'val_loss:', 0.97495274662971498)
('epoch', 375, 'train_loss:', 3.8766613745689393, 'val_loss:', 0.97513474464416505)
('epoch', 376, 'train_loss:', 3.8770582389831545, 'val_loss:', 0.97630415439605711)
('epoch', 377, 'train_loss:', 3.8683464050292971, 'val_loss:', 0.97221782326698303)
('epoch', 378, 'train_loss:', 3.8651967203617095, 'val_loss:', 0.97686719655990606)
('epoch', 379, 'train_loss:', 3.8614500284194948, 'val_loss:', 0.97239531159400938)
('epoch', 380, 'train_loss:', 3.8595999419689178, 'val_loss:', 0.96919701218605037)
('epoch', 381, 'train_loss:', 3.8683575522899627, 'val_loss:', 0.97020077824592588)
('epoch', 382, 'train_loss:', 3.8615495514869691, 'val_loss:', 0.97089736342430111)
('epoch', 383, 'train_loss:', 3.8598608553409575, 'val_loss:', 0.97490149378776547)
('epoch', 384, 'train_loss:', 3.8493269622325896, 'val_loss:', 0.97345886468887333)
('epoch', 385, 'train_loss:', 3.8585125708580019, 'val_loss:', 0.97504030585289003)
('epoch', 386, 'train_loss:', 3.8508177602291109, 'val_loss:', 0.97037075400352479)
('epoch', 387, 'train_loss:', 3.8452456676959992, 'val_loss:', 0.96964917540550233)
('epoch', 388, 'train_loss:', 3.8520882487297059, 'val_loss:', 0.9697774934768677)
('epoch', 389, 'train_loss:', 3.8414406061172484, 'val_loss:', 0.96903007030487065)
('epoch', 390, 'train_loss:', 3.8469676649570466, 'val_loss:', 0.96839363217353824)
('epoch', 391, 'train_loss:', 3.8371294963359834, 'val_loss:', 0.96859163165092466)
('epoch', 392, 'train_loss:', 3.834765385389328, 'val_loss:', 0.96727900981903081)
('epoch', 393, 'train_loss:', 3.8360956931114196, 'val_loss:', 0.96558616042137146)
('epoch', 394, 'train_loss:', 3.8347708010673522, 'val_loss:', 0.96675566911697386)
('epoch', 395, 'train_loss:', 3.8330376410484313, 'val_loss:', 0.96757398724555965)
('epoch', 396, 'train_loss:', 3.8279868137836455, 'val_loss:', 0.9637913119792938)
('epoch', 397, 'train_loss:', 3.8327366364002229, 'val_loss:', 0.96514741301536555)
('epoch', 398, 'train_loss:', 3.8321524572372438, 'val_loss:', 0.96638586878776556)
('epoch', 399, 'train_loss:', 3.8294532620906829, 'val_loss:', 0.96371940851211546)
('epoch', 400, 'train_loss:', 3.8197702920436858, 'val_loss:', 0.96047641754150392)
('epoch', 401, 'train_loss:', 3.8191591942310334, 'val_loss:', 0.96513925552368163)
('epoch', 402, 'train_loss:', 3.8235032188892366, 'val_loss:', 0.96110264420509339)
('epoch', 403, 'train_loss:', 3.8254161047935487, 'val_loss:', 0.96297165393829343)
('epoch', 404, 'train_loss:', 3.8242749524116517, 'val_loss:', 0.96030005812644958)
('epoch', 405, 'train_loss:', 3.8197362947463991, 'val_loss:', 0.959516988992691)
('epoch', 406, 'train_loss:', 3.8144118154048918, 'val_loss:', 0.9572036528587341)
('epoch', 407, 'train_loss:', 3.8135034334659577, 'val_loss:', 0.95909043431282048)
('epoch', 408, 'train_loss:', 3.8125290656089783, 'val_loss:', 0.95922035574913023)
('epoch', 409, 'train_loss:', 3.8115753746032714, 'val_loss:', 0.95851270079612727)
('epoch', 410, 'train_loss:', 3.813928371667862, 'val_loss:', 0.9594571673870087)
('epoch', 411, 'train_loss:', 3.8061440801620483, 'val_loss:', 0.95709335803985596)
('epoch', 412, 'train_loss:', 3.8029219698905945, 'val_loss:', 0.96047319769859318)
('epoch', 413, 'train_loss:', 3.7986043035984038, 'val_loss:', 0.958785445690155)
('epoch', 414, 'train_loss:', 3.8034492611885069, 'val_loss:', 0.95898566842079158)
('epoch', 415, 'train_loss:', 3.7924516785144804, 'val_loss:', 0.95650816917419434)
('epoch', 416, 'train_loss:', 3.7959870576858519, 'val_loss:', 0.95616886615753172)
('epoch', 417, 'train_loss:', 3.798228305578232, 'val_loss:', 0.96068085551261906)
('epoch', 418, 'train_loss:', 3.7905153620243071, 'val_loss:', 0.95900841951370241)
('epoch', 419, 'train_loss:', 3.7870562160015107, 'val_loss:', 0.95706136345863346)
('epoch', 420, 'train_loss:', 3.7935754466056824, 'val_loss:', 0.95663410902023316)
('epoch', 421, 'train_loss:', 3.7879567098617555, 'val_loss:', 0.9552797949314118)
('epoch', 422, 'train_loss:', 3.7863895976543427, 'val_loss:', 0.95191887736320491)
('epoch', 423, 'train_loss:', 3.7903574967384337, 'val_loss:', 0.95255403757095336)
('epoch', 424, 'train_loss:', 3.7805480611324311, 'val_loss:', 0.95666604042053227)
('epoch', 425, 'train_loss:', 3.78007453083992, 'val_loss:', 0.95426101088523863)
('epoch', 426, 'train_loss:', 3.7852519977092745, 'val_loss:', 0.95421173691749572)
('epoch', 427, 'train_loss:', 3.7811528933048248, 'val_loss:', 0.95170654892921447)
('epoch', 428, 'train_loss:', 3.7746976017951965, 'val_loss:', 0.94854717254638676)
('epoch', 429, 'train_loss:', 3.7749613618850706, 'val_loss:', 0.94934684872627262)
('epoch', 430, 'train_loss:', 3.7791684210300445, 'val_loss:', 0.9535679566860199)
('epoch', 431, 'train_loss:', 3.7728640520572663, 'val_loss:', 0.95164041399955746)
('epoch', 432, 'train_loss:', 3.763844665288925, 'val_loss:', 0.9519299983978271)
('epoch', 433, 'train_loss:', 3.7690683305263519, 'val_loss:', 0.95061356663703922)
('epoch', 434, 'train_loss:', 3.7645552933216093, 'val_loss:', 0.94993236064910891)
('epoch', 435, 'train_loss:', 3.7652432644367217, 'val_loss:', 0.95062134861946102)
('epoch', 436, 'train_loss:', 3.7567309367656709, 'val_loss:', 0.94554848313331608)
('epoch', 437, 'train_loss:', 3.7574606215953827, 'val_loss:', 0.94617376923561092)
('epoch', 438, 'train_loss:', 3.7587178182601928, 'val_loss:', 0.94795576214790345)
('epoch', 439, 'train_loss:', 3.7598748457431794, 'val_loss:', 0.95177458643913271)
('epoch', 440, 'train_loss:', 3.759724544286728, 'val_loss:', 0.94654861211776731)
('epoch', 441, 'train_loss:', 3.7541322124004366, 'val_loss:', 0.94637251377105713)
('epoch', 442, 'train_loss:', 3.7559003138542177, 'val_loss:', 0.94908106565475459)
('epoch', 443, 'train_loss:', 3.7529675030708312, 'val_loss:', 0.94730781197547909)
('epoch', 444, 'train_loss:', 3.7526898300647735, 'val_loss:', 0.94679708242416383)
('epoch', 445, 'train_loss:', 3.7512190496921538, 'val_loss:', 0.94624042153358456)
('epoch', 446, 'train_loss:', 3.7527343773841859, 'val_loss:', 0.94887298107147222)
('epoch', 447, 'train_loss:', 3.7491615056991576, 'val_loss:', 0.94347753643989563)
('epoch', 448, 'train_loss:', 3.7451690077781676, 'val_loss:', 0.94598542094230653)
('epoch', 449, 'train_loss:', 3.7418168282508848, 'val_loss:', 0.94390453338623048)
('epoch', 450, 'train_loss:', 3.7423937892913819, 'val_loss:', 0.94238611459732058)
('epoch', 451, 'train_loss:', 3.743231769800186, 'val_loss:', 0.93924223780632021)
('epoch', 452, 'train_loss:', 3.7359166204929353, 'val_loss:', 0.94353610992431636)
('epoch', 453, 'train_loss:', 3.7349970638751984, 'val_loss:', 0.94067949652671812)
('epoch', 454, 'train_loss:', 3.7289788496494292, 'val_loss:', 0.94808699846267697)
('epoch', 455, 'train_loss:', 3.7331655502319334, 'val_loss:', 0.94235423803329466)
('epoch', 456, 'train_loss:', 3.7277033269405364, 'val_loss:', 0.94067835211753847)
('epoch', 457, 'train_loss:', 3.7327135145664214, 'val_loss:', 0.94279205322265625)
('epoch', 458, 'train_loss:', 3.7357084405422212, 'val_loss:', 0.94294126749038698)
('epoch', 459, 'train_loss:', 3.7284636116027832, 'val_loss:', 0.94235072016716004)
('epoch', 460, 'train_loss:', 3.726118162870407, 'val_loss:', 0.94208625674247737)
('epoch', 461, 'train_loss:', 3.7261128425598145, 'val_loss:', 0.93980308771133425)
('epoch', 462, 'train_loss:', 3.7270515835285187, 'val_loss:', 0.94053935289382939)
('epoch', 463, 'train_loss:', 3.7302573347091674, 'val_loss:', 0.94366679191589353)
('epoch', 464, 'train_loss:', 3.7231770789623262, 'val_loss:', 0.93869908690452575)
('epoch', 465, 'train_loss:', 3.7189956295490263, 'val_loss:', 0.94025288343429569)
('epoch', 466, 'train_loss:', 3.7126956892013552, 'val_loss:', 0.93763488411903384)
('epoch', 467, 'train_loss:', 3.717822252511978, 'val_loss:', 0.93887496113777158)
('epoch', 468, 'train_loss:', 3.7211236500740053, 'val_loss:', 0.93609211564064021)
('epoch', 469, 'train_loss:', 3.7190555620193479, 'val_loss:', 0.93914803862571716)
('epoch', 470, 'train_loss:', 3.7179066002368928, 'val_loss:', 0.94010440587997435)
('epoch', 471, 'train_loss:', 3.712766774892807, 'val_loss:', 0.93925644874572756)
('epoch', 472, 'train_loss:', 3.7110919857025149, 'val_loss:', 0.93744455933570858)
('epoch', 473, 'train_loss:', 3.7097348618507384, 'val_loss:', 0.93525329113006594)
('epoch', 474, 'train_loss:', 3.7088441073894503, 'val_loss:', 0.93714789986610414)
('epoch', 475, 'train_loss:', 3.7009400963783263, 'val_loss:', 0.9330837833881378)
('epoch', 476, 'train_loss:', 3.7153792703151702, 'val_loss:', 0.93575946807861332)
('epoch', 477, 'train_loss:', 3.7010232853889464, 'val_loss:', 0.93477119207382198)
('epoch', 478, 'train_loss:', 3.7088863825798035, 'val_loss:', 0.93455831766128539)
('epoch', 479, 'train_loss:', 3.7045885860919952, 'val_loss:', 0.93377715110778814)
('epoch', 480, 'train_loss:', 3.698182302713394, 'val_loss:', 0.9322810757160187)
('epoch', 481, 'train_loss:', 3.6946306335926056, 'val_loss:', 0.93154679179191591)
('epoch', 482, 'train_loss:', 3.7018004894256591, 'val_loss:', 0.93384039044380185)
('epoch', 483, 'train_loss:', 3.6936472189426421, 'val_loss:', 0.93202076077461238)
('epoch', 484, 'train_loss:', 3.6914417600631713, 'val_loss:', 0.9352047884464264)
('epoch', 485, 'train_loss:', 3.6894964540004729, 'val_loss:', 0.93274572968482972)
('epoch', 486, 'train_loss:', 3.6950318396091459, 'val_loss:', 0.93620855212211607)
('epoch', 487, 'train_loss:', 3.6877913069725037, 'val_loss:', 0.93262816905975343)
('epoch', 488, 'train_loss:', 3.6935740292072294, 'val_loss:', 0.93168239712715151)
('epoch', 489, 'train_loss:', 3.6852128255367278, 'val_loss:', 0.93307954788208003)
('epoch', 490, 'train_loss:', 3.6885030388832094, 'val_loss:', 0.93657278656959531)
('epoch', 491, 'train_loss:', 3.6842182481288912, 'val_loss:', 0.92794298052787783)
('epoch', 492, 'train_loss:', 3.6899483406543734, 'val_loss:', 0.93149035811424252)
('epoch', 493, 'train_loss:', 3.6809492647647857, 'val_loss:', 0.9295432043075561)
('epoch', 494, 'train_loss:', 3.6752873134613036, 'val_loss:', 0.92766191601753234)
('epoch', 495, 'train_loss:', 3.6761239135265349, 'val_loss:', 0.93528222560882568)
('epoch', 496, 'train_loss:', 3.676930272579193, 'val_loss:', 0.92963464617729186)
('epoch', 497, 'train_loss:', 3.6741845703124998, 'val_loss:', 0.92769690752029421)
('epoch', 498, 'train_loss:', 3.6792931234836579, 'val_loss:', 0.93043667197227475)
('epoch', 499, 'train_loss:', 3.6727221941947938, 'val_loss:', 0.92861900806427)
('epoch', 500, 'train_loss:', 3.6717742753028868, 'val_loss:', 0.9266228640079498)
('epoch', 501, 'train_loss:', 3.6742576551437378, 'val_loss:', 0.92694315671920779)
('epoch', 502, 'train_loss:', 3.6705582559108736, 'val_loss:', 0.92939390063285832)
('epoch', 503, 'train_loss:', 3.6688240325450896, 'val_loss:', 0.92640759110450743)
('epoch', 504, 'train_loss:', 3.667134300470352, 'val_loss:', 0.92708715677261355)
('epoch', 505, 'train_loss:', 3.6657709121704101, 'val_loss:', 0.92721082448959347)
('epoch', 506, 'train_loss:', 3.6627849221229551, 'val_loss:', 0.92522770524024966)
('epoch', 507, 'train_loss:', 3.6672867476940154, 'val_loss:', 0.92895392656326292)
('epoch', 508, 'train_loss:', 3.6660552239418029, 'val_loss:', 0.92818197011947634)
('epoch', 509, 'train_loss:', 3.6618776047229766, 'val_loss:', 0.92485054492950436)
('epoch', 510, 'train_loss:', 3.658632265329361, 'val_loss:', 0.92731555342674254)
('epoch', 511, 'train_loss:', 3.655071563720703, 'val_loss:', 0.92890158534049982)
('epoch', 512, 'train_loss:', 3.6645788598060607, 'val_loss:', 0.92694761753082278)
('epoch', 513, 'train_loss:', 3.660115257501602, 'val_loss:', 0.92120552897453312)
('epoch', 514, 'train_loss:', 3.656609399318695, 'val_loss:', 0.92377594590187073)
('epoch', 515, 'train_loss:', 3.6570104873180389, 'val_loss:', 0.92475731492042546)
('epoch', 516, 'train_loss:', 3.6525665068626405, 'val_loss:', 0.92340445637702939)
('epoch', 517, 'train_loss:', 3.657056930065155, 'val_loss:', 0.92090754628181459)
('epoch', 518, 'train_loss:', 3.6509259080886842, 'val_loss:', 0.92574297189712529)
('epoch', 519, 'train_loss:', 3.6540797615051268, 'val_loss:', 0.9232347393035889)
('epoch', 520, 'train_loss:', 3.6456251680850982, 'val_loss:', 0.92527134537696842)
('epoch', 521, 'train_loss:', 3.6478563511371611, 'val_loss:', 0.91955687046051027)
('epoch', 522, 'train_loss:', 3.6443321549892427, 'val_loss:', 0.9223232913017273)
('epoch', 523, 'train_loss:', 3.6460434794425964, 'val_loss:', 0.92047035932540888)
('epoch', 524, 'train_loss:', 3.6487489187717439, 'val_loss:', 0.92343801975250239)
('epoch', 525, 'train_loss:', 3.6387512314319612, 'val_loss:', 0.92198655486106873)
('epoch', 526, 'train_loss:', 3.6378411602973939, 'val_loss:', 0.91902778267860408)
('epoch', 527, 'train_loss:', 3.6434863209724426, 'val_loss:', 0.92149066686630254)
('epoch', 528, 'train_loss:', 3.6424394059181213, 'val_loss:', 0.92022927284240719)
('epoch', 529, 'train_loss:', 3.638667576313019, 'val_loss:', 0.92080146312713618)
('epoch', 530, 'train_loss:', 3.6343485701084135, 'val_loss:', 0.91827027201652522)
('epoch', 531, 'train_loss:', 3.6403524541854857, 'val_loss:', 0.92048585891723633)
('epoch', 532, 'train_loss:', 3.639472509622574, 'val_loss:', 0.91651140570640566)
('epoch', 533, 'train_loss:', 3.6346613705158233, 'val_loss:', 0.91791187286376952)
('epoch', 534, 'train_loss:', 3.6289255714416502, 'val_loss:', 0.92015005230903624)
('epoch', 535, 'train_loss:', 3.6351088488101961, 'val_loss:', 0.92212814211845395)
('epoch', 536, 'train_loss:', 3.6338273835182191, 'val_loss:', 0.9181804275512695)
('epoch', 537, 'train_loss:', 3.6392313265800476, 'val_loss:', 0.91803296446800231)
('epoch', 538, 'train_loss:', 3.6305507373809816, 'val_loss:', 0.91973326921463017)
('epoch', 539, 'train_loss:', 3.6270715534687041, 'val_loss:', 0.91851344227790832)
('epoch', 540, 'train_loss:', 3.621248495578766, 'val_loss:', 0.91983830332756045)
('epoch', 541, 'train_loss:', 3.624805061817169, 'val_loss:', 0.9175082921981812)
('epoch', 542, 'train_loss:', 3.6302527451515196, 'val_loss:', 0.91893178820610044)
('epoch', 543, 'train_loss:', 3.6289422047138213, 'val_loss:', 0.91842081189155578)
('epoch', 544, 'train_loss:', 3.620701402425766, 'val_loss:', 0.91870473980903622)
('epoch', 545, 'train_loss:', 3.6279972016811373, 'val_loss:', 0.91796155452728267)
('epoch', 546, 'train_loss:', 3.6188930857181547, 'val_loss:', 0.91725131154060369)
('epoch', 547, 'train_loss:', 3.6170247077941893, 'val_loss:', 0.92187511920928955)
('epoch', 548, 'train_loss:', 3.6229082953929903, 'val_loss:', 0.9142531406879425)
('epoch', 549, 'train_loss:', 3.6191150534152983, 'val_loss:', 0.91430592298507696)
('epoch', 550, 'train_loss:', 3.6181274008750917, 'val_loss:', 0.91299605369567871)
('epoch', 551, 'train_loss:', 3.6121877610683439, 'val_loss:', 0.91985699295997625)
('epoch', 552, 'train_loss:', 3.6124340581893919, 'val_loss:', 0.91666067600250245)
('epoch', 553, 'train_loss:', 3.6152224051952362, 'val_loss:', 0.9151404643058777)
('epoch', 554, 'train_loss:', 3.6100028049945831, 'val_loss:', 0.91305701375007631)
('epoch', 555, 'train_loss:', 3.6129574823379516, 'val_loss:', 0.91396444439888003)
('epoch', 556, 'train_loss:', 3.6071076107025148, 'val_loss:', 0.91220362305641178)
('epoch', 557, 'train_loss:', 3.6090290224552155, 'val_loss:', 0.91785489082336424)
('epoch', 558, 'train_loss:', 3.609915678501129, 'val_loss:', 0.9159002614021301)
('epoch', 559, 'train_loss:', 3.6059239685535429, 'val_loss:', 0.91460045218467712)
('epoch', 560, 'train_loss:', 3.6001361739635467, 'val_loss:', 0.91413384914398188)
('epoch', 561, 'train_loss:', 3.6038825738430025, 'val_loss:', 0.91462738990783687)
('epoch', 562, 'train_loss:', 3.6099683010578154, 'val_loss:', 0.91389137744903559)
('epoch', 563, 'train_loss:', 3.6028128314018248, 'val_loss:', 0.91490748167037961)
('epoch', 564, 'train_loss:', 3.6049433624744416, 'val_loss:', 0.91360805749893192)
('epoch', 565, 'train_loss:', 3.6032187831401825, 'val_loss:', 0.90888690590858456)
('epoch', 566, 'train_loss:', 3.6005431210994718, 'val_loss:', 0.91329415798187252)
('epoch', 567, 'train_loss:', 3.5984479236602782, 'val_loss:', 0.91585588097572324)
('epoch', 568, 'train_loss:', 3.6008081233501432, 'val_loss:', 0.91465995907783504)
('epoch', 569, 'train_loss:', 3.5945285725593568, 'val_loss:', 0.91141955733299251)
('epoch', 570, 'train_loss:', 3.5938911521434784, 'val_loss:', 0.91602569103240972)
('epoch', 571, 'train_loss:', 3.5873255264759063, 'val_loss:', 0.90917400121688841)
('epoch', 572, 'train_loss:', 3.5888113224506379, 'val_loss:', 0.90993584156036378)
('epoch', 573, 'train_loss:', 3.5977571916580202, 'val_loss:', 0.90904174566268925)
('epoch', 574, 'train_loss:', 3.5949861752986907, 'val_loss:', 0.91148084521293637)
('epoch', 575, 'train_loss:', 3.5915628778934479, 'val_loss:', 0.90682335972785955)
('epoch', 576, 'train_loss:', 3.5871567451953887, 'val_loss:', 0.91232512593269344)
('epoch', 577, 'train_loss:', 3.5876471638679504, 'val_loss:', 0.91169598579406741)
('epoch', 578, 'train_loss:', 3.5872859942913053, 'val_loss:', 0.90888584375381465)
('epoch', 579, 'train_loss:', 3.5861162531375883, 'val_loss:', 0.91023267269134522)
('epoch', 580, 'train_loss:', 3.5835827660560606, 'val_loss:', 0.90793299674987793)
('epoch', 581, 'train_loss:', 3.5870194482803344, 'val_loss:', 0.91027742862701411)
('epoch', 582, 'train_loss:', 3.5876444268226622, 'val_loss:', 0.90891983270645138)
('epoch', 583, 'train_loss:', 3.5837846755981446, 'val_loss:', 0.90739787578582765)
('epoch', 584, 'train_loss:', 3.5861522519588469, 'val_loss:', 0.91114379882812502)
('epoch', 585, 'train_loss:', 3.5883028972148896, 'val_loss:', 0.90716184377670284)
('epoch', 586, 'train_loss:', 3.5780083835124969, 'val_loss:', 0.90522072553634647)
('epoch', 587, 'train_loss:', 3.5813474869728088, 'val_loss:', 0.90859548687934877)
('epoch', 588, 'train_loss:', 3.5771246159076693, 'val_loss:', 0.90792486310005183)
('epoch', 589, 'train_loss:', 3.5805334436893461, 'val_loss:', 0.90822354197502131)
('epoch', 590, 'train_loss:', 3.577413957118988, 'val_loss:', 0.91075952887535094)
('epoch', 591, 'train_loss:', 3.5804723274707793, 'val_loss:', 0.90699159502983095)
('epoch', 592, 'train_loss:', 3.5684867143630981, 'val_loss:', 0.90567966461181637)
('epoch', 593, 'train_loss:', 3.5776788032054903, 'val_loss:', 0.90609834551811219)
('epoch', 594, 'train_loss:', 3.5744457542896271, 'val_loss:', 0.9053667223453522)
('epoch', 595, 'train_loss:', 3.5685102558135986, 'val_loss:', 0.90631359219551089)
('epoch', 596, 'train_loss:', 3.5747562515735627, 'val_loss:', 0.90466968297958372)
('epoch', 597, 'train_loss:', 3.5708524835109712, 'val_loss:', 0.90808346271514895)
('epoch', 598, 'train_loss:', 3.5743217873573303, 'val_loss:', 0.90622812032699585)
('epoch', 599, 'train_loss:', 3.5728562545776366, 'val_loss:', 0.90833857893943781)
