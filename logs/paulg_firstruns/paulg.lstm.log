(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=0 python lstm.py -t
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2280 get requests, put_count=2269 evicted_count=1000 eviction_rate=0.440723 and unsatisfied allocation rate=0.487281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3416 get requests, put_count=3395 evicted_count=1000 eviction_rate=0.294551 and unsatisfied allocation rate=0.305621
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
('epoch', 0, 'train_loss:', 8.6270897912979123, 'val_loss:', 1.9034836649894715)
('epoch', 1, 'train_loss:', 7.2667571902275085, 'val_loss:', 1.7231252121925353)
('epoch', 2, 'train_loss:', 6.6904914069175723, 'val_loss:', 1.6468233275413513)
('epoch', 3, 'train_loss:', 6.4227400422096252, 'val_loss:', 1.5747274875640869)
('epoch', 4, 'train_loss:', 6.2206568551063537, 'val_loss:', 1.5250043153762818)
('epoch', 5, 'train_loss:', 6.0198132324218747, 'val_loss:', 1.4755827856063843)
('epoch', 6, 'train_loss:', 5.8202280187606812, 'val_loss:', 1.4339089155197144)
('epoch', 7, 'train_loss:', 5.6408240032196044, 'val_loss:', 1.3805280208587647)
('epoch', 8, 'train_loss:', 5.4743262171745304, 'val_loss:', 1.3424950337409973)
('epoch', 9, 'train_loss:', 5.3207300996780393, 'val_loss:', 1.3089015769958496)
('epoch', 10, 'train_loss:', 5.1780455017089846, 'val_loss:', 1.2757204031944276)
('epoch', 11, 'train_loss:', 5.0486318409442905, 'val_loss:', 1.2443882989883424)
('epoch', 12, 'train_loss:', 4.9373590183258056, 'val_loss:', 1.2164096939563751)
('epoch', 13, 'train_loss:', 4.8264074313640597, 'val_loss:', 1.1977802670001985)
('epoch', 14, 'train_loss:', 4.7402428627014164, 'val_loss:', 1.1720563471317291)
('epoch', 15, 'train_loss:', 4.6505062603950504, 'val_loss:', 1.1548583734035491)
('epoch', 16, 'train_loss:', 4.5681065881252287, 'val_loss:', 1.136007580757141)
('epoch', 17, 'train_loss:', 4.4985768604278569, 'val_loss:', 1.1108333468437195)
('epoch', 18, 'train_loss:', 4.4303305780887605, 'val_loss:', 1.0986894142627717)
('epoch', 19, 'train_loss:', 4.3739582407474522, 'val_loss:', 1.0850576889514922)
('epoch', 20, 'train_loss:', 4.316714603900909, 'val_loss:', 1.0718308174610138)
('epoch', 21, 'train_loss:', 4.2636068785190586, 'val_loss:', 1.061612309217453)
('epoch', 22, 'train_loss:', 4.2056817996501925, 'val_loss:', 1.0493937921524048)
('epoch', 23, 'train_loss:', 4.1748563611507414, 'val_loss:', 1.0438636088371276)
('epoch', 24, 'train_loss:', 4.1367550110816955, 'val_loss:', 1.0357760953903199)
('epoch', 25, 'train_loss:', 4.0959280455112461, 'val_loss:', 1.0197341191768645)
('epoch', 26, 'train_loss:', 4.0571087133884429, 'val_loss:', 1.0127322471141815)
('epoch', 27, 'train_loss:', 4.0262340056896209, 'val_loss:', 1.0055917012691498)
('epoch', 28, 'train_loss:', 3.9847889292240142, 'val_loss:', 0.99783854603767397)
('epoch', 29, 'train_loss:', 3.9655575394630431, 'val_loss:', 0.99717892527580265)
('epoch', 30, 'train_loss:', 3.9320280325412749, 'val_loss:', 0.98926792979240419)
('epoch', 31, 'train_loss:', 3.9019503462314606, 'val_loss:', 0.98144296407699583)
('epoch', 32, 'train_loss:', 3.8798959064483642, 'val_loss:', 0.97669583916664127)
('epoch', 33, 'train_loss:', 3.857151520252228, 'val_loss:', 0.96737546443939204)
('epoch', 34, 'train_loss:', 3.8331611204147338, 'val_loss:', 0.9650598537921905)
('epoch', 35, 'train_loss:', 3.802282452583313, 'val_loss:', 0.95507242441177365)
('epoch', 36, 'train_loss:', 3.7845517814159395, 'val_loss:', 0.9534038627147674)
('epoch', 37, 'train_loss:', 3.7698953342437744, 'val_loss:', 0.94895724296569828)
('epoch', 38, 'train_loss:', 3.7512868523597716, 'val_loss:', 0.94576936244964604)
('epoch', 39, 'train_loss:', 3.7411010742187498, 'val_loss:', 0.94464284181594849)
('epoch', 40, 'train_loss:', 3.717677674293518, 'val_loss:', 0.93638168334960936)
('epoch', 41, 'train_loss:', 3.6903577220439909, 'val_loss:', 0.93151472687721248)
('epoch', 42, 'train_loss:', 3.6826425087451935, 'val_loss:', 0.9313503777980805)
('epoch', 43, 'train_loss:', 3.6623823034763334, 'val_loss:', 0.92925084948539738)
('epoch', 44, 'train_loss:', 3.6534306657314302, 'val_loss:', 0.92632613778114314)
('epoch', 45, 'train_loss:', 3.6438649070262907, 'val_loss:', 0.9202714586257934)
('epoch', 46, 'train_loss:', 3.620946776866913, 'val_loss:', 0.91722901582717897)
('epoch', 47, 'train_loss:', 3.6133779931068419, 'val_loss:', 0.91812460422515874)
('epoch', 48, 'train_loss:', 3.6045838868618012, 'val_loss:', 0.91404011726379397)
('epoch', 49, 'train_loss:', 3.5878399085998534, 'val_loss:', 0.91503523945808407)
('epoch', 50, 'train_loss:', 3.5805664122104646, 'val_loss:', 0.90536540508270269)
('epoch', 51, 'train_loss:', 3.5632968425750731, 'val_loss:', 0.90502744913101196)
('epoch', 52, 'train_loss:', 3.5523167765140533, 'val_loss:', 0.90303953528404235)
('epoch', 53, 'train_loss:', 3.5456126582622529, 'val_loss:', 0.89988979220390319)
('epoch', 54, 'train_loss:', 3.5379365015029909, 'val_loss:', 0.89886044621467587)
('epoch', 55, 'train_loss:', 3.5205848002433777, 'val_loss:', 0.89470754742622371)
('epoch', 56, 'train_loss:', 3.5180463838577269, 'val_loss:', 0.89538901090621947)
('epoch', 57, 'train_loss:', 3.5007539486885069, 'val_loss:', 0.89009756803512574)
('epoch', 58, 'train_loss:', 3.4910484707355498, 'val_loss:', 0.89059505343437195)
('epoch', 59, 'train_loss:', 3.4853949928283692, 'val_loss:', 0.89327034711837772)
('epoch', 60, 'train_loss:', 3.4790854084491731, 'val_loss:', 0.89181568264961242)
('epoch', 61, 'train_loss:', 3.4666946542263033, 'val_loss:', 0.88473153948783878)
('epoch', 62, 'train_loss:', 3.4596605038642885, 'val_loss:', 0.88470946073532108)
('epoch', 63, 'train_loss:', 3.4583655786514282, 'val_loss:', 0.88173860192298892)
('epoch', 64, 'train_loss:', 3.4415179681777954, 'val_loss:', 0.88500831604003904)
('epoch', 65, 'train_loss:', 3.4401533830165865, 'val_loss:', 0.88331224322319035)
('epoch', 66, 'train_loss:', 3.4346156477928163, 'val_loss:', 0.88092710256576534)
('epoch', 67, 'train_loss:', 3.4249567568302153, 'val_loss:', 0.88474578261375425)
('epoch', 68, 'train_loss:', 3.4179562687873841, 'val_loss:', 0.87859585762023928)
('epoch', 69, 'train_loss:', 3.4079272341728211, 'val_loss:', 0.87544778108596799)
('epoch', 70, 'train_loss:', 3.4070092058181762, 'val_loss:', 0.87655936360359188)
('epoch', 71, 'train_loss:', 3.395609711408615, 'val_loss:', 0.87337898135185243)
('epoch', 72, 'train_loss:', 3.3880494785308839, 'val_loss:', 0.87780725479125976)
('epoch', 73, 'train_loss:', 3.3829769897460937, 'val_loss:', 0.87191470742225652)
('epoch', 74, 'train_loss:', 3.3717959213256834, 'val_loss:', 0.87121801018714906)
('epoch', 75, 'train_loss:', 3.3689234662055969, 'val_loss:', 0.8662873780727387)
('epoch', 76, 'train_loss:', 3.3594099187850954, 'val_loss:', 0.86788956165313724)
('epoch', 77, 'train_loss:', 3.3555370080471039, 'val_loss:', 0.86880311608314509)
('epoch', 78, 'train_loss:', 3.3495421135425567, 'val_loss:', 0.86182498574256894)
('epoch', 79, 'train_loss:', 3.3540707719326019, 'val_loss:', 0.86417122006416325)
('epoch', 80, 'train_loss:', 3.3435628676414488, 'val_loss:', 0.86628448128700253)
('epoch', 81, 'train_loss:', 3.3347904574871063, 'val_loss:', 0.86146963238716123)
('epoch', 82, 'train_loss:', 3.328856807947159, 'val_loss:', 0.86228464603424071)
('epoch', 83, 'train_loss:', 3.333355301618576, 'val_loss:', 0.86231079697608948)
('epoch', 84, 'train_loss:', 3.3164389371871947, 'val_loss:', 0.86205266594886776)
('epoch', 85, 'train_loss:', 3.3167763543128967, 'val_loss:', 0.86303968071937565)
('epoch', 86, 'train_loss:', 3.310791325569153, 'val_loss:', 0.86425213456153871)
('epoch', 87, 'train_loss:', 3.307236295938492, 'val_loss:', 0.86088022589683533)
('epoch', 88, 'train_loss:', 3.2953798162937162, 'val_loss:', 0.85606224298477174)
('epoch', 89, 'train_loss:', 3.2961346971988679, 'val_loss:', 0.85609564542770389)
('epoch', 90, 'train_loss:', 3.2892090785503387, 'val_loss:', 0.86090054512023928)
('epoch', 91, 'train_loss:', 3.2876378273963929, 'val_loss:', 0.85531795382499698)
('epoch', 92, 'train_loss:', 3.2874295520782471, 'val_loss:', 0.85352674245834348)
('epoch', 93, 'train_loss:', 3.2807658648490907, 'val_loss:', 0.86014008879661563)
('epoch', 94, 'train_loss:', 3.2720777142047881, 'val_loss:', 0.85601726412773127)
('epoch', 95, 'train_loss:', 3.2751401984691619, 'val_loss:', 0.85587813377380373)
('epoch', 96, 'train_loss:', 3.2667639052867887, 'val_loss:', 0.85224896311759946)
('epoch', 97, 'train_loss:', 3.2634361016750337, 'val_loss:', 0.85261953353881836)
('epoch', 98, 'train_loss:', 3.25315087556839, 'val_loss:', 0.85458665728569028)
('epoch', 99, 'train_loss:', 3.2536473298072814, 'val_loss:', 0.85218253970146174)
('epoch', 100, 'train_loss:', 3.2432101953029631, 'val_loss:', 0.85323998928070066)
('epoch', 101, 'train_loss:', 3.245521831512451, 'val_loss:', 0.85216223120689394)
('epoch', 102, 'train_loss:', 3.238594263792038, 'val_loss:', 0.84875490903854367)
('epoch', 103, 'train_loss:', 3.2343405866622925, 'val_loss:', 0.84917566537857059)
('epoch', 104, 'train_loss:', 3.2296532237529756, 'val_loss:', 0.84744021654129031)
('epoch', 105, 'train_loss:', 3.2262508296966552, 'val_loss:', 0.84894068956375124)
('epoch', 106, 'train_loss:', 3.2169750607013703, 'val_loss:', 0.84637250423431398)
('epoch', 107, 'train_loss:', 3.221411670446396, 'val_loss:', 0.84783474326133723)
('epoch', 108, 'train_loss:', 3.2176720213890078, 'val_loss:', 0.84591096282005307)
('epoch', 109, 'train_loss:', 3.209144047498703, 'val_loss:', 0.84976541280746465)
('epoch', 110, 'train_loss:', 3.206861091852188, 'val_loss:', 0.84632720112800597)
('epoch', 111, 'train_loss:', 3.1987128078937532, 'val_loss:', 0.84869598627090459)
('epoch', 112, 'train_loss:', 3.2005211210250852, 'val_loss:', 0.84762799024581914)
('epoch', 113, 'train_loss:', 3.1992893552780153, 'val_loss:', 0.84624835610389715)
('epoch', 114, 'train_loss:', 3.1987357234954832, 'val_loss:', 0.8484411549568176)
('epoch', 115, 'train_loss:', 3.1923758888244631, 'val_loss:', 0.84903279662132258)
('epoch', 116, 'train_loss:', 3.1852448070049286, 'val_loss:', 0.84293969154357906)
('epoch', 117, 'train_loss:', 3.1850790297985077, 'val_loss:', 0.84307207703590392)
('epoch', 118, 'train_loss:', 3.1787890398502352, 'val_loss:', 0.84165251970291133)
('epoch', 119, 'train_loss:', 3.180228420495987, 'val_loss:', 0.84535709142684934)
('epoch', 120, 'train_loss:', 3.1765357089042663, 'val_loss:', 0.84665674209594721)
('epoch', 121, 'train_loss:', 3.1682899951934815, 'val_loss:', 0.84324077129364017)
('epoch', 122, 'train_loss:', 3.1735344982147216, 'val_loss:', 0.84066222071647645)
('epoch', 123, 'train_loss:', 3.1665615010261536, 'val_loss:', 0.84464150071144106)
('epoch', 124, 'train_loss:', 3.1657354211807252, 'val_loss:', 0.84256538629531863)
('epoch', 125, 'train_loss:', 3.1635680663585664, 'val_loss:', 0.84254708647727972)
('epoch', 126, 'train_loss:', 3.1613533449172975, 'val_loss:', 0.84496448516845701)
('epoch', 127, 'train_loss:', 3.1525426840782167, 'val_loss:', 0.8405105340480804)
('epoch', 128, 'train_loss:', 3.147349342107773, 'val_loss:', 0.84502281665802004)
('epoch', 129, 'train_loss:', 3.1493413460254671, 'val_loss:', 0.84390963435173039)
('epoch', 130, 'train_loss:', 3.1369389569759369, 'val_loss:', 0.84066554307937624)
('epoch', 131, 'train_loss:', 3.1426679813861846, 'val_loss:', 0.84026392817497253)
('epoch', 132, 'train_loss:', 3.1374187767505646, 'val_loss:', 0.84157664060592652)
('epoch', 133, 'train_loss:', 3.1318506205081942, 'val_loss:', 0.84099137187004092)
('epoch', 134, 'train_loss:', 3.133043713569641, 'val_loss:', 0.84238518357276915)
('epoch', 135, 'train_loss:', 3.1266520226001742, 'val_loss:', 0.83759968757629399)
('epoch', 136, 'train_loss:', 3.1165825235843658, 'val_loss:', 0.84039557695388789)
('epoch', 137, 'train_loss:', 3.1166384696960447, 'val_loss:', 0.8433870947360993)
('epoch', 138, 'train_loss:', 3.1142897486686705, 'val_loss:', 0.83823798060417176)
('epoch', 139, 'train_loss:', 3.115793036222458, 'val_loss:', 0.84070181488990781)
('epoch', 140, 'train_loss:', 3.1109694159030914, 'val_loss:', 0.84323203206062314)
('epoch', 141, 'train_loss:', 3.110506844520569, 'val_loss:', 0.84229038834571834)
('epoch', 142, 'train_loss:', 3.1030501437187197, 'val_loss:', 0.83758464455604553)
('epoch', 143, 'train_loss:', 3.1047198283672333, 'val_loss:', 0.83926455736160277)
('epoch', 144, 'train_loss:', 3.1004939854145048, 'val_loss:', 0.83801774263381956)
('epoch', 145, 'train_loss:', 3.0966866505146027, 'val_loss:', 0.83902535200119022)
('epoch', 146, 'train_loss:', 3.0942286431789396, 'val_loss:', 0.8371001219749451)
('epoch', 147, 'train_loss:', 3.0878403294086456, 'val_loss:', 0.8374072849750519)
('epoch', 148, 'train_loss:', 3.0856633329391481, 'val_loss:', 0.83520670413970943)
('epoch', 149, 'train_loss:', 3.0802337324619291, 'val_loss:', 0.83844735860824582)
('epoch', 150, 'train_loss:', 3.0875134932994843, 'val_loss:', 0.83925656676292415)
('epoch', 151, 'train_loss:', 3.0760434055328369, 'val_loss:', 0.84230168223381041)
('epoch', 152, 'train_loss:', 3.0768122649192811, 'val_loss:', 0.83754567980766292)
('epoch', 153, 'train_loss:', 3.0700642168521881, 'val_loss:', 0.83709414482116695)
('epoch', 154, 'train_loss:', 3.0709744620323183, 'val_loss:', 0.83442141771316525)
('epoch', 155, 'train_loss:', 3.0709413385391233, 'val_loss:', 0.83978137612342829)
('epoch', 156, 'train_loss:', 3.0579621350765227, 'val_loss:', 0.83886391043663022)
('epoch', 157, 'train_loss:', 3.0627915883064269, 'val_loss:', 0.83749597072601323)
('epoch', 158, 'train_loss:', 3.0608270895481109, 'val_loss:', 0.83918149352073668)
('epoch', 159, 'train_loss:', 3.0605446195602415, 'val_loss:', 0.83574181675910952)
('epoch', 160, 'train_loss:', 3.0441248285770417, 'val_loss:', 0.83902571558952332)
('epoch', 161, 'train_loss:', 3.0489619529247283, 'val_loss:', 0.83847059011459346)
('epoch', 162, 'train_loss:', 3.0503351545333861, 'val_loss:', 0.83780716419219969)
('epoch', 163, 'train_loss:', 3.0506234323978423, 'val_loss:', 0.83598109483718874)
('epoch', 164, 'train_loss:', 3.0437296903133393, 'val_loss:', 0.83625674724578858)
('epoch', 165, 'train_loss:', 3.0386560165882113, 'val_loss:', 0.83666065335273743)
('epoch', 166, 'train_loss:', 3.0399608242511751, 'val_loss:', 0.8345686984062195)
('epoch', 167, 'train_loss:', 3.0365382707118989, 'val_loss:', 0.83821606636047363)
('epoch', 168, 'train_loss:', 3.0295126831531523, 'val_loss:', 0.83707603216171267)
('epoch', 169, 'train_loss:', 3.0329512107372283, 'val_loss:', 0.83781437158584593)
('epoch', 170, 'train_loss:', 3.0332453799247743, 'val_loss:', 0.83659637570381162)
('epoch', 171, 'train_loss:', 3.02408585190773, 'val_loss:', 0.83760142445564267)
('epoch', 172, 'train_loss:', 3.0258072447776794, 'val_loss:', 0.84048239111900325)
('epoch', 173, 'train_loss:', 3.0272024440765382, 'val_loss:', 0.83591135144233708)
('epoch', 174, 'train_loss:', 3.0220336830615997, 'val_loss:', 0.83813775897026066)
('epoch', 175, 'train_loss:', 3.0146418499946592, 'val_loss:', 0.83994664788246154)
('epoch', 176, 'train_loss:', 3.0152502083778381, 'val_loss:', 0.83652889132499697)
('epoch', 177, 'train_loss:', 3.0144722843170166, 'val_loss:', 0.8394642424583435)
('epoch', 178, 'train_loss:', 3.0094003450870512, 'val_loss:', 0.83766284823417658)
('epoch', 179, 'train_loss:', 3.0015425086021423, 'val_loss:', 0.83914348721504206)
('epoch', 180, 'train_loss:', 3.0024308228492735, 'val_loss:', 0.83732542395591736)
('epoch', 181, 'train_loss:', 3.0051726233959197, 'val_loss:', 0.83764435768127443)
('epoch', 182, 'train_loss:', 2.999944682121277, 'val_loss:', 0.84236281514167788)
('epoch', 183, 'train_loss:', 2.9978095114231111, 'val_loss:', 0.83298175454139711)
('epoch', 184, 'train_loss:', 2.9932185447216035, 'val_loss:', 0.83550030589103697)
('epoch', 185, 'train_loss:', 2.9932354474067688, 'val_loss:', 0.83750897288322446)
('epoch', 186, 'train_loss:', 2.9908878290653229, 'val_loss:', 0.83465490341186521)
('epoch', 187, 'train_loss:', 2.9815595591068269, 'val_loss:', 0.83733853697776794)
('epoch', 188, 'train_loss:', 2.9829059529304502, 'val_loss:', 0.83669240713119508)
('epoch', 189, 'train_loss:', 2.9803261327743531, 'val_loss:', 0.83859725475311275)
('epoch', 190, 'train_loss:', 2.9808915328979491, 'val_loss:', 0.83431542754173282)
('epoch', 191, 'train_loss:', 2.9758774495124816, 'val_loss:', 0.83862391114234924)
('epoch', 192, 'train_loss:', 2.9793961942195892, 'val_loss:', 0.83541185498237613)
('epoch', 193, 'train_loss:', 2.9764093804359435, 'val_loss:', 0.83959122419357302)
('epoch', 194, 'train_loss:', 2.9722406744956968, 'val_loss:', 0.83669546484947199)
('epoch', 195, 'train_loss:', 2.9638916397094728, 'val_loss:', 0.83863833546638489)
('epoch', 196, 'train_loss:', 2.966421855688095, 'val_loss:', 0.83762827754020686)
('epoch', 197, 'train_loss:', 2.9697106516361238, 'val_loss:', 0.83960051178932193)
('epoch', 198, 'train_loss:', 2.9651589965820313, 'val_loss:', 0.84084553837776188)
('epoch', 199, 'train_loss:', 2.9557252442836761, 'val_loss:', 0.83515216708183293)
('epoch', 200, 'train_loss:', 2.9576121127605437, 'val_loss:', 0.8390396869182587)
('epoch', 201, 'train_loss:', 2.9508436834812164, 'val_loss:', 0.83990953922271727)
('epoch', 202, 'train_loss:', 2.9493775510787965, 'val_loss:', 0.8402762675285339)
('epoch', 203, 'train_loss:', 2.9515834891796113, 'val_loss:', 0.8418315052986145)
('epoch', 204, 'train_loss:', 2.9467838013172152, 'val_loss:', 0.84364238858222962)
('epoch', 205, 'train_loss:', 2.943960417509079, 'val_loss:', 0.84057466268539427)
('epoch', 206, 'train_loss:', 2.9409957778453828, 'val_loss:', 0.84117187023162843)
('epoch', 207, 'train_loss:', 2.9402751779556273, 'val_loss:', 0.84120120763778683)
('epoch', 208, 'train_loss:', 2.9392181301116942, 'val_loss:', 0.84399019718170165)
('epoch', 209, 'train_loss:', 2.9339529669284818, 'val_loss:', 0.84325558543205259)
('epoch', 210, 'train_loss:', 2.936295747756958, 'val_loss:', 0.84005530118942262)
('epoch', 211, 'train_loss:', 2.9264698600769044, 'val_loss:', 0.84535949349403383)
('epoch', 212, 'train_loss:', 2.9356751608848572, 'val_loss:', 0.84101341605186464)
('epoch', 213, 'train_loss:', 2.9329683244228364, 'val_loss:', 0.8446216714382172)
('epoch', 214, 'train_loss:', 2.9210529208183287, 'val_loss:', 0.84280174136161801)
('epoch', 215, 'train_loss:', 2.9199770355224608, 'val_loss:', 0.84833320617675778)
('epoch', 216, 'train_loss:', 2.9182586002349855, 'val_loss:', 0.83763094782829284)
('epoch', 217, 'train_loss:', 2.9192010521888734, 'val_loss:', 0.84428636670112611)
('epoch', 218, 'train_loss:', 2.9126972460746767, 'val_loss:', 0.84388233900070186)
('epoch', 219, 'train_loss:', 2.9152282786369326, 'val_loss:', 0.84224236369132999)
('epoch', 220, 'train_loss:', 2.9093001830577849, 'val_loss:', 0.8428147327899933)
('epoch', 221, 'train_loss:', 2.9012216114997864, 'val_loss:', 0.84578098773956301)
('epoch', 222, 'train_loss:', 2.9088419818878175, 'val_loss:', 0.84424652934074407)
('epoch', 223, 'train_loss:', 2.9020955896377565, 'val_loss:', 0.84332521557807927)
('epoch', 224, 'train_loss:', 2.9025488090515137, 'val_loss:', 0.84600556492805479)
('epoch', 225, 'train_loss:', 2.8936819314956663, 'val_loss:', 0.84391826272010806)
('epoch', 226, 'train_loss:', 2.8983654773235319, 'val_loss:', 0.84367617249488835)
('epoch', 227, 'train_loss:', 2.8959425926208495, 'val_loss:', 0.84595982074737552)
('epoch', 228, 'train_loss:', 2.8938260424137114, 'val_loss:', 0.84528274297714234)
('epoch', 229, 'train_loss:', 2.89051802277565, 'val_loss:', 0.84432612895965575)
('epoch', 230, 'train_loss:', 2.8848579061031341, 'val_loss:', 0.84575386285781862)
('epoch', 231, 'train_loss:', 2.8863739871978762, 'val_loss:', 0.84475686550140383)
('epoch', 232, 'train_loss:', 2.887137141227722, 'val_loss:', 0.84652937650680538)
('epoch', 233, 'train_loss:', 2.878682487010956, 'val_loss:', 0.8469908380508423)
('epoch', 234, 'train_loss:', 2.8732929933071136, 'val_loss:', 0.84516288042068477)
('epoch', 235, 'train_loss:', 2.8757114028930664, 'val_loss:', 0.84850793600082397)
('epoch', 236, 'train_loss:', 2.8736263239383697, 'val_loss:', 0.85060532093048091)
('epoch', 237, 'train_loss:', 2.8734203004837036, 'val_loss:', 0.84586782217025758)
('epoch', 238, 'train_loss:', 2.8699476170539855, 'val_loss:', 0.84756248235702514)
('epoch', 239, 'train_loss:', 2.8681685280799867, 'val_loss:', 0.84905846834182741)
('epoch', 240, 'train_loss:', 2.8655666244029998, 'val_loss:', 0.84865768909454342)
('epoch', 241, 'train_loss:', 2.86435342669487, 'val_loss:', 0.84916648745536805)
('epoch', 242, 'train_loss:', 2.8617869758605958, 'val_loss:', 0.84877577304840091)
('epoch', 243, 'train_loss:', 2.8594037187099457, 'val_loss:', 0.85048276305198667)
('epoch', 244, 'train_loss:', 2.8618342995643617, 'val_loss:', 0.84566681742668148)
('epoch', 245, 'train_loss:', 2.8491755449771881, 'val_loss:', 0.8511325597763062)
('epoch', 246, 'train_loss:', 2.8548906719684601, 'val_loss:', 0.84774640440940852)
('epoch', 247, 'train_loss:', 2.8516166746616363, 'val_loss:', 0.85019303560256954)
('epoch', 248, 'train_loss:', 2.848034107685089, 'val_loss:', 0.85467481732368467)
('epoch', 249, 'train_loss:', 2.8464216566085816, 'val_loss:', 0.85554371476173396)
('epoch', 250, 'train_loss:', 2.8473886072635652, 'val_loss:', 0.85346758008003232)
('epoch', 251, 'train_loss:', 2.8468989384174348, 'val_loss:', 0.85303174614906307)
('epoch', 252, 'train_loss:', 2.8429009795188902, 'val_loss:', 0.85500137686729427)
('epoch', 253, 'train_loss:', 2.8464218938350676, 'val_loss:', 0.85243177890777588)
('epoch', 254, 'train_loss:', 2.8345466423034669, 'val_loss:', 0.85305344462394717)
('epoch', 255, 'train_loss:', 2.8321385085582733, 'val_loss:', 0.85237072587013241)
('epoch', 256, 'train_loss:', 2.828264718055725, 'val_loss:', 0.85461143612861634)
('epoch', 257, 'train_loss:', 2.8292595672607423, 'val_loss:', 0.85232644081115727)
('epoch', 258, 'train_loss:', 2.8257657277584074, 'val_loss:', 0.8547924554347992)
('epoch', 259, 'train_loss:', 2.8207479894161223, 'val_loss:', 0.85520420312881473)
('epoch', 260, 'train_loss:', 2.8207417798042296, 'val_loss:', 0.85390466570854184)
('epoch', 261, 'train_loss:', 2.8196622359752657, 'val_loss:', 0.8558652400970459)
('epoch', 262, 'train_loss:', 2.8193453800678254, 'val_loss:', 0.85653557777404787)
('epoch', 263, 'train_loss:', 2.8161690890789033, 'val_loss:', 0.85809331417083745)
('epoch', 264, 'train_loss:', 2.8124044549465181, 'val_loss:', 0.86123156666755674)
('epoch', 265, 'train_loss:', 2.8097263407707214, 'val_loss:', 0.85656473517417908)
('epoch', 266, 'train_loss:', 2.8177057242393495, 'val_loss:', 0.8625717353820801)
('epoch', 267, 'train_loss:', 2.8059685277938842, 'val_loss:', 0.85815751314163213)
('epoch', 268, 'train_loss:', 2.8044537734985351, 'val_loss:', 0.85729428052902223)
('epoch', 269, 'train_loss:', 2.8049790918827058, 'val_loss:', 0.85739055991172786)
('epoch', 270, 'train_loss:', 2.8068052268028261, 'val_loss:', 0.86143693447113034)
('epoch', 271, 'train_loss:', 2.8008445954322814, 'val_loss:', 0.85954395651817317)
('epoch', 272, 'train_loss:', 2.7935856091976166, 'val_loss:', 0.86284910202026366)
('epoch', 273, 'train_loss:', 2.797385712862015, 'val_loss:', 0.8586534440517426)
('epoch', 274, 'train_loss:', 2.7892826747894288, 'val_loss:', 0.86395729660987852)
('epoch', 275, 'train_loss:', 2.7904735505580902, 'val_loss:', 0.86276304721832275)
('epoch', 276, 'train_loss:', 2.7903735625743864, 'val_loss:', 0.8605661308765411)
('epoch', 277, 'train_loss:', 2.7846254718303682, 'val_loss:', 0.86389703869819645)
('epoch', 278, 'train_loss:', 2.7824635398387909, 'val_loss:', 0.8641625571250916)
('epoch', 279, 'train_loss:', 2.7817554497718811, 'val_loss:', 0.8628978681564331)
('epoch', 280, 'train_loss:', 2.7780907773971557, 'val_loss:', 0.86165853142738347)
('epoch', 281, 'train_loss:', 2.7873288094997406, 'val_loss:', 0.86208477377891546)
('epoch', 282, 'train_loss:', 2.7797667324542998, 'val_loss:', 0.86768247961997991)
('epoch', 283, 'train_loss:', 2.7751723587512971, 'val_loss:', 0.86260695695877077)
('epoch', 284, 'train_loss:', 2.7753870224952699, 'val_loss:', 0.86794527411460876)
('epoch', 285, 'train_loss:', 2.7734103882312775, 'val_loss:', 0.86550244688987732)
('epoch', 286, 'train_loss:', 2.7678891181945802, 'val_loss:', 0.86647252798080443)
('epoch', 287, 'train_loss:', 2.7653423237800596, 'val_loss:', 0.86787247419357305)
('epoch', 288, 'train_loss:', 2.7617072701454162, 'val_loss:', 0.86673920273780825)
('epoch', 289, 'train_loss:', 2.7618313193321229, 'val_loss:', 0.86854533910751341)
('epoch', 290, 'train_loss:', 2.758600879907608, 'val_loss:', 0.86597863078117365)
('epoch', 291, 'train_loss:', 2.7567477047443392, 'val_loss:', 0.87376390457153319)
('epoch', 292, 'train_loss:', 2.7592395973205566, 'val_loss:', 0.86977171063423153)
('epoch', 293, 'train_loss:', 2.7501998031139374, 'val_loss:', 0.87026357173919677)
('epoch', 294, 'train_loss:', 2.7480301296710969, 'val_loss:', 0.86812618970870969)
('epoch', 295, 'train_loss:', 2.7455542469024659, 'val_loss:', 0.86957176566123962)
('epoch', 296, 'train_loss:', 2.7435783541202543, 'val_loss:', 0.86871644854545593)
('epoch', 297, 'train_loss:', 2.7392224299907686, 'val_loss:', 0.87465552926063539)
('epoch', 298, 'train_loss:', 2.7405032801628111, 'val_loss:', 0.86827923178672795)
('epoch', 299, 'train_loss:', 2.7414811885356904, 'val_loss:', 0.8711143517494202)
('epoch', 300, 'train_loss:', 2.736233661174774, 'val_loss:', 0.87057937145233155)
('epoch', 301, 'train_loss:', 2.7335288977622985, 'val_loss:', 0.87327451825141911)
('epoch', 302, 'train_loss:', 2.7322861397266389, 'val_loss:', 0.8736205458641052)
('epoch', 303, 'train_loss:', 2.7307987093925474, 'val_loss:', 0.87587197184562682)
('epoch', 304, 'train_loss:', 2.7300880813598631, 'val_loss:', 0.87436423420906062)
('epoch', 305, 'train_loss:', 2.7282479560375212, 'val_loss:', 0.88020861625671387)
('epoch', 306, 'train_loss:', 2.7202422010898588, 'val_loss:', 0.87433576703071592)
('epoch', 307, 'train_loss:', 2.7256233346462251, 'val_loss:', 0.87077436566352839)
('epoch', 308, 'train_loss:', 2.7195863234996795, 'val_loss:', 0.87550147891044616)
('epoch', 309, 'train_loss:', 2.7153992033004761, 'val_loss:', 0.8784225249290466)
('epoch', 310, 'train_loss:', 2.7146949076652529, 'val_loss:', 0.87395681023597716)
('epoch', 311, 'train_loss:', 2.7173151016235351, 'val_loss:', 0.87859244823455807)
('epoch', 312, 'train_loss:', 2.709041302204132, 'val_loss:', 0.87789646029472346)
('epoch', 313, 'train_loss:', 2.7132357347011564, 'val_loss:', 0.87246053218841557)
('epoch', 314, 'train_loss:', 2.7127481949329377, 'val_loss:', 0.87646203994750982)
('epoch', 315, 'train_loss:', 2.701039742231369, 'val_loss:', 0.87866629362106319)
('epoch', 316, 'train_loss:', 2.705052947998047, 'val_loss:', 0.87972651958465575)
('epoch', 317, 'train_loss:', 2.6992533910274505, 'val_loss:', 0.8807255351543426)
('epoch', 318, 'train_loss:', 2.6987892854213715, 'val_loss:', 0.88251337885856629)
('epoch', 319, 'train_loss:', 2.7024261152744291, 'val_loss:', 0.87954466462135317)
('epoch', 320, 'train_loss:', 2.6941634082794188, 'val_loss:', 0.88624063014984134)
('epoch', 321, 'train_loss:', 2.6954499983787539, 'val_loss:', 0.88440314650535579)
('epoch', 322, 'train_loss:', 2.6935017442703248, 'val_loss:', 0.8873147213459015)
('epoch', 323, 'train_loss:', 2.6919836890697479, 'val_loss:', 0.88169558167457585)
('epoch', 324, 'train_loss:', 2.6818826699256899, 'val_loss:', 0.8820736765861511)
('epoch', 325, 'train_loss:', 2.6866796660423278, 'val_loss:', 0.88094796061515812)
('epoch', 326, 'train_loss:', 2.6834239768981933, 'val_loss:', 0.88453960299491885)
('epoch', 327, 'train_loss:', 2.6851333367824552, 'val_loss:', 0.88393028140068053)
('epoch', 328, 'train_loss:', 2.67883443236351, 'val_loss:', 0.88361191868782041)
('epoch', 329, 'train_loss:', 2.671801152229309, 'val_loss:', 0.88774578452110287)
('epoch', 330, 'train_loss:', 2.6809543156623841, 'val_loss:', 0.88639674067497254)
('epoch', 331, 'train_loss:', 2.6692418575286867, 'val_loss:', 0.88746920824050901)
('epoch', 332, 'train_loss:', 2.6722069668769834, 'val_loss:', 0.88337097883224491)
('epoch', 333, 'train_loss:', 2.6668579244613646, 'val_loss:', 0.88603943824768061)
('epoch', 334, 'train_loss:', 2.6683986127376556, 'val_loss:', 0.89447471141815182)
('epoch', 335, 'train_loss:', 2.6650358343124392, 'val_loss:', 0.88943703413009645)
('epoch', 336, 'train_loss:', 2.6637079262733461, 'val_loss:', 0.8914129722118378)
('epoch', 337, 'train_loss:', 2.6634076368808746, 'val_loss:', 0.89405861020088195)
('epoch', 338, 'train_loss:', 2.6584703934192659, 'val_loss:', 0.89153819203376772)
('epoch', 339, 'train_loss:', 2.6601040625572203, 'val_loss:', 0.89225510120391849)
('epoch', 340, 'train_loss:', 2.6569186627864836, 'val_loss:', 0.89512932538986201)
('epoch', 341, 'train_loss:', 2.6521731424331665, 'val_loss:', 0.89450212359428405)
('epoch', 342, 'train_loss:', 2.6525607860088347, 'val_loss:', 0.8970779740810394)
('epoch', 343, 'train_loss:', 2.6507061111927031, 'val_loss:', 0.89431431889533997)
('epoch', 344, 'train_loss:', 2.6458742189407349, 'val_loss:', 0.89412371873855589)
('epoch', 345, 'train_loss:', 2.6459637629985808, 'val_loss:', 0.89435338377952578)
('epoch', 346, 'train_loss:', 2.6435653138160706, 'val_loss:', 0.89552624702453609)
('epoch', 347, 'train_loss:', 2.6353152155876161, 'val_loss:', 0.89449963331222537)
('epoch', 348, 'train_loss:', 2.6402092897891998, 'val_loss:', 0.89028801321983342)
('epoch', 349, 'train_loss:', 2.6365154027938842, 'val_loss:', 0.89699839234352108)
('epoch', 350, 'train_loss:', 2.6380945396423341, 'val_loss:', 0.89672414898872377)
('epoch', 351, 'train_loss:', 2.6324815297126771, 'val_loss:', 0.89454860091209409)
('epoch', 352, 'train_loss:', 2.6266056942939757, 'val_loss:', 0.90169512987136846)
('epoch', 353, 'train_loss:', 2.6285396039485933, 'val_loss:', 0.89518838882446294)
('epoch', 354, 'train_loss:', 2.6264913177490232, 'val_loss:', 0.90023668050765993)
('epoch', 355, 'train_loss:', 2.6260702812671664, 'val_loss:', 0.899693603515625)
('epoch', 356, 'train_loss:', 2.6236469197273253, 'val_loss:', 0.89611593008041379)
('epoch', 357, 'train_loss:', 2.6174606478214262, 'val_loss:', 0.89887638807296755)
('epoch', 358, 'train_loss:', 2.6213696694374082, 'val_loss:', 0.89775755167007443)
('epoch', 359, 'train_loss:', 2.6264495372772219, 'val_loss:', 0.90276741743087774)
('epoch', 360, 'train_loss:', 2.615817526578903, 'val_loss:', 0.90243846058845523)
('epoch', 361, 'train_loss:', 2.6146852040290831, 'val_loss:', 0.89888894200325009)
('epoch', 362, 'train_loss:', 2.6110047137737276, 'val_loss:', 0.90497528195381161)
('epoch', 363, 'train_loss:', 2.6092825782299043, 'val_loss:', 0.90236670136451724)
('epoch', 364, 'train_loss:', 2.6066000115871431, 'val_loss:', 0.90596937656402587)
('epoch', 365, 'train_loss:', 2.6070211100578309, 'val_loss:', 0.90830412626266477)
('epoch', 366, 'train_loss:', 2.6042857289314272, 'val_loss:', 0.91014173269271847)
('epoch', 367, 'train_loss:', 2.6018973278999327, 'val_loss:', 0.90753616929054259)
('epoch', 368, 'train_loss:', 2.6004550707340242, 'val_loss:', 0.9104422760009766)
('epoch', 369, 'train_loss:', 2.5965987217426298, 'val_loss:', 0.90753816723823544)
('epoch', 370, 'train_loss:', 2.5990918123722078, 'val_loss:', 0.90873391628265376)
('epoch', 371, 'train_loss:', 2.5936935472488405, 'val_loss:', 0.90903509140014649)
('epoch', 372, 'train_loss:', 2.5912931716442107, 'val_loss:', 0.9051702451705933)
('epoch', 373, 'train_loss:', 2.5897886478900909, 'val_loss:', 0.90880025506019591)
('epoch', 374, 'train_loss:', 2.5861374950408935, 'val_loss:', 0.90638223648071292)
('epoch', 375, 'train_loss:', 2.582062314748764, 'val_loss:', 0.90966801524162288)
('epoch', 376, 'train_loss:', 2.5830150866508483, 'val_loss:', 0.90718502998352046)
('epoch', 377, 'train_loss:', 2.5816674548387528, 'val_loss:', 0.91018432974815366)
('epoch', 378, 'train_loss:', 2.5808746755123138, 'val_loss:', 0.91195991277694699)
('epoch', 379, 'train_loss:', 2.5837393432855604, 'val_loss:', 0.91209992289543151)
('epoch', 380, 'train_loss:', 2.5707284843921663, 'val_loss:', 0.91191846728324888)
('epoch', 381, 'train_loss:', 2.5732286155223845, 'val_loss:', 0.91489570975303647)
('epoch', 382, 'train_loss:', 2.5766999971866609, 'val_loss:', 0.91336973547935485)
('epoch', 383, 'train_loss:', 2.5726036494970321, 'val_loss:', 0.9128307890892029)
('epoch', 384, 'train_loss:', 2.5699056529998781, 'val_loss:', 0.91413515448570248)
('epoch', 385, 'train_loss:', 2.5672592329978943, 'val_loss:', 0.92116064667701725)
('epoch', 386, 'train_loss:', 2.5616625350713731, 'val_loss:', 0.91727029561996465)
('epoch', 387, 'train_loss:', 2.5651093554496764, 'val_loss:', 0.92140992879867556)
('epoch', 388, 'train_loss:', 2.5612465018033981, 'val_loss:', 0.91400250077247625)
('epoch', 389, 'train_loss:', 2.5598891270160675, 'val_loss:', 0.91979691863059998)
('epoch', 390, 'train_loss:', 2.5576089608669279, 'val_loss:', 0.91691893696784976)
('epoch', 391, 'train_loss:', 2.5542137628793715, 'val_loss:', 0.91522981047630314)
('epoch', 392, 'train_loss:', 2.560007394552231, 'val_loss:', 0.92024212360382085)
('epoch', 393, 'train_loss:', 2.5552677929401399, 'val_loss:', 0.91822702050209049)
('epoch', 394, 'train_loss:', 2.5462624543905257, 'val_loss:', 0.92728946924209599)
('epoch', 395, 'train_loss:', 2.5486852598190306, 'val_loss:', 0.91867097616195681)
('epoch', 396, 'train_loss:', 2.5462155014276506, 'val_loss:', 0.92009265899658199)
('epoch', 397, 'train_loss:', 2.541201981306076, 'val_loss:', 0.92383472919464116)
('epoch', 398, 'train_loss:', 2.5441094785928726, 'val_loss:', 0.92286061763763427)
('epoch', 399, 'train_loss:', 2.5409625864028929, 'val_loss:', 0.922861624956131)
('epoch', 400, 'train_loss:', 2.5418374359607698, 'val_loss:', 0.92656818747520442)
('epoch', 401, 'train_loss:', 2.5399574935436249, 'val_loss:', 0.92368364095687872)
('epoch', 402, 'train_loss:', 2.5352369600534441, 'val_loss:', 0.92313826441764835)
('epoch', 403, 'train_loss:', 2.5380330294370652, 'val_loss:', 0.92534288525581365)
('epoch', 404, 'train_loss:', 2.52888724565506, 'val_loss:', 0.92921904325485227)
('epoch', 405, 'train_loss:', 2.5308524584770202, 'val_loss:', 0.92411716699600222)
('epoch', 406, 'train_loss:', 2.529194864630699, 'val_loss:', 0.92895212054252629)
('epoch', 407, 'train_loss:', 2.5303290987014773, 'val_loss:', 0.92814302921295166)
('epoch', 408, 'train_loss:', 2.5236146593093873, 'val_loss:', 0.93204609990119935)
('epoch', 409, 'train_loss:', 2.5242882347106934, 'val_loss:', 0.92742483854293822)
('epoch', 410, 'train_loss:', 2.5156231153011324, 'val_loss:', 0.93008460044860841)
('epoch', 411, 'train_loss:', 2.5222610545158388, 'val_loss:', 0.93479056954383855)
('epoch', 412, 'train_loss:', 2.5175573104619979, 'val_loss:', 0.92692614197731016)
('epoch', 413, 'train_loss:', 2.5247850579023363, 'val_loss:', 0.92859249234199526)
('epoch', 414, 'train_loss:', 2.5162521630525587, 'val_loss:', 0.93099556088447566)
('epoch', 415, 'train_loss:', 2.5147758710384367, 'val_loss:', 0.93135910749435424)
('epoch', 416, 'train_loss:', 2.5101654773950575, 'val_loss:', 0.9350167667865753)
('epoch', 417, 'train_loss:', 2.511979416012764, 'val_loss:', 0.93023125648498539)
('epoch', 418, 'train_loss:', 2.5073890691995619, 'val_loss:', 0.92698987245559694)
('epoch', 419, 'train_loss:', 2.5057066214084625, 'val_loss:', 0.93991312384605408)
('epoch', 420, 'train_loss:', 2.5049824368953706, 'val_loss:', 0.93551882147789001)
('epoch', 421, 'train_loss:', 2.4991551250219346, 'val_loss:', 0.934786102771759)
('epoch', 422, 'train_loss:', 2.5045498263835908, 'val_loss:', 0.93523041248321537)
('epoch', 423, 'train_loss:', 2.4981907469034197, 'val_loss:', 0.93711641073226926)
('epoch', 424, 'train_loss:', 2.5004493314027787, 'val_loss:', 0.93754076123237606)
('epoch', 425, 'train_loss:', 2.496077593564987, 'val_loss:', 0.93779199004173275)
('epoch', 426, 'train_loss:', 2.4883171242475508, 'val_loss:', 0.93742478489875791)
('epoch', 427, 'train_loss:', 2.4956433504819868, 'val_loss:', 0.9423166060447693)
('epoch', 428, 'train_loss:', 2.4982958579063417, 'val_loss:', 0.93429845213890073)
('epoch', 429, 'train_loss:', 2.4881003677844999, 'val_loss:', 0.93913041949272158)
('epoch', 430, 'train_loss:', 2.4880488097667692, 'val_loss:', 0.94774350047111511)
('epoch', 431, 'train_loss:', 2.4855634349584581, 'val_loss:', 0.93596505403518682)
('epoch', 432, 'train_loss:', 2.4838761192560197, 'val_loss:', 0.93774250507354739)
('epoch', 433, 'train_loss:', 2.4843042933940889, 'val_loss:', 0.94003133296966557)
('epoch', 434, 'train_loss:', 2.4807118326425552, 'val_loss:', 0.94805783987045289)
('epoch', 435, 'train_loss:', 2.4807931017875671, 'val_loss:', 0.94567677140235906)
('epoch', 436, 'train_loss:', 2.4812476414442064, 'val_loss:', 0.94607424974441523)
('epoch', 437, 'train_loss:', 2.4715782004594802, 'val_loss:', 0.94929956555366513)
('epoch', 438, 'train_loss:', 2.4755219733715057, 'val_loss:', 0.94835161685943603)
('epoch', 439, 'train_loss:', 2.4708101040124895, 'val_loss:', 0.94798280119895939)
('epoch', 440, 'train_loss:', 2.464293311238289, 'val_loss:', 0.94594075202941896)
('epoch', 441, 'train_loss:', 2.4647639924287796, 'val_loss:', 0.94707035779953008)
('epoch', 442, 'train_loss:', 2.468435814380646, 'val_loss:', 0.94448338031768797)
('epoch', 443, 'train_loss:', 2.4700212210416792, 'val_loss:', 0.9482016789913178)
('epoch', 444, 'train_loss:', 2.4617750525474547, 'val_loss:', 0.94803948402404781)
('epoch', 445, 'train_loss:', 2.4606319975852968, 'val_loss:', 0.95043022036552427)
('epoch', 446, 'train_loss:', 2.4607066488265992, 'val_loss:', 0.95151512742042543)
('epoch', 447, 'train_loss:', 2.4625664639472959, 'val_loss:', 0.9480441749095917)
('epoch', 448, 'train_loss:', 2.4605137676000597, 'val_loss:', 0.94733928918838506)
('epoch', 449, 'train_loss:', 2.4598784017562867, 'val_loss:', 0.9518724107742309)
('epoch', 450, 'train_loss:', 2.4573421066999437, 'val_loss:', 0.95295755743980404)
('epoch', 451, 'train_loss:', 2.4474973315000534, 'val_loss:', 0.94810140132904053)
('epoch', 452, 'train_loss:', 2.4545437896251681, 'val_loss:', 0.95281514048576355)
('epoch', 453, 'train_loss:', 2.4518281745910646, 'val_loss:', 0.95393835425376894)
('epoch', 454, 'train_loss:', 2.4466642743349074, 'val_loss:', 0.95086636304855343)
('epoch', 455, 'train_loss:', 2.4467102801799774, 'val_loss:', 0.95683119416236873)
('epoch', 456, 'train_loss:', 2.4452296847105028, 'val_loss:', 0.9531503295898438)
('epoch', 457, 'train_loss:', 2.4440299588441849, 'val_loss:', 0.9573654413223267)
('epoch', 458, 'train_loss:', 2.444035209417343, 'val_loss:', 0.95772280573844915)
('epoch', 459, 'train_loss:', 2.43271722137928, 'val_loss:', 0.95732659101486206)
('epoch', 460, 'train_loss:', 2.4372282314300535, 'val_loss:', 0.9564374041557312)
('epoch', 461, 'train_loss:', 2.4370434099435805, 'val_loss:', 0.95756019592285158)
('epoch', 462, 'train_loss:', 2.4347776854038239, 'val_loss:', 0.9582990181446075)
('epoch', 463, 'train_loss:', 2.4345751410722731, 'val_loss:', 0.96560685396194457)
('epoch', 464, 'train_loss:', 2.4372110682725907, 'val_loss:', 0.95772157669067381)
('epoch', 465, 'train_loss:', 2.4309114778041838, 'val_loss:', 0.9630377149581909)
('epoch', 466, 'train_loss:', 2.4303955715894698, 'val_loss:', 0.96164776325225831)
('epoch', 467, 'train_loss:', 2.4243654584884644, 'val_loss:', 0.96510871171951296)
('epoch', 468, 'train_loss:', 2.4239460277557372, 'val_loss:', 0.96196687340736386)
('epoch', 469, 'train_loss:', 2.4242899262905122, 'val_loss:', 0.96327436923980714)
('epoch', 470, 'train_loss:', 2.4160632389783858, 'val_loss:', 0.95930800557136531)
('epoch', 471, 'train_loss:', 2.418933825492859, 'val_loss:', 0.96303222656249998)
('epoch', 472, 'train_loss:', 2.4181689405441285, 'val_loss:', 0.96020254254341131)
('epoch', 473, 'train_loss:', 2.4249774521589278, 'val_loss:', 0.96227757215499876)
('epoch', 474, 'train_loss:', 2.4178636091947556, 'val_loss:', 0.9643516278266907)
('epoch', 475, 'train_loss:', 2.4187195676565172, 'val_loss:', 0.96290235996246343)
('epoch', 476, 'train_loss:', 2.4175139117240905, 'val_loss:', 0.96541018962860103)
('epoch', 477, 'train_loss:', 2.4111215525865557, 'val_loss:', 0.96450487375259397)
('epoch', 478, 'train_loss:', 2.4073552203178408, 'val_loss:', 0.96121323704719541)
('epoch', 479, 'train_loss:', 2.4089718329906464, 'val_loss:', 0.96629726409912109)
('epoch', 480, 'train_loss:', 2.4057006001472474, 'val_loss:', 0.96558186411857605)
('epoch', 481, 'train_loss:', 2.4091513323783875, 'val_loss:', 0.9651678287982941)
('epoch', 482, 'train_loss:', 2.4061195695400239, 'val_loss:', 0.9655547714233399)
('epoch', 483, 'train_loss:', 2.4010914021730425, 'val_loss:', 0.97506207942962642)
('epoch', 484, 'train_loss:', 2.4027656960487365, 'val_loss:', 0.96912921786308293)
('epoch', 485, 'train_loss:', 2.3977129399776458, 'val_loss:', 0.97385702133178709)
('epoch', 486, 'train_loss:', 2.4011713975667952, 'val_loss:', 0.97235716700553898)
('epoch', 487, 'train_loss:', 2.3900983214378355, 'val_loss:', 0.97292830705642697)
('epoch', 488, 'train_loss:', 2.3956046277284622, 'val_loss:', 0.97264589071273799)
('epoch', 489, 'train_loss:', 2.3927607548236849, 'val_loss:', 0.97793764352798462)
('epoch', 490, 'train_loss:', 2.3908624732494355, 'val_loss:', 0.97122219920158381)
('epoch', 491, 'train_loss:', 2.3871874886751177, 'val_loss:', 0.96710009574890132)
('epoch', 492, 'train_loss:', 2.3886972415447234, 'val_loss:', 0.97516919255256651)
('epoch', 493, 'train_loss:', 2.3905525082349777, 'val_loss:', 0.97705702424049379)
('epoch', 494, 'train_loss:', 2.3877723950147627, 'val_loss:', 0.97553500056266784)
('epoch', 495, 'train_loss:', 2.3786450666189194, 'val_loss:', 0.97524734497070309)
('epoch', 496, 'train_loss:', 2.3844864749908448, 'val_loss:', 0.97795835971832279)
('epoch', 497, 'train_loss:', 2.3817209690809249, 'val_loss:', 0.97241889834403994)
('epoch', 498, 'train_loss:', 2.3785155993700027, 'val_loss:', 0.97424339175224306)
('epoch', 499, 'train_loss:', 2.3761542600393297, 'val_loss:', 0.97804832577705381)
