(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=1 python lstm-additive-forget-no-sigmoid-linearapprox.py -t
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2382 get requests, put_count=2222 evicted_count=1000 eviction_rate=0.450045 and unsatisfied allocation rate=0.528967
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2441 get requests, put_count=2487 evicted_count=1000 eviction_rate=0.402091 and unsatisfied allocation rate=0.400246
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4776 get requests, put_count=4671 evicted_count=1000 eviction_rate=0.214087 and unsatisfied allocation rate=0.243719
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 0, 'train_loss:', 7.8723027682304378, 'val_loss:', 1.9180348181724549)
('epoch', 1, 'train_loss:', 7.6347425031661986, 'val_loss:', 1.8941278457641602)
('epoch', 2, 'train_loss:', 7.4856226968765256, 'val_loss:', 1.8424796485900878)
('epoch', 3, 'train_loss:', 7.1797299861907957, 'val_loss:', 1.7546909403800965)
('epoch', 4, 'train_loss:', 6.9143056631088253, 'val_loss:', 1.7060860347747804)
('epoch', 5, 'train_loss:', 6.7651317858695981, 'val_loss:', 1.6733455061912537)
('epoch', 6, 'train_loss:', 6.6413744115829472, 'val_loss:', 1.6496629834175109)
('epoch', 7, 'train_loss:', 6.5463239812850951, 'val_loss:', 1.6264916849136353)
('epoch', 8, 'train_loss:', 6.4724944472312931, 'val_loss:', 1.6090337657928466)
('epoch', 9, 'train_loss:', 6.4211617970466612, 'val_loss:', 1.598157901763916)
('epoch', 10, 'train_loss:', 6.3783921575546261, 'val_loss:', 1.586993248462677)
('epoch', 11, 'train_loss:', 6.3393614244461061, 'val_loss:', 1.5786155772209167)
('epoch', 12, 'train_loss:', 6.3071347284317021, 'val_loss:', 1.5700361371040343)
('epoch', 13, 'train_loss:', 6.2798584723472599, 'val_loss:', 1.5650666308403016)
('epoch', 14, 'train_loss:', 6.2561147785186764, 'val_loss:', 1.5616495704650879)
('epoch', 15, 'train_loss:', 6.239851682186127, 'val_loss:', 1.5548934960365295)
('epoch', 16, 'train_loss:', 6.2139159989356996, 'val_loss:', 1.550783383846283)
('epoch', 17, 'train_loss:', 6.208919222354889, 'val_loss:', 1.5444315671920776)
('epoch', 18, 'train_loss:', 6.1859195852279667, 'val_loss:', 1.545491750240326)
('epoch', 19, 'train_loss:', 6.1748326492309573, 'val_loss:', 1.5382593035697938)
('epoch', 20, 'train_loss:', 6.1634915089607238, 'val_loss:', 1.5362243390083312)
('epoch', 21, 'train_loss:', 6.144713385105133, 'val_loss:', 1.5325717329978943)
('epoch', 22, 'train_loss:', 6.1285734963417049, 'val_loss:', 1.5265514588356017)
('epoch', 23, 'train_loss:', 6.107715067863464, 'val_loss:', 1.5231903505325317)
('epoch', 24, 'train_loss:', 6.0988350439071652, 'val_loss:', 1.5224497866630555)
('epoch', 25, 'train_loss:', 6.0884541797637937, 'val_loss:', 1.5193648743629455)
('epoch', 26, 'train_loss:', 6.0709967470169071, 'val_loss:', 1.5175156164169312)
('epoch', 27, 'train_loss:', 6.0595408558845518, 'val_loss:', 1.5082520842552185)
('epoch', 28, 'train_loss:', 6.043087177276611, 'val_loss:', 1.5060599875450134)
('epoch', 29, 'train_loss:', 6.0311992621421817, 'val_loss:', 1.5062269473075867)
('epoch', 30, 'train_loss:', 6.0156804037094114, 'val_loss:', 1.5013631749153138)
('epoch', 31, 'train_loss:', 6.007085382938385, 'val_loss:', 1.4987681722640991)
('epoch', 32, 'train_loss:', 5.9915801024436952, 'val_loss:', 1.4949446082115174)
('epoch', 33, 'train_loss:', 5.9787797617912295, 'val_loss:', 1.4939414501190185)
('epoch', 34, 'train_loss:', 5.9640544366836545, 'val_loss:', 1.4829047632217407)
('epoch', 35, 'train_loss:', 5.955448443889618, 'val_loss:', 1.4828997373580932)
('epoch', 36, 'train_loss:', 5.939505531787872, 'val_loss:', 1.4806144881248473)
('epoch', 37, 'train_loss:', 5.9224499845504761, 'val_loss:', 1.4759812140464783)
('epoch', 38, 'train_loss:', 5.9111606788635251, 'val_loss:', 1.4746730780601502)
('epoch', 39, 'train_loss:', 5.9012459349632262, 'val_loss:', 1.4721540164947511)
('epoch', 40, 'train_loss:', 5.890999519824982, 'val_loss:', 1.4706432700157166)
('epoch', 41, 'train_loss:', 5.8768296790122987, 'val_loss:', 1.4682283115386963)
('epoch', 42, 'train_loss:', 5.8623335766792302, 'val_loss:', 1.4650939679145814)
('epoch', 43, 'train_loss:', 5.8608336234092713, 'val_loss:', 1.4631016373634338)
('epoch', 44, 'train_loss:', 5.8448182916641231, 'val_loss:', 1.4622191095352173)
('epoch', 45, 'train_loss:', 5.8354202103614803, 'val_loss:', 1.455282073020935)
('epoch', 46, 'train_loss:', 5.8220535373687747, 'val_loss:', 1.4558425831794739)
('epoch', 47, 'train_loss:', 5.8059091329574581, 'val_loss:', 1.4500323057174682)
('epoch', 48, 'train_loss:', 5.7975118637084959, 'val_loss:', 1.44571302652359)
('epoch', 49, 'train_loss:', 5.7874027156829833, 'val_loss:', 1.4433309817314148)
('epoch', 50, 'train_loss:', 5.7760681605339048, 'val_loss:', 1.4424703121185303)
('epoch', 51, 'train_loss:', 5.7585533666610722, 'val_loss:', 1.4391766691207886)
('epoch', 52, 'train_loss:', 5.7428299903869631, 'val_loss:', 1.4316783475875854)
('epoch', 53, 'train_loss:', 5.7303267478942868, 'val_loss:', 1.4304291343688964)
('epoch', 54, 'train_loss:', 5.7263015365600589, 'val_loss:', 1.4350358748435974)
('epoch', 55, 'train_loss:', 5.7034868001937866, 'val_loss:', 1.4226414036750794)
('epoch', 56, 'train_loss:', 5.6948638033866885, 'val_loss:', 1.4200978422164916)
('epoch', 57, 'train_loss:', 5.67404531955719, 'val_loss:', 1.4157646298408508)
('epoch', 58, 'train_loss:', 5.6603569698333738, 'val_loss:', 1.4131682777404786)
('epoch', 59, 'train_loss:', 5.6427243018150328, 'val_loss:', 1.4099979281425477)
('epoch', 60, 'train_loss:', 5.6324238586425786, 'val_loss:', 1.4008982038497926)
('epoch', 61, 'train_loss:', 5.6130960822105411, 'val_loss:', 1.404484875202179)
('epoch', 62, 'train_loss:', 5.6019555044174192, 'val_loss:', 1.4002345967292786)
('epoch', 63, 'train_loss:', 5.5891299867630009, 'val_loss:', 1.3960856533050536)
('epoch', 64, 'train_loss:', 5.5719719624519346, 'val_loss:', 1.3935597562789916)
('epoch', 65, 'train_loss:', 5.5609829902648924, 'val_loss:', 1.3871472525596618)
('epoch', 66, 'train_loss:', 5.5502972269058226, 'val_loss:', 1.3833617949485779)
('epoch', 67, 'train_loss:', 5.5288749551773071, 'val_loss:', 1.3806244087219239)
('epoch', 68, 'train_loss:', 5.5195659375190731, 'val_loss:', 1.3768780183792115)
('epoch', 69, 'train_loss:', 5.5057154679298401, 'val_loss:', 1.379384582042694)
('epoch', 70, 'train_loss:', 5.494681458473206, 'val_loss:', 1.3680636119842529)
('epoch', 71, 'train_loss:', 5.4816089081764225, 'val_loss:', 1.3691692113876344)
('epoch', 72, 'train_loss:', 5.4702207207679745, 'val_loss:', 1.3639045929908753)
('epoch', 73, 'train_loss:', 5.4588999176025395, 'val_loss:', 1.3658464550971985)
('epoch', 74, 'train_loss:', 5.4489644670486452, 'val_loss:', 1.3619730591773986)
('epoch', 75, 'train_loss:', 5.434645290374756, 'val_loss:', 1.3590584635734557)
('epoch', 76, 'train_loss:', 5.426528925895691, 'val_loss:', 1.3535605835914613)
('epoch', 77, 'train_loss:', 5.4189483356475829, 'val_loss:', 1.3511421251296998)
('epoch', 78, 'train_loss:', 5.4066360807418823, 'val_loss:', 1.347565038204193)
('epoch', 79, 'train_loss:', 5.3982478904724118, 'val_loss:', 1.3464930224418641)
('epoch', 80, 'train_loss:', 5.3851997280120854, 'val_loss:', 1.3432131266593934)
('epoch', 81, 'train_loss:', 5.3791749548912051, 'val_loss:', 1.3412966728210449)
('epoch', 82, 'train_loss:', 5.3623542118072507, 'val_loss:', 1.337576277256012)
('epoch', 83, 'train_loss:', 5.3523414301872254, 'val_loss:', 1.3362162542343139)
('epoch', 84, 'train_loss:', 5.337855370044708, 'val_loss:', 1.3354511380195617)
('epoch', 85, 'train_loss:', 5.3369162321090702, 'val_loss:', 1.3325812387466431)
('epoch', 86, 'train_loss:', 5.3221960163116453, 'val_loss:', 1.3288232469558716)
('epoch', 87, 'train_loss:', 5.3127284908294676, 'val_loss:', 1.3293009567260743)
('epoch', 88, 'train_loss:', 5.3017469739913938, 'val_loss:', 1.3234290456771851)
('epoch', 89, 'train_loss:', 5.2898681449890139, 'val_loss:', 1.323592038154602)
('epoch', 90, 'train_loss:', 5.2845223689079281, 'val_loss:', 1.3161113977432251)
('epoch', 91, 'train_loss:', 5.2768199014663697, 'val_loss:', 1.314976692199707)
('epoch', 92, 'train_loss:', 5.261622986793518, 'val_loss:', 1.3136578464508057)
('epoch', 93, 'train_loss:', 5.2505227947235111, 'val_loss:', 1.3106387805938722)
('epoch', 94, 'train_loss:', 5.2441744518280027, 'val_loss:', 1.3078908371925353)
('epoch', 95, 'train_loss:', 5.2418157196044923, 'val_loss:', 1.3056668424606324)
('epoch', 96, 'train_loss:', 5.2226220989227299, 'val_loss:', 1.3021723294258118)
('epoch', 97, 'train_loss:', 5.2129829978942874, 'val_loss:', 1.3029665303230287)
('epoch', 98, 'train_loss:', 5.2026413226127621, 'val_loss:', 1.2951395153999328)
('epoch', 99, 'train_loss:', 5.1882168149948118, 'val_loss:', 1.2973495125770569)
('epoch', 100, 'train_loss:', 5.1867121911048892, 'val_loss:', 1.2926170945167541)
('epoch', 101, 'train_loss:', 5.1693871593475338, 'val_loss:', 1.2902609705924988)
('epoch', 102, 'train_loss:', 5.1599131369590756, 'val_loss:', 1.2874200677871703)
('epoch', 103, 'train_loss:', 5.1538286590576172, 'val_loss:', 1.2857277083396912)
('epoch', 104, 'train_loss:', 5.1443176078796391, 'val_loss:', 1.2838280701637268)
('epoch', 105, 'train_loss:', 5.13169264793396, 'val_loss:', 1.2808129811286926)
('epoch', 106, 'train_loss:', 5.1195790398120877, 'val_loss:', 1.2823585677146911)
('epoch', 107, 'train_loss:', 5.1143506264686582, 'val_loss:', 1.2782750964164733)
('epoch', 108, 'train_loss:', 5.1035170054435728, 'val_loss:', 1.2734676432609557)
('epoch', 109, 'train_loss:', 5.0939586341381071, 'val_loss:', 1.2693389284610748)
('epoch', 110, 'train_loss:', 5.0863531136512758, 'val_loss:', 1.2684301853179931)
('epoch', 111, 'train_loss:', 5.0752508974075319, 'val_loss:', 1.2697165846824645)
('epoch', 112, 'train_loss:', 5.0640418255329136, 'val_loss:', 1.2626707708835603)
('epoch', 113, 'train_loss:', 5.0539559948444364, 'val_loss:', 1.261243702173233)
('epoch', 114, 'train_loss:', 5.0494351971149447, 'val_loss:', 1.2597545874118805)
('epoch', 115, 'train_loss:', 5.0396445107460019, 'val_loss:', 1.2562388026714324)
('epoch', 116, 'train_loss:', 5.0282795774936675, 'val_loss:', 1.2565783584117889)
('epoch', 117, 'train_loss:', 5.024172259569168, 'val_loss:', 1.2515022647380829)
('epoch', 118, 'train_loss:', 5.0116531038284302, 'val_loss:', 1.2535799276828765)
('epoch', 119, 'train_loss:', 5.0016561222076419, 'val_loss:', 1.2488333773612976)
('epoch', 120, 'train_loss:', 4.992355434894562, 'val_loss:', 1.2468637943267822)
('epoch', 121, 'train_loss:', 4.9903118574619292, 'val_loss:', 1.2460861575603486)
('epoch', 122, 'train_loss:', 4.9776709592342376, 'val_loss:', 1.2428786098957061)
('epoch', 123, 'train_loss:', 4.9631481385231018, 'val_loss:', 1.2408203101158142)
('epoch', 124, 'train_loss:', 4.9614051222801212, 'val_loss:', 1.2386484599113465)
('epoch', 125, 'train_loss:', 4.9517441165447238, 'val_loss:', 1.2384596467018127)
('epoch', 126, 'train_loss:', 4.9483834755420686, 'val_loss:', 1.2307652223110199)
('epoch', 127, 'train_loss:', 4.9334088671207432, 'val_loss:', 1.2315862810611724)
('epoch', 128, 'train_loss:', 4.9302244400978088, 'val_loss:', 1.2287621355056764)
('epoch', 129, 'train_loss:', 4.9247628390789036, 'val_loss:', 1.2277163887023925)
('epoch', 130, 'train_loss:', 4.9081544101238253, 'val_loss:', 1.2280381882190705)
('epoch', 131, 'train_loss:', 4.8998475098609928, 'val_loss:', 1.2235772490501404)
('epoch', 132, 'train_loss:', 4.8925957620143894, 'val_loss:', 1.2218455004692077)
('epoch', 133, 'train_loss:', 4.8875087726116178, 'val_loss:', 1.2222614312171936)
('epoch', 134, 'train_loss:', 4.8804973518848422, 'val_loss:', 1.2183776199817657)
('epoch', 135, 'train_loss:', 4.8719181632995605, 'val_loss:', 1.2201177680492401)
('epoch', 136, 'train_loss:', 4.8719287323951725, 'val_loss:', 1.2166478407382966)
('epoch', 137, 'train_loss:', 4.8568670380115506, 'val_loss:', 1.2126208806037904)
('epoch', 138, 'train_loss:', 4.8533681309223171, 'val_loss:', 1.2101427233219146)
('epoch', 139, 'train_loss:', 4.8414088189601898, 'val_loss:', 1.2094848430156708)
('epoch', 140, 'train_loss:', 4.8354251968860629, 'val_loss:', 1.2073365986347198)
('epoch', 141, 'train_loss:', 4.8263215148448948, 'val_loss:', 1.2061277067661285)
('epoch', 142, 'train_loss:', 4.8231652724742888, 'val_loss:', 1.2065543842315674)
('epoch', 143, 'train_loss:', 4.8142249989509587, 'val_loss:', 1.2035072612762452)
('epoch', 144, 'train_loss:', 4.8072519719600679, 'val_loss:', 1.203953629732132)
('epoch', 145, 'train_loss:', 4.8047982192039491, 'val_loss:', 1.1957159900665284)
('epoch', 146, 'train_loss:', 4.7875065696239467, 'val_loss:', 1.1950925993919372)
('epoch', 147, 'train_loss:', 4.7866626060008999, 'val_loss:', 1.1936457204818725)
('epoch', 148, 'train_loss:', 4.7745013320446015, 'val_loss:', 1.1934802222251892)
('epoch', 149, 'train_loss:', 4.7746147871017453, 'val_loss:', 1.1927351343631745)
('epoch', 150, 'train_loss:', 4.7619383370876314, 'val_loss:', 1.1906147813796997)
('epoch', 151, 'train_loss:', 4.7555402231216428, 'val_loss:', 1.1897004091739654)
('epoch', 152, 'train_loss:', 4.7488408875465389, 'val_loss:', 1.1910792970657349)
('epoch', 153, 'train_loss:', 4.7483158278465272, 'val_loss:', 1.1854871678352357)
('epoch', 154, 'train_loss:', 4.7312751471996304, 'val_loss:', 1.1836641967296599)
('epoch', 155, 'train_loss:', 4.7335451042652128, 'val_loss:', 1.1817925250530243)
('epoch', 156, 'train_loss:', 4.722085472345352, 'val_loss:', 1.180548119544983)
('epoch', 157, 'train_loss:', 4.7184378874301913, 'val_loss:', 1.1807094538211822)
('epoch', 158, 'train_loss:', 4.7103849935531619, 'val_loss:', 1.1791161656379701)
('epoch', 159, 'train_loss:', 4.7005352818965909, 'val_loss:', 1.1731753957271576)
('epoch', 160, 'train_loss:', 4.6985973787307742, 'val_loss:', 1.1747178387641908)
('epoch', 161, 'train_loss:', 4.6901754653453827, 'val_loss:', 1.173225336074829)
('epoch', 162, 'train_loss:', 4.6843323457241057, 'val_loss:', 1.1755631613731383)
('epoch', 163, 'train_loss:', 4.6816229176521302, 'val_loss:', 1.170105024576187)
('epoch', 164, 'train_loss:', 4.6772935044765473, 'val_loss:', 1.1646325647830964)
('epoch', 165, 'train_loss:', 4.6591610991954804, 'val_loss:', 1.1670629537105561)
('epoch', 166, 'train_loss:', 4.6612705409526827, 'val_loss:', 1.164936306476593)
('epoch', 167, 'train_loss:', 4.649662971496582, 'val_loss:', 1.1600973474979401)
('epoch', 168, 'train_loss:', 4.6466856205463412, 'val_loss:', 1.1622421312332154)
('epoch', 169, 'train_loss:', 4.6452804648876187, 'val_loss:', 1.1607233273983002)
('epoch', 170, 'train_loss:', 4.6340298509597782, 'val_loss:', 1.1605212616920471)
('epoch', 171, 'train_loss:', 4.6292511153221128, 'val_loss:', 1.1568674552440643)
('epoch', 172, 'train_loss:', 4.620286685228348, 'val_loss:', 1.1575303554534913)
('epoch', 173, 'train_loss:', 4.6219191169738769, 'val_loss:', 1.1567338490486145)
('epoch', 174, 'train_loss:', 4.6117888116836552, 'val_loss:', 1.1518898415565491)
('epoch', 175, 'train_loss:', 4.6055887937545776, 'val_loss:', 1.1519288682937623)
('epoch', 176, 'train_loss:', 4.5924127101898193, 'val_loss:', 1.1485894024372101)
('epoch', 177, 'train_loss:', 4.5945957529544827, 'val_loss:', 1.1466163635253905)
('epoch', 178, 'train_loss:', 4.5904931640625, 'val_loss:', 1.1475309324264527)
('epoch', 179, 'train_loss:', 4.580236104726791, 'val_loss:', 1.1445510923862456)
('epoch', 180, 'train_loss:', 4.5761945927143097, 'val_loss:', 1.144426441192627)
('epoch', 181, 'train_loss:', 4.5704244756698609, 'val_loss:', 1.1434738087654113)
('epoch', 182, 'train_loss:', 4.5616618156433102, 'val_loss:', 1.1398555719852448)
('epoch', 183, 'train_loss:', 4.5518828332424164, 'val_loss:', 1.1399398159980774)
('epoch', 184, 'train_loss:', 4.5513153314590458, 'val_loss:', 1.1388737010955809)
('epoch', 185, 'train_loss:', 4.5446660256385805, 'val_loss:', 1.1357140910625458)
('epoch', 186, 'train_loss:', 4.5384475433826443, 'val_loss:', 1.1351298999786377)
('epoch', 187, 'train_loss:', 4.5335720264911652, 'val_loss:', 1.1326596057415008)
('epoch', 188, 'train_loss:', 4.5245346021652217, 'val_loss:', 1.1340228080749513)
('epoch', 189, 'train_loss:', 4.5259750115871427, 'val_loss:', 1.1286823630332947)
('epoch', 190, 'train_loss:', 4.5176470708847045, 'val_loss:', 1.1285252439975739)
('epoch', 191, 'train_loss:', 4.5155417203903196, 'val_loss:', 1.1282651925086975)
('epoch', 192, 'train_loss:', 4.5077364265918733, 'val_loss:', 1.1236744213104248)
('epoch', 193, 'train_loss:', 4.4973342072963716, 'val_loss:', 1.1217459976673125)
('epoch', 194, 'train_loss:', 4.4923705792427064, 'val_loss:', 1.1245735323429107)
('epoch', 195, 'train_loss:', 4.4825370156764981, 'val_loss:', 1.1239543640613556)
('epoch', 196, 'train_loss:', 4.4869786417484283, 'val_loss:', 1.1213099801540374)
('epoch', 197, 'train_loss:', 4.47761545419693, 'val_loss:', 1.1177600765228271)
('epoch', 198, 'train_loss:', 4.4690444231033322, 'val_loss:', 1.1178216326236725)
('epoch', 199, 'train_loss:', 4.4678572642803189, 'val_loss:', 1.1172784519195558)
('epoch', 200, 'train_loss:', 4.4587871575355527, 'val_loss:', 1.1169285571575165)
('epoch', 201, 'train_loss:', 4.4520081579685211, 'val_loss:', 1.112264643907547)
('epoch', 202, 'train_loss:', 4.4469292306900021, 'val_loss:', 1.1142985117435455)
('epoch', 203, 'train_loss:', 4.4403672504425051, 'val_loss:', 1.1111900591850281)
('epoch', 204, 'train_loss:', 4.4347570216655727, 'val_loss:', 1.1077832508087158)
('epoch', 205, 'train_loss:', 4.4284428095817567, 'val_loss:', 1.1078545880317687)
('epoch', 206, 'train_loss:', 4.4281164813041691, 'val_loss:', 1.107599015235901)
('epoch', 207, 'train_loss:', 4.4199280941486361, 'val_loss:', 1.1090087485313416)
('epoch', 208, 'train_loss:', 4.4167131817340852, 'val_loss:', 1.1040019953250886)
('epoch', 209, 'train_loss:', 4.408193476200104, 'val_loss:', 1.1051533782482148)
('epoch', 210, 'train_loss:', 4.402621669769287, 'val_loss:', 1.1044844126701354)
('epoch', 211, 'train_loss:', 4.4027476131916048, 'val_loss:', 1.1004026865959167)
('epoch', 212, 'train_loss:', 4.3918546319007872, 'val_loss:', 1.1005084788799286)
('epoch', 213, 'train_loss:', 4.3866354417800899, 'val_loss:', 1.0994138777256013)
('epoch', 214, 'train_loss:', 4.3854219448566436, 'val_loss:', 1.1010890340805053)
('epoch', 215, 'train_loss:', 4.3805768859386447, 'val_loss:', 1.0953354847431183)
('epoch', 216, 'train_loss:', 4.3734626007080077, 'val_loss:', 1.0950370657444)
('epoch', 217, 'train_loss:', 4.3677540540695192, 'val_loss:', 1.0906238973140716)
('epoch', 218, 'train_loss:', 4.3621890771389005, 'val_loss:', 1.0913129377365112)
('epoch', 219, 'train_loss:', 4.3577059757709504, 'val_loss:', 1.0929999041557312)
('epoch', 220, 'train_loss:', 4.3498330187797549, 'val_loss:', 1.0885934400558472)
('epoch', 221, 'train_loss:', 4.3516799020767216, 'val_loss:', 1.0891394114494324)
('epoch', 222, 'train_loss:', 4.3478778636455537, 'val_loss:', 1.0869035196304322)
('epoch', 223, 'train_loss:', 4.3377986049652097, 'val_loss:', 1.0859834349155426)
('epoch', 224, 'train_loss:', 4.3340235495567319, 'val_loss:', 1.0868798696994781)
('epoch', 225, 'train_loss:', 4.3243373060226444, 'val_loss:', 1.0827614235877991)
('epoch', 226, 'train_loss:', 4.3194027042388914, 'val_loss:', 1.0834606456756593)
('epoch', 227, 'train_loss:', 4.3162610673904416, 'val_loss:', 1.0814895606040955)
('epoch', 228, 'train_loss:', 4.3112260806560521, 'val_loss:', 1.0804027736186981)
('epoch', 229, 'train_loss:', 4.3148129141330722, 'val_loss:', 1.0798989582061767)
('epoch', 230, 'train_loss:', 4.3044530022144318, 'val_loss:', 1.0767672502994536)
('epoch', 231, 'train_loss:', 4.3086253821849825, 'val_loss:', 1.0779007124900817)
('epoch', 232, 'train_loss:', 4.2972654116153715, 'val_loss:', 1.0712098073959351)
('epoch', 233, 'train_loss:', 4.2869718897342679, 'val_loss:', 1.0720074474811554)
('epoch', 234, 'train_loss:', 4.2863032710552211, 'val_loss:', 1.0748222684860229)
('epoch', 235, 'train_loss:', 4.2800232040882111, 'val_loss:', 1.0724123084545136)
('epoch', 236, 'train_loss:', 4.2700362432003018, 'val_loss:', 1.0682531034946441)
('epoch', 237, 'train_loss:', 4.2776418542861938, 'val_loss:', 1.0677467536926271)
('epoch', 238, 'train_loss:', 4.2681882846355439, 'val_loss:', 1.0728044080734254)
('epoch', 239, 'train_loss:', 4.2614916634559634, 'val_loss:', 1.066038578748703)
('epoch', 240, 'train_loss:', 4.2633730113506321, 'val_loss:', 1.0654877030849457)
('epoch', 241, 'train_loss:', 4.2608521628379821, 'val_loss:', 1.0632134866714478)
('epoch', 242, 'train_loss:', 4.2494557285308838, 'val_loss:', 1.0645420610904694)
('epoch', 243, 'train_loss:', 4.2499751079082486, 'val_loss:', 1.0638363230228425)
('epoch', 244, 'train_loss:', 4.2410540807247159, 'val_loss:', 1.0623439717292786)
('epoch', 245, 'train_loss:', 4.2369837141036983, 'val_loss:', 1.0627487349510192)
('epoch', 246, 'train_loss:', 4.2397701799869534, 'val_loss:', 1.0597122621536255)
('epoch', 247, 'train_loss:', 4.2351481091976169, 'val_loss:', 1.0593317329883576)
('epoch', 248, 'train_loss:', 4.2341971647739411, 'val_loss:', 1.0589369118213654)
('epoch', 249, 'train_loss:', 4.2199090862274167, 'val_loss:', 1.0606464815139771)
('epoch', 250, 'train_loss:', 4.2241608715057373, 'val_loss:', 1.0587688660621644)
('epoch', 251, 'train_loss:', 4.2186731886863704, 'val_loss:', 1.0564081096649169)
('epoch', 252, 'train_loss:', 4.2109499800205228, 'val_loss:', 1.057860907316208)
('epoch', 253, 'train_loss:', 4.2093964362144467, 'val_loss:', 1.054882664680481)
('epoch', 254, 'train_loss:', 4.2033916294574736, 'val_loss:', 1.0535378420352937)
('epoch', 255, 'train_loss:', 4.2049584627151493, 'val_loss:', 1.0532710695266723)
('epoch', 256, 'train_loss:', 4.1947151744365696, 'val_loss:', 1.0468939995765687)
('epoch', 257, 'train_loss:', 4.1913584685325622, 'val_loss:', 1.050363940000534)
('epoch', 258, 'train_loss:', 4.1899482166767124, 'val_loss:', 1.049227350950241)
('epoch', 259, 'train_loss:', 4.1843291342258455, 'val_loss:', 1.0495570969581605)
('epoch', 260, 'train_loss:', 4.180768702030182, 'val_loss:', 1.046426179409027)
('epoch', 261, 'train_loss:', 4.174261109828949, 'val_loss:', 1.0465036571025848)
('epoch', 262, 'train_loss:', 4.1716599285602571, 'val_loss:', 1.0448137044906616)
('epoch', 263, 'train_loss:', 4.1625793039798733, 'val_loss:', 1.0428551578521728)
('epoch', 264, 'train_loss:', 4.1588064658641812, 'val_loss:', 1.0452278745174408)
('epoch', 265, 'train_loss:', 4.1582176196575169, 'val_loss:', 1.0418295693397521)
('epoch', 266, 'train_loss:', 4.1584676897525785, 'val_loss:', 1.039710783958435)
('epoch', 267, 'train_loss:', 4.1554501056671143, 'val_loss:', 1.041198774576187)
('epoch', 268, 'train_loss:', 4.1527921462059023, 'val_loss:', 1.041493957042694)
('epoch', 269, 'train_loss:', 4.1483226954936985, 'val_loss:', 1.0360871291160583)
('epoch', 270, 'train_loss:', 4.1415874600410465, 'val_loss:', 1.0423041081428528)
('epoch', 271, 'train_loss:', 4.1462108540534972, 'val_loss:', 1.0376384103298186)
('epoch', 272, 'train_loss:', 4.1400406730175021, 'val_loss:', 1.0353875470161438)
('epoch', 273, 'train_loss:', 4.1262319970130923, 'val_loss:', 1.0379827463626861)
('epoch', 274, 'train_loss:', 4.1328819131851198, 'val_loss:', 1.0347303390502929)
('epoch', 275, 'train_loss:', 4.1244334089756016, 'val_loss:', 1.0329777741432189)
('epoch', 276, 'train_loss:', 4.1208424425125125, 'val_loss:', 1.0391459715366365)
('epoch', 277, 'train_loss:', 4.1198257339000701, 'val_loss:', 1.0333631920814514)
('epoch', 278, 'train_loss:', 4.1136926412582397, 'val_loss:', 1.0334298944473266)
('epoch', 279, 'train_loss:', 4.117936899662018, 'val_loss:', 1.030172278881073)
('epoch', 280, 'train_loss:', 4.1149803853034976, 'val_loss:', 1.0287483930587769)
('epoch', 281, 'train_loss:', 4.1095223128795624, 'val_loss:', 1.0286307251453399)
('epoch', 282, 'train_loss:', 4.0963600707054137, 'val_loss:', 1.0285388541221618)
('epoch', 283, 'train_loss:', 4.0988749074935917, 'val_loss:', 1.0269451105594636)
('epoch', 284, 'train_loss:', 4.0933314311504363, 'val_loss:', 1.0264390587806702)
('epoch', 285, 'train_loss:', 4.097584068775177, 'val_loss:', 1.0269066202640533)
('epoch', 286, 'train_loss:', 4.0930871152877808, 'val_loss:', 1.0273771464824677)
('epoch', 287, 'train_loss:', 4.0849389433860779, 'val_loss:', 1.0261339974403381)
('epoch', 288, 'train_loss:', 4.0829970514774319, 'val_loss:', 1.0232989978790283)
('epoch', 289, 'train_loss:', 4.07577360868454, 'val_loss:', 1.0198941135406494)
('epoch', 290, 'train_loss:', 4.0745227110385898, 'val_loss:', 1.0216923797130584)
('epoch', 291, 'train_loss:', 4.0753627026081087, 'val_loss:', 1.02021448969841)
('epoch', 292, 'train_loss:', 4.0774282920360569, 'val_loss:', 1.0205708658695221)
('epoch', 293, 'train_loss:', 4.0638287866115572, 'val_loss:', 1.0216019010543824)
('epoch', 294, 'train_loss:', 4.0629569208621978, 'val_loss:', 1.0213834357261657)
('epoch', 295, 'train_loss:', 4.0645448684692385, 'val_loss:', 1.0183902835845948)
('epoch', 296, 'train_loss:', 4.0546395266056061, 'val_loss:', 1.0197729766368866)
('epoch', 297, 'train_loss:', 4.0579822552204128, 'val_loss:', 1.0174321949481964)
('epoch', 298, 'train_loss:', 4.0515136611461635, 'val_loss:', 1.017059133052826)
('epoch', 299, 'train_loss:', 4.0527708995342255, 'val_loss:', 1.0161753499507904)
('epoch', 300, 'train_loss:', 4.048097302913666, 'val_loss:', 1.015634878873825)
('epoch', 301, 'train_loss:', 4.0465268349647525, 'val_loss:', 1.0142340648174286)
('epoch', 302, 'train_loss:', 4.0336320447921752, 'val_loss:', 1.0129816508293152)
('epoch', 303, 'train_loss:', 4.0444663023948673, 'val_loss:', 1.0157898128032685)
('epoch', 304, 'train_loss:', 4.0449060273170474, 'val_loss:', 1.0150292789936066)
('epoch', 305, 'train_loss:', 4.0371298980712886, 'val_loss:', 1.0136458921432494)
('epoch', 306, 'train_loss:', 4.0282301855087281, 'val_loss:', 1.0098102986812592)
('epoch', 307, 'train_loss:', 4.0263042151927948, 'val_loss:', 1.010291692018509)
('epoch', 308, 'train_loss:', 4.0281005811691282, 'val_loss:', 1.0091956281661987)
('epoch', 309, 'train_loss:', 4.018240712881088, 'val_loss:', 1.0079838728904724)
('epoch', 310, 'train_loss:', 4.0232236111164097, 'val_loss:', 1.0071940112113953)
('epoch', 311, 'train_loss:', 4.0218447983264927, 'val_loss:', 1.011782615184784)
('epoch', 312, 'train_loss:', 4.0152565979957577, 'val_loss:', 1.0081247794628143)
('epoch', 313, 'train_loss:', 4.013139979839325, 'val_loss:', 1.0032103896141051)
('epoch', 314, 'train_loss:', 4.0067013120651245, 'val_loss:', 1.0057122707366943)
('epoch', 315, 'train_loss:', 4.0011557638645172, 'val_loss:', 1.0083261775970458)
('epoch', 316, 'train_loss:', 4.0034742867946624, 'val_loss:', 1.0065215301513672)
('epoch', 317, 'train_loss:', 3.9999152159690858, 'val_loss:', 1.0052801775932312)
('epoch', 318, 'train_loss:', 3.9957978105545044, 'val_loss:', 1.0032556498050689)
('epoch', 319, 'train_loss:', 3.9950608086585997, 'val_loss:', 1.0039638471603394)
('epoch', 320, 'train_loss:', 3.9979586493968964, 'val_loss:', 1.0039311528205872)
('epoch', 321, 'train_loss:', 3.9883252930641175, 'val_loss:', 1.0023107302188874)
('epoch', 322, 'train_loss:', 3.9846181130409239, 'val_loss:', 0.99980654835700988)
('epoch', 323, 'train_loss:', 3.9835110700130461, 'val_loss:', 0.99977607011795044)
('epoch', 324, 'train_loss:', 3.9755257034301756, 'val_loss:', 1.0028101122379303)
('epoch', 325, 'train_loss:', 3.9807979655265808, 'val_loss:', 0.99850462675094609)
('epoch', 326, 'train_loss:', 3.9793879103660585, 'val_loss:', 0.99824021577835087)
('epoch', 327, 'train_loss:', 3.9760097432136536, 'val_loss:', 0.99937035560607912)
('epoch', 328, 'train_loss:', 3.9714471530914306, 'val_loss:', 0.99563100218772893)
('epoch', 329, 'train_loss:', 3.9684561550617219, 'val_loss:', 0.99703146100044249)
('epoch', 330, 'train_loss:', 3.9670400977134705, 'val_loss:', 0.99590791702270509)
('epoch', 331, 'train_loss:', 3.9721353399753569, 'val_loss:', 0.99579614400863647)
('epoch', 332, 'train_loss:', 3.9608355927467347, 'val_loss:', 0.9954501485824585)
('epoch', 333, 'train_loss:', 3.9585692393779754, 'val_loss:', 0.99806102633476257)
('epoch', 334, 'train_loss:', 3.9571827232837675, 'val_loss:', 0.99569888114929195)
('epoch', 335, 'train_loss:', 3.9604606270790099, 'val_loss:', 0.99833842873573309)
('epoch', 336, 'train_loss:', 3.9548017752170561, 'val_loss:', 0.9941876232624054)
('epoch', 337, 'train_loss:', 3.9504636931419372, 'val_loss:', 0.99157952547073369)
('epoch', 338, 'train_loss:', 3.9462258374691008, 'val_loss:', 0.99355482935905459)
('epoch', 339, 'train_loss:', 3.9460031390190125, 'val_loss:', 0.99310442209243777)
('epoch', 340, 'train_loss:', 3.9347215080261231, 'val_loss:', 0.98740885615348817)
('epoch', 341, 'train_loss:', 3.9370407807826995, 'val_loss:', 0.99048317551612852)
('epoch', 342, 'train_loss:', 3.9364408779144289, 'val_loss:', 0.9901931297779083)
('epoch', 343, 'train_loss:', 3.935409381389618, 'val_loss:', 0.98883072376251224)
('epoch', 344, 'train_loss:', 3.9337629246711732, 'val_loss:', 0.9899655199050903)
('epoch', 345, 'train_loss:', 3.9330802488327028, 'val_loss:', 0.98817514181137089)
('epoch', 346, 'train_loss:', 3.9329478728771208, 'val_loss:', 0.98460672140121464)
('epoch', 347, 'train_loss:', 3.933324855566025, 'val_loss:', 0.98670065641403193)
('epoch', 348, 'train_loss:', 3.9252139246463775, 'val_loss:', 0.98716193079948422)
('epoch', 349, 'train_loss:', 3.9216562616825104, 'val_loss:', 0.98687787055969234)
('epoch', 350, 'train_loss:', 3.9200936853885651, 'val_loss:', 0.98443433165550231)
('epoch', 351, 'train_loss:', 3.9172509300708769, 'val_loss:', 0.98534538626670842)
('epoch', 352, 'train_loss:', 3.9096919810771942, 'val_loss:', 0.98219977974891659)
('epoch', 353, 'train_loss:', 3.910739974975586, 'val_loss:', 0.98312113165855408)
('epoch', 354, 'train_loss:', 3.9039189636707308, 'val_loss:', 0.98150811553001405)
('epoch', 355, 'train_loss:', 3.9124304902553559, 'val_loss:', 0.98382532715797422)
('epoch', 356, 'train_loss:', 3.8981113398075102, 'val_loss:', 0.98233432173728941)
('epoch', 357, 'train_loss:', 3.9116630756855013, 'val_loss:', 0.98428626537322994)
('epoch', 358, 'train_loss:', 3.8991247785091399, 'val_loss:', 0.98169087886810302)
('epoch', 359, 'train_loss:', 3.9002636981010439, 'val_loss:', 0.98113050341606145)
('epoch', 360, 'train_loss:', 3.893358042240143, 'val_loss:', 0.97826868295669556)
('epoch', 361, 'train_loss:', 3.8982928681373594, 'val_loss:', 0.97841362118721009)
('epoch', 362, 'train_loss:', 3.8892404639720919, 'val_loss:', 0.97977245926856993)
('epoch', 363, 'train_loss:', 3.8941592347621916, 'val_loss:', 0.98026878237724302)
('epoch', 364, 'train_loss:', 3.8816979587078095, 'val_loss:', 0.97644426822662356)
('epoch', 365, 'train_loss:', 3.8890873765945435, 'val_loss:', 0.97807550430297852)
('epoch', 366, 'train_loss:', 3.8863876211643218, 'val_loss:', 0.97700401425361638)
('epoch', 367, 'train_loss:', 3.8857750439643861, 'val_loss:', 0.97839744210243229)
('epoch', 368, 'train_loss:', 3.8789590716362001, 'val_loss:', 0.97728375315666194)
('epoch', 369, 'train_loss:', 3.8806828320026399, 'val_loss:', 0.9766757547855377)
('epoch', 370, 'train_loss:', 3.8715724456310272, 'val_loss:', 0.97174122214317327)
('epoch', 371, 'train_loss:', 3.8699728798866273, 'val_loss:', 0.97490455627441408)
('epoch', 372, 'train_loss:', 3.8748267364501952, 'val_loss:', 0.97750347614288335)
('epoch', 373, 'train_loss:', 3.8762632715702057, 'val_loss:', 0.97155181288719172)
('epoch', 374, 'train_loss:', 3.8692670738697053, 'val_loss:', 0.97143203020095825)
('epoch', 375, 'train_loss:', 3.868240851163864, 'val_loss:', 0.97225657701492307)
('epoch', 376, 'train_loss:', 3.8628337323665618, 'val_loss:', 0.97182959914207456)
('epoch', 377, 'train_loss:', 3.8647556686401368, 'val_loss:', 0.96958912730216984)
('epoch', 378, 'train_loss:', 3.8611593651771545, 'val_loss:', 0.96776906609535218)
('epoch', 379, 'train_loss:', 3.8584565806388853, 'val_loss:', 0.96791376948356633)
('epoch', 380, 'train_loss:', 3.8578318309783937, 'val_loss:', 0.96869208931922912)
('epoch', 381, 'train_loss:', 3.8590478432178497, 'val_loss:', 0.9699442648887634)
('epoch', 382, 'train_loss:', 3.8602599775791169, 'val_loss:', 0.96894152164459224)
('epoch', 383, 'train_loss:', 3.8450897586345674, 'val_loss:', 0.96747235536575316)
('epoch', 384, 'train_loss:', 3.8541087508201599, 'val_loss:', 0.97156975984573368)
('epoch', 385, 'train_loss:', 3.8509782993793489, 'val_loss:', 0.96828282117843623)
('epoch', 386, 'train_loss:', 3.8459586298465727, 'val_loss:', 0.96635235309600831)
('epoch', 387, 'train_loss:', 3.8379765212535859, 'val_loss:', 0.96637656927108762)
('epoch', 388, 'train_loss:', 3.8443098700046541, 'val_loss:', 0.96402747988700865)
('epoch', 389, 'train_loss:', 3.8415880572795866, 'val_loss:', 0.96960745930671688)
('epoch', 390, 'train_loss:', 3.8403025567531586, 'val_loss:', 0.96779536485672002)
('epoch', 391, 'train_loss:', 3.8375286853313444, 'val_loss:', 0.96591948151588436)
('epoch', 392, 'train_loss:', 3.8283576261997223, 'val_loss:', 0.96638715267181396)
('epoch', 393, 'train_loss:', 3.8292239236831667, 'val_loss:', 0.96139874100685119)
('epoch', 394, 'train_loss:', 3.8307170879840853, 'val_loss:', 0.96713956713676452)
('epoch', 395, 'train_loss:', 3.8266634225845335, 'val_loss:', 0.95993309855461117)
('epoch', 396, 'train_loss:', 3.8287093639373779, 'val_loss:', 0.96132262706756588)
('epoch', 397, 'train_loss:', 3.8258471941947936, 'val_loss:', 0.96222095608711244)
('epoch', 398, 'train_loss:', 3.8194543397426606, 'val_loss:', 0.95968373417854314)
('epoch', 399, 'train_loss:', 3.8199030017852782, 'val_loss:', 0.96048638343811032)
('epoch', 400, 'train_loss:', 3.813752928972244, 'val_loss:', 0.96033434271812435)
('epoch', 401, 'train_loss:', 3.822827124595642, 'val_loss:', 0.95904170393943788)
('epoch', 402, 'train_loss:', 3.8193413043022155, 'val_loss:', 0.95852111697196962)
('epoch', 403, 'train_loss:', 3.817936370372772, 'val_loss:', 0.96132315278053282)
('epoch', 404, 'train_loss:', 3.8105316483974456, 'val_loss:', 0.95542731881141663)
('epoch', 405, 'train_loss:', 3.8077321302890779, 'val_loss:', 0.95932880997657777)
('epoch', 406, 'train_loss:', 3.8105769443511961, 'val_loss:', 0.95707760095596317)
('epoch', 407, 'train_loss:', 3.804024875164032, 'val_loss:', 0.95505758404731755)
('epoch', 408, 'train_loss:', 3.8008373260498045, 'val_loss:', 0.9566654658317566)
('epoch', 409, 'train_loss:', 3.8039353919029235, 'val_loss:', 0.9571724784374237)
('epoch', 410, 'train_loss:', 3.8033346128463745, 'val_loss:', 0.95592556238174442)
('epoch', 411, 'train_loss:', 3.8048193156719208, 'val_loss:', 0.95543825030326845)
('epoch', 412, 'train_loss:', 3.7955535614490508, 'val_loss:', 0.95971348285675051)
('epoch', 413, 'train_loss:', 3.7915543329715731, 'val_loss:', 0.95231938123703008)
('epoch', 414, 'train_loss:', 3.7920867478847504, 'val_loss:', 0.95280982255935665)
('epoch', 415, 'train_loss:', 3.7893856120109559, 'val_loss:', 0.95507629632949831)
('epoch', 416, 'train_loss:', 3.7919594216346741, 'val_loss:', 0.95323957085609434)
('epoch', 417, 'train_loss:', 3.7942126500606537, 'val_loss:', 0.95245059967041013)
('epoch', 418, 'train_loss:', 3.7843298864364625, 'val_loss:', 0.95417068362236024)
('epoch', 419, 'train_loss:', 3.7805744743347169, 'val_loss:', 0.95267009019851689)
('epoch', 420, 'train_loss:', 3.7847181391716003, 'val_loss:', 0.95330890178680416)
('epoch', 421, 'train_loss:', 3.7806859934329986, 'val_loss:', 0.95244788408279424)
('epoch', 422, 'train_loss:', 3.7794056737422945, 'val_loss:', 0.94927498698234558)
('epoch', 423, 'train_loss:', 3.7783601021766664, 'val_loss:', 0.95405962347984319)
('epoch', 424, 'train_loss:', 3.7762967097759246, 'val_loss:', 0.95120448708534244)
('epoch', 425, 'train_loss:', 3.7735693001747133, 'val_loss:', 0.95463715672492977)
('epoch', 426, 'train_loss:', 3.770542024374008, 'val_loss:', 0.94658881425857544)
('epoch', 427, 'train_loss:', 3.7780734968185423, 'val_loss:', 0.9479086196422577)
('epoch', 428, 'train_loss:', 3.7715811145305635, 'val_loss:', 0.95035227894783025)
('epoch', 429, 'train_loss:', 3.7693010747432707, 'val_loss:', 0.94642542600631718)
('epoch', 430, 'train_loss:', 3.7658407354354857, 'val_loss:', 0.9486199963092804)
('epoch', 431, 'train_loss:', 3.7648377335071563, 'val_loss:', 0.9507954382896423)
('epoch', 432, 'train_loss:', 3.7631415486335755, 'val_loss:', 0.9519525253772736)
('epoch', 433, 'train_loss:', 3.7691427898406982, 'val_loss:', 0.95075307488441463)
('epoch', 434, 'train_loss:', 3.7657072412967683, 'val_loss:', 0.94790597796440124)
('epoch', 435, 'train_loss:', 3.756826158761978, 'val_loss:', 0.94667360186576843)
('epoch', 436, 'train_loss:', 3.7613562214374543, 'val_loss:', 0.94848216772079463)
('epoch', 437, 'train_loss:', 3.756673136949539, 'val_loss:', 0.94656172871589661)
('epoch', 438, 'train_loss:', 3.7542966580390931, 'val_loss:', 0.94495900869369509)
('epoch', 439, 'train_loss:', 3.7506059777736662, 'val_loss:', 0.94538030862808231)
('epoch', 440, 'train_loss:', 3.7540839958190917, 'val_loss:', 0.94440257787704462)
('epoch', 441, 'train_loss:', 3.753369963169098, 'val_loss:', 0.95049223423004148)
('epoch', 442, 'train_loss:', 3.744809401035309, 'val_loss:', 0.94280658841133114)
('epoch', 443, 'train_loss:', 3.7446588480472567, 'val_loss:', 0.94181003928184515)
('epoch', 444, 'train_loss:', 3.7448856413364409, 'val_loss:', 0.94696925878524785)
('epoch', 445, 'train_loss:', 3.7392405188083648, 'val_loss:', 0.94435666203498836)
('epoch', 446, 'train_loss:', 3.7365862095355986, 'val_loss:', 0.94330542922019955)
('epoch', 447, 'train_loss:', 3.7379752027988435, 'val_loss:', 0.94376037240028376)
('epoch', 448, 'train_loss:', 3.7420136654376983, 'val_loss:', 0.94247057914733889)
('epoch', 449, 'train_loss:', 3.7413032114505769, 'val_loss:', 0.94300799369812016)
('epoch', 450, 'train_loss:', 3.7372643661499025, 'val_loss:', 0.94073104858398438)
('epoch', 451, 'train_loss:', 3.7342948055267335, 'val_loss:', 0.94341204762458797)
('epoch', 452, 'train_loss:', 3.7364105415344238, 'val_loss:', 0.9409517705440521)
('epoch', 453, 'train_loss:', 3.72984330534935, 'val_loss:', 0.93724756360054018)
('epoch', 454, 'train_loss:', 3.7295528984069826, 'val_loss:', 0.9380461323261261)
('epoch', 455, 'train_loss:', 3.7234149372577665, 'val_loss:', 0.94194064259529109)
('epoch', 456, 'train_loss:', 3.7203087127208709, 'val_loss:', 0.93771605491638188)
('epoch', 457, 'train_loss:', 3.7295036506652832, 'val_loss:', 0.94192338228225703)
('epoch', 458, 'train_loss:', 3.724950295686722, 'val_loss:', 0.94360620498657222)
('epoch', 459, 'train_loss:', 3.7235662448406219, 'val_loss:', 0.93768079400062565)
('epoch', 460, 'train_loss:', 3.713773456811905, 'val_loss:', 0.9374060082435608)
('epoch', 461, 'train_loss:', 3.7184973406791686, 'val_loss:', 0.93770677924156187)
('epoch', 462, 'train_loss:', 3.717545597553253, 'val_loss:', 0.93681132197380068)
('epoch', 463, 'train_loss:', 3.7139029419422149, 'val_loss:', 0.93664160251617434)
('epoch', 464, 'train_loss:', 3.7183091390132903, 'val_loss:', 0.93788241028785702)
('epoch', 465, 'train_loss:', 3.7112615025043487, 'val_loss:', 0.93548641085624695)
('epoch', 466, 'train_loss:', 3.7146306550502777, 'val_loss:', 0.93767132878303527)
('epoch', 467, 'train_loss:', 3.7130569970607756, 'val_loss:', 0.93509364724159239)
('epoch', 468, 'train_loss:', 3.7115929400920868, 'val_loss:', 0.93395920515060427)
('epoch', 469, 'train_loss:', 3.710356159210205, 'val_loss:', 0.93728949308395382)
('epoch', 470, 'train_loss:', 3.7068274700641632, 'val_loss:', 0.9341376566886902)
('epoch', 471, 'train_loss:', 3.7151445090770721, 'val_loss:', 0.93353746056556697)
('epoch', 472, 'train_loss:', 3.7072121393680573, 'val_loss:', 0.93336060762405393)
('epoch', 473, 'train_loss:', 3.7016624546051027, 'val_loss:', 0.93487962841987615)
('epoch', 474, 'train_loss:', 3.7004763412475588, 'val_loss:', 0.93599121332168578)
('epoch', 475, 'train_loss:', 3.698730763196945, 'val_loss:', 0.9310486531257629)
('epoch', 476, 'train_loss:', 3.6949364829063414, 'val_loss:', 0.93476879477500918)
('epoch', 477, 'train_loss:', 3.7030298376083373, 'val_loss:', 0.93768871426582334)
('epoch', 478, 'train_loss:', 3.6997026872634886, 'val_loss:', 0.93407343268394472)
('epoch', 479, 'train_loss:', 3.693964365720749, 'val_loss:', 0.93166777729988093)
('epoch', 480, 'train_loss:', 3.6908829760551454, 'val_loss:', 0.93305622458457949)
('epoch', 481, 'train_loss:', 3.6972961246967317, 'val_loss:', 0.93272978425025943)
('epoch', 482, 'train_loss:', 3.6901624405384066, 'val_loss:', 0.9321165764331818)
('epoch', 483, 'train_loss:', 3.6944610702991487, 'val_loss:', 0.93169426083564755)
('epoch', 484, 'train_loss:', 3.6905804228782655, 'val_loss:', 0.93205032706260682)
('epoch', 485, 'train_loss:', 3.6823734056949617, 'val_loss:', 0.92820560097694393)
('epoch', 486, 'train_loss:', 3.6862639617919921, 'val_loss:', 0.93277106881141658)
('epoch', 487, 'train_loss:', 3.6877156651020049, 'val_loss:', 0.93247474312782286)
('epoch', 488, 'train_loss:', 3.6796737909317017, 'val_loss:', 0.92731974244117732)
('epoch', 489, 'train_loss:', 3.6863703191280366, 'val_loss:', 0.9304577112197876)
('epoch', 490, 'train_loss:', 3.6709161901474001, 'val_loss:', 0.92802890300750729)
('epoch', 491, 'train_loss:', 3.6794940459728243, 'val_loss:', 0.93118345618247989)
('epoch', 492, 'train_loss:', 3.6812108361721041, 'val_loss:', 0.93182319641113276)
('epoch', 493, 'train_loss:', 3.681717151403427, 'val_loss:', 0.92632849931716921)
('epoch', 494, 'train_loss:', 3.6800081622600556, 'val_loss:', 0.92925259113311764)
('epoch', 495, 'train_loss:', 3.6729424166679383, 'val_loss:', 0.93015946507453917)
('epoch', 496, 'train_loss:', 3.6682025599479675, 'val_loss:', 0.92686565279960631)
('epoch', 497, 'train_loss:', 3.6714704394340516, 'val_loss:', 0.92381152153015134)
('epoch', 498, 'train_loss:', 3.6689211964607238, 'val_loss:', 0.92876444816589354)
('epoch', 499, 'train_loss:', 3.6676309657096864, 'val_loss:', 0.92785968422889709)

