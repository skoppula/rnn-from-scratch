(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=2 python lstm-additive-forget-no-sigmoid.py -t
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:81:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2680 get requests, put_count=2458 evicted_count=1000 eviction_rate=0.406835 and unsatisfied allocation rate=0.493284
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2755 get requests, put_count=2874 evicted_count=1000 eviction_rate=0.347947 and unsatisfied allocation rate=0.328131
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24354 get requests, put_count=24366 evicted_count=1000 eviction_rate=0.0410408 and unsatisfied allocation rate=0.0429909
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 0, 'train_loss:', 9.5156626176834109, 'val_loss:', 2.1344187593460084)
('epoch', 1, 'train_loss:', 8.2335898661613456, 'val_loss:', 2.0017017030715945)
('epoch', 2, 'train_loss:', 7.8447946619987485, 'val_loss:', 1.9183625745773316)
('epoch', 3, 'train_loss:', 7.4753873324394222, 'val_loss:', 1.8117497372627258)
('epoch', 4, 'train_loss:', 7.0597204756736751, 'val_loss:', 1.7236983227729796)
('epoch', 5, 'train_loss:', 6.7800477004051212, 'val_loss:', 1.6690500211715698)
('epoch', 6, 'train_loss:', 6.5967190480232238, 'val_loss:', 1.630908169746399)
('epoch', 7, 'train_loss:', 6.4703342223167422, 'val_loss:', 1.6049824929237366)
('epoch', 8, 'train_loss:', 6.3825805020332336, 'val_loss:', 1.5833923745155334)
('epoch', 9, 'train_loss:', 6.3024085259437559, 'val_loss:', 1.5646649551391603)
('epoch', 10, 'train_loss:', 6.2370727753639219, 'val_loss:', 1.5496423459053039)
('epoch', 11, 'train_loss:', 6.173736386299133, 'val_loss:', 1.5336734318733216)
('epoch', 12, 'train_loss:', 6.1150615191459652, 'val_loss:', 1.5197490072250366)
('epoch', 13, 'train_loss:', 6.0610194635391235, 'val_loss:', 1.5072702240943909)
('epoch', 14, 'train_loss:', 6.0073525238037107, 'val_loss:', 1.489637589454651)
('epoch', 15, 'train_loss:', 5.9483396768569943, 'val_loss:', 1.4783676195144653)
('epoch', 16, 'train_loss:', 5.9035356330871585, 'val_loss:', 1.4671895384788514)
('epoch', 17, 'train_loss:', 5.8556555867195126, 'val_loss:', 1.4571882462501526)
('epoch', 18, 'train_loss:', 5.8106430506706239, 'val_loss:', 1.4445040726661682)
('epoch', 19, 'train_loss:', 5.7622901701927187, 'val_loss:', 1.4371086144447327)
('epoch', 20, 'train_loss:', 5.7201624250411989, 'val_loss:', 1.4234085440635682)
('epoch', 21, 'train_loss:', 5.6857619261741634, 'val_loss:', 1.4162002992630005)
('epoch', 22, 'train_loss:', 5.6430223941802975, 'val_loss:', 1.4054463195800782)
('epoch', 23, 'train_loss:', 5.6067224860191347, 'val_loss:', 1.3943434643745423)
('epoch', 24, 'train_loss:', 5.5713621330261232, 'val_loss:', 1.3873130941390992)
('epoch', 25, 'train_loss:', 5.5360991287231442, 'val_loss:', 1.3761452555656433)
('epoch', 26, 'train_loss:', 5.5038141202926631, 'val_loss:', 1.3674027800559998)
('epoch', 27, 'train_loss:', 5.4684385418891903, 'val_loss:', 1.3603479957580566)
('epoch', 28, 'train_loss:', 5.4372321033477782, 'val_loss:', 1.3520002222061158)
('epoch', 29, 'train_loss:', 5.4011311817169192, 'val_loss:', 1.3464055752754212)
('epoch', 30, 'train_loss:', 5.3784669828414913, 'val_loss:', 1.3401410126686095)
('epoch', 31, 'train_loss:', 5.3410998892784116, 'val_loss:', 1.3300364589691163)
('epoch', 32, 'train_loss:', 5.3110406827926635, 'val_loss:', 1.3237096309661864)
('epoch', 33, 'train_loss:', 5.294348986148834, 'val_loss:', 1.3179355645179749)
('epoch', 34, 'train_loss:', 5.2601936364173891, 'val_loss:', 1.3095946764945985)
('epoch', 35, 'train_loss:', 5.2381236004829406, 'val_loss:', 1.3036427640914916)
('epoch', 36, 'train_loss:', 5.2091260170936584, 'val_loss:', 1.294581913948059)
('epoch', 37, 'train_loss:', 5.1888227558135984, 'val_loss:', 1.2898083233833313)
('epoch', 38, 'train_loss:', 5.1554246640205381, 'val_loss:', 1.2862513208389281)
('epoch', 39, 'train_loss:', 5.133027009963989, 'val_loss:', 1.2803237485885619)
('epoch', 40, 'train_loss:', 5.1116898131370547, 'val_loss:', 1.2771034693717958)
('epoch', 41, 'train_loss:', 5.088860995769501, 'val_loss:', 1.267764813899994)
('epoch', 42, 'train_loss:', 5.0623261213302611, 'val_loss:', 1.2666546130180358)
('epoch', 43, 'train_loss:', 5.0427580130100251, 'val_loss:', 1.2595370948314666)
('epoch', 44, 'train_loss:', 5.0253806304931636, 'val_loss:', 1.2559965968132019)
('epoch', 45, 'train_loss:', 5.0069930088520049, 'val_loss:', 1.2468148863315582)
('epoch', 46, 'train_loss:', 4.9792014575004577, 'val_loss:', 1.2444862484931947)
('epoch', 47, 'train_loss:', 4.9571265101432802, 'val_loss:', 1.2426673638820649)
('epoch', 48, 'train_loss:', 4.9419272494316102, 'val_loss:', 1.2300972414016724)
('epoch', 49, 'train_loss:', 4.9172036659717557, 'val_loss:', 1.2282488417625428)
('epoch', 50, 'train_loss:', 4.9063551175594329, 'val_loss:', 1.2211141407489776)
('epoch', 51, 'train_loss:', 4.8773912858963016, 'val_loss:', 1.2181657767295837)
('epoch', 52, 'train_loss:', 4.8570031511783602, 'val_loss:', 1.2176191318035126)
('epoch', 53, 'train_loss:', 4.843435705900192, 'val_loss:', 1.2050350797176361)
('epoch', 54, 'train_loss:', 4.8262993979454043, 'val_loss:', 1.2020729851722718)
('epoch', 55, 'train_loss:', 4.8038367354869846, 'val_loss:', 1.1992485284805299)
('epoch', 56, 'train_loss:', 4.793874886035919, 'val_loss:', 1.1942115986347199)
('epoch', 57, 'train_loss:', 4.7744842696189878, 'val_loss:', 1.1884059381484986)
('epoch', 58, 'train_loss:', 4.7568522107601163, 'val_loss:', 1.1860774147510529)
('epoch', 59, 'train_loss:', 4.742359412908554, 'val_loss:', 1.1827888011932373)
('epoch', 60, 'train_loss:', 4.7226622509956364, 'val_loss:', 1.1820113110542296)
('epoch', 61, 'train_loss:', 4.7013884937763217, 'val_loss:', 1.1724244558811188)
('epoch', 62, 'train_loss:', 4.6910555219650272, 'val_loss:', 1.1690365898609161)
('epoch', 63, 'train_loss:', 4.6797697389125821, 'val_loss:', 1.166790007352829)
('epoch', 64, 'train_loss:', 4.6584528660774227, 'val_loss:', 1.1639493453502654)
('epoch', 65, 'train_loss:', 4.6487682509422301, 'val_loss:', 1.1581697297096252)
('epoch', 66, 'train_loss:', 4.6261929106712341, 'val_loss:', 1.1568412101268768)
('epoch', 67, 'train_loss:', 4.6180337905883793, 'val_loss:', 1.1540340578556061)
('epoch', 68, 'train_loss:', 4.60225834608078, 'val_loss:', 1.1501633059978484)
('epoch', 69, 'train_loss:', 4.5872021126747136, 'val_loss:', 1.1492585813999177)
('epoch', 70, 'train_loss:', 4.5743034589290623, 'val_loss:', 1.1436350083351134)
('epoch', 71, 'train_loss:', 4.557212413549423, 'val_loss:', 1.1387045586109161)
('epoch', 72, 'train_loss:', 4.548566724061966, 'val_loss:', 1.1330070257186891)
('epoch', 73, 'train_loss:', 4.5325931668281552, 'val_loss:', 1.1333791995048523)
('epoch', 74, 'train_loss:', 4.5239873754978177, 'val_loss:', 1.1253989541530609)
('epoch', 75, 'train_loss:', 4.5133060479164122, 'val_loss:', 1.1251788687705995)
('epoch', 76, 'train_loss:', 4.5039638912677766, 'val_loss:', 1.1202111995220185)
('epoch', 77, 'train_loss:', 4.4875379693508144, 'val_loss:', 1.1193047165870667)
('epoch', 78, 'train_loss:', 4.4719825518131255, 'val_loss:', 1.1180068933963776)
('epoch', 79, 'train_loss:', 4.4650518453121189, 'val_loss:', 1.1146185970306397)
('epoch', 80, 'train_loss:', 4.450268278121948, 'val_loss:', 1.1090226399898528)
('epoch', 81, 'train_loss:', 4.4344530463218685, 'val_loss:', 1.1085517001152039)
('epoch', 82, 'train_loss:', 4.4243139386177059, 'val_loss:', 1.1013404309749604)
('epoch', 83, 'train_loss:', 4.409693475961685, 'val_loss:', 1.1014023852348327)
('epoch', 84, 'train_loss:', 4.4032109093666074, 'val_loss:', 1.1026118648052217)
('epoch', 85, 'train_loss:', 4.3859637427330016, 'val_loss:', 1.093545799255371)
('epoch', 86, 'train_loss:', 4.3741969788074497, 'val_loss:', 1.0934618437290191)
('epoch', 87, 'train_loss:', 4.3689693510532379, 'val_loss:', 1.0914521968364717)
('epoch', 88, 'train_loss:', 4.3561640179157255, 'val_loss:', 1.088567008972168)
('epoch', 89, 'train_loss:', 4.3452665352821347, 'val_loss:', 1.0855660688877107)
('epoch', 90, 'train_loss:', 4.3326611864566802, 'val_loss:', 1.0834967052936555)
('epoch', 91, 'train_loss:', 4.3252902853488919, 'val_loss:', 1.0768515014648437)
('epoch', 92, 'train_loss:', 4.3145458269119263, 'val_loss:', 1.0763459181785584)
('epoch', 93, 'train_loss:', 4.3022680425643918, 'val_loss:', 1.0798860275745392)
('epoch', 94, 'train_loss:', 4.2962388110160825, 'val_loss:', 1.0778056657314301)
('epoch', 95, 'train_loss:', 4.2907934582233427, 'val_loss:', 1.0689289641380311)
('epoch', 96, 'train_loss:', 4.2776428413391114, 'val_loss:', 1.0695029425621032)
('epoch', 97, 'train_loss:', 4.2616602718830112, 'val_loss:', 1.0658971631526948)
('epoch', 98, 'train_loss:', 4.2559724116325377, 'val_loss:', 1.0644026052951814)
('epoch', 99, 'train_loss:', 4.2430443108081821, 'val_loss:', 1.064236820936203)
('epoch', 100, 'train_loss:', 4.2417434108257295, 'val_loss:', 1.0620759773254393)
('epoch', 101, 'train_loss:', 4.2368985164165496, 'val_loss:', 1.0624702715873717)
('epoch', 102, 'train_loss:', 4.2218958401679991, 'val_loss:', 1.0582415890693664)
('epoch', 103, 'train_loss:', 4.2244248139858245, 'val_loss:', 1.0542062246799468)
('epoch', 104, 'train_loss:', 4.2141802453994748, 'val_loss:', 1.0511272513866425)
('epoch', 105, 'train_loss:', 4.1972533571720128, 'val_loss:', 1.0514812159538269)
('epoch', 106, 'train_loss:', 4.1960324466228487, 'val_loss:', 1.0511909651756286)
('epoch', 107, 'train_loss:', 4.1817515265941623, 'val_loss:', 1.0460738706588746)
('epoch', 108, 'train_loss:', 4.1753742504119877, 'val_loss:', 1.0419804036617279)
('epoch', 109, 'train_loss:', 4.1656980657577511, 'val_loss:', 1.0420178997516631)
('epoch', 110, 'train_loss:', 4.1617107820510864, 'val_loss:', 1.0411294603347778)
('epoch', 111, 'train_loss:', 4.1494481027126309, 'val_loss:', 1.0384837353229524)
('epoch', 112, 'train_loss:', 4.149310207366943, 'val_loss:', 1.0406573569774629)
('epoch', 113, 'train_loss:', 4.1474171102046968, 'val_loss:', 1.0362067604064942)
('epoch', 114, 'train_loss:', 4.1333148050308228, 'val_loss:', 1.0330524194240569)
('epoch', 115, 'train_loss:', 4.1198552584648134, 'val_loss:', 1.0320547592639924)
('epoch', 116, 'train_loss:', 4.1225796079635622, 'val_loss:', 1.031298758983612)
('epoch', 117, 'train_loss:', 4.1115627801418304, 'val_loss:', 1.0273926532268525)
('epoch', 118, 'train_loss:', 4.1036807703971867, 'val_loss:', 1.0291726982593536)
('epoch', 119, 'train_loss:', 4.0972620260715482, 'val_loss:', 1.0291782784461976)
('epoch', 120, 'train_loss:', 4.0974895787239074, 'val_loss:', 1.0257753121852875)
('epoch', 121, 'train_loss:', 4.0865648126602174, 'val_loss:', 1.0214352202415466)
('epoch', 122, 'train_loss:', 4.0789203262329101, 'val_loss:', 1.0221968328952789)
('epoch', 123, 'train_loss:', 4.0810097754001617, 'val_loss:', 1.0231956613063813)
('epoch', 124, 'train_loss:', 4.0671134960651401, 'val_loss:', 1.0196963357925415)
('epoch', 125, 'train_loss:', 4.0651095008850096, 'val_loss:', 1.0145712649822236)
('epoch', 126, 'train_loss:', 4.0522535133361819, 'val_loss:', 1.0169507181644439)
('epoch', 127, 'train_loss:', 4.0418704390525821, 'val_loss:', 1.0203263592720031)
('epoch', 128, 'train_loss:', 4.0428181624412538, 'val_loss:', 1.0126913166046143)
('epoch', 129, 'train_loss:', 4.0348727416992185, 'val_loss:', 1.0130446684360503)
('epoch', 130, 'train_loss:', 4.0348344397544862, 'val_loss:', 1.0106083869934082)
('epoch', 131, 'train_loss:', 4.0229992663860319, 'val_loss:', 1.0094177997112275)
('epoch', 132, 'train_loss:', 4.0166348755359653, 'val_loss:', 1.0109253871440886)
('epoch', 133, 'train_loss:', 4.0163394749164585, 'val_loss:', 1.0042040348052979)
('epoch', 134, 'train_loss:', 4.012779424190521, 'val_loss:', 1.0026646912097932)
('epoch', 135, 'train_loss:', 4.0043805563449864, 'val_loss:', 1.0018147683143617)
('epoch', 136, 'train_loss:', 4.0010157322883604, 'val_loss:', 0.99735920906066899)
('epoch', 137, 'train_loss:', 3.9934756577014925, 'val_loss:', 1.0005886614322663)
('epoch', 138, 'train_loss:', 3.9884581923484803, 'val_loss:', 1.0036612963676452)
('epoch', 139, 'train_loss:', 3.9775355803966521, 'val_loss:', 0.99890272021293636)
('epoch', 140, 'train_loss:', 3.9706972587108611, 'val_loss:', 0.99185449957847593)
('epoch', 141, 'train_loss:', 3.9705400013923646, 'val_loss:', 0.99419427990913389)
('epoch', 142, 'train_loss:', 3.9666302490234373, 'val_loss:', 0.99628097772598267)
('epoch', 143, 'train_loss:', 3.9648914635181427, 'val_loss:', 0.98986622452735906)
('epoch', 144, 'train_loss:', 3.9566581392288209, 'val_loss:', 0.99180324077606197)
('epoch', 145, 'train_loss:', 3.9459536862373352, 'val_loss:', 0.99119062662124635)
('epoch', 146, 'train_loss:', 3.9493346440792085, 'val_loss:', 0.99310754776000976)
('epoch', 147, 'train_loss:', 3.9415784919261934, 'val_loss:', 0.99019308924674987)
('epoch', 148, 'train_loss:', 3.937819789648056, 'val_loss:', 0.98635485768318176)
('epoch', 149, 'train_loss:', 3.9308892369270323, 'val_loss:', 0.98411271691322322)
('epoch', 150, 'train_loss:', 3.9225429582595823, 'val_loss:', 0.9861905407905579)
('epoch', 151, 'train_loss:', 3.9247616016864777, 'val_loss:', 0.98962523698806759)
('epoch', 152, 'train_loss:', 3.9165224957466127, 'val_loss:', 0.98374434947967526)
('epoch', 153, 'train_loss:', 3.9129867243766783, 'val_loss:', 0.98010969161987305)
('epoch', 154, 'train_loss:', 3.9139003801345824, 'val_loss:', 0.9828018760681152)
('epoch', 155, 'train_loss:', 3.9061591923236847, 'val_loss:', 0.98016822814941407)
('epoch', 156, 'train_loss:', 3.9077151310443878, 'val_loss:', 0.98141922235488888)
('epoch', 157, 'train_loss:', 3.9023471856117249, 'val_loss:', 0.97577558517456053)
('epoch', 158, 'train_loss:', 3.8948113703727723, 'val_loss:', 0.97716333866119387)
('epoch', 159, 'train_loss:', 3.8902913939952852, 'val_loss:', 0.97282868504524234)
('epoch', 160, 'train_loss:', 3.8841499817371368, 'val_loss:', 0.97565056920051574)
('epoch', 161, 'train_loss:', 3.8768615674972535, 'val_loss:', 0.976225221157074)
('epoch', 162, 'train_loss:', 3.8793761813640595, 'val_loss:', 0.972986056804657)
('epoch', 163, 'train_loss:', 3.8660547161102294, 'val_loss:', 0.97172330975532528)
('epoch', 164, 'train_loss:', 3.8631668925285338, 'val_loss:', 0.97377322554588319)
('epoch', 165, 'train_loss:', 3.8681527137756349, 'val_loss:', 0.97360743641853331)
('epoch', 166, 'train_loss:', 3.8587834250926973, 'val_loss:', 0.9679399955272675)
('epoch', 167, 'train_loss:', 3.8613451063632964, 'val_loss:', 0.97050532698631287)
('epoch', 168, 'train_loss:', 3.8539657354354859, 'val_loss:', 0.96445899486541753)
('epoch', 169, 'train_loss:', 3.8561920201778412, 'val_loss:', 0.96757411003112792)
('epoch', 170, 'train_loss:', 3.8470033442974092, 'val_loss:', 0.97037425398826604)
('epoch', 171, 'train_loss:', 3.8371547937393187, 'val_loss:', 0.96610869050025938)
('epoch', 172, 'train_loss:', 3.8409622335433959, 'val_loss:', 0.96311096668243412)
('epoch', 173, 'train_loss:', 3.8341280043125154, 'val_loss:', 0.96291951179504398)
('epoch', 174, 'train_loss:', 3.8345690107345582, 'val_loss:', 0.96667155027389529)
('epoch', 175, 'train_loss:', 3.8240251660346987, 'val_loss:', 0.96109588503837584)
('epoch', 176, 'train_loss:', 3.8236312413215638, 'val_loss:', 0.96462340831756588)
('epoch', 177, 'train_loss:', 3.8224319505691526, 'val_loss:', 0.9604066598415375)
('epoch', 178, 'train_loss:', 3.8130346751213073, 'val_loss:', 0.95760077714920044)
('epoch', 179, 'train_loss:', 3.813170576095581, 'val_loss:', 0.95918214440345761)
('epoch', 180, 'train_loss:', 3.8146693468093873, 'val_loss:', 0.9585827803611755)
('epoch', 181, 'train_loss:', 3.8079636907577514, 'val_loss:', 0.95671012401580813)
('epoch', 182, 'train_loss:', 3.8024623811244966, 'val_loss:', 0.9556373655796051)
('epoch', 183, 'train_loss:', 3.796339249610901, 'val_loss:', 0.9582603085041046)
('epoch', 184, 'train_loss:', 3.7990028107166292, 'val_loss:', 0.95562712788581849)
('epoch', 185, 'train_loss:', 3.7993818581104279, 'val_loss:', 0.95184262752532955)
('epoch', 186, 'train_loss:', 3.7921678543090822, 'val_loss:', 0.9511912882328033)
('epoch', 187, 'train_loss:', 3.787395405769348, 'val_loss:', 0.95067713618278504)
('epoch', 188, 'train_loss:', 3.7908024978637695, 'val_loss:', 0.95423329234123233)
('epoch', 189, 'train_loss:', 3.7765618193149568, 'val_loss:', 0.94674072265624998)
('epoch', 190, 'train_loss:', 3.7825845360755919, 'val_loss:', 0.9472200286388397)
('epoch', 191, 'train_loss:', 3.7828722143173219, 'val_loss:', 0.95105030298233029)
('epoch', 192, 'train_loss:', 3.7780867004394532, 'val_loss:', 0.94933736324310303)
('epoch', 193, 'train_loss:', 3.7702729213237762, 'val_loss:', 0.94908185243606569)
('epoch', 194, 'train_loss:', 3.7635410225391386, 'val_loss:', 0.94512941479682921)
('epoch', 195, 'train_loss:', 3.7750164580345156, 'val_loss:', 0.94565794706344608)
('epoch', 196, 'train_loss:', 3.760200699567795, 'val_loss:', 0.94355536818504337)
('epoch', 197, 'train_loss:', 3.7594304239749907, 'val_loss:', 0.9447453641891479)
('epoch', 198, 'train_loss:', 3.7557782578468322, 'val_loss:', 0.9426342248916626)
('epoch', 199, 'train_loss:', 3.7559306156635284, 'val_loss:', 0.94688152074813847)
('epoch', 200, 'train_loss:', 3.7516314172744751, 'val_loss:', 0.94740830659866337)
('epoch', 201, 'train_loss:', 3.7481059312820433, 'val_loss:', 0.94316377282142638)
('epoch', 202, 'train_loss:', 3.7397542130947112, 'val_loss:', 0.94362873554229731)
('epoch', 203, 'train_loss:', 3.7465312385559084, 'val_loss:', 0.94375341415405278)
('epoch', 204, 'train_loss:', 3.7396811985969545, 'val_loss:', 0.94509748458862308)
('epoch', 205, 'train_loss:', 3.7331982827186585, 'val_loss:', 0.94299657225608824)
('epoch', 206, 'train_loss:', 3.739902732372284, 'val_loss:', 0.94094926714897154)
('epoch', 207, 'train_loss:', 3.7204863464832307, 'val_loss:', 0.93510645985603336)
('epoch', 208, 'train_loss:', 3.7277629697322845, 'val_loss:', 0.9372385501861572)
('epoch', 209, 'train_loss:', 3.7281717109680175, 'val_loss:', 0.93737572312355044)
('epoch', 210, 'train_loss:', 3.7205122542381286, 'val_loss:', 0.93722573041915891)
('epoch', 211, 'train_loss:', 3.7204256677627563, 'val_loss:', 0.93509368538856508)
('epoch', 212, 'train_loss:', 3.7148490917682646, 'val_loss:', 0.93469229698181155)
('epoch', 213, 'train_loss:', 3.7154323518276215, 'val_loss:', 0.93447866320610051)
('epoch', 214, 'train_loss:', 3.7134047055244448, 'val_loss:', 0.93955348134040828)
('epoch', 215, 'train_loss:', 3.7118345594406126, 'val_loss:', 0.93470554471015932)
('epoch', 216, 'train_loss:', 3.7103507554531099, 'val_loss:', 0.93338751554489141)
('epoch', 217, 'train_loss:', 3.7060098493099214, 'val_loss:', 0.93898411870002751)
('epoch', 218, 'train_loss:', 3.702856993675232, 'val_loss:', 0.92970398545265198)
('epoch', 219, 'train_loss:', 3.7010992336273194, 'val_loss:', 0.93298679947853091)
('epoch', 220, 'train_loss:', 3.6946507406234743, 'val_loss:', 0.93287509679794312)
('epoch', 221, 'train_loss:', 3.6911078572273253, 'val_loss:', 0.93249648809432983)
('epoch', 222, 'train_loss:', 3.6926544702053068, 'val_loss:', 0.9273108065128326)
('epoch', 223, 'train_loss:', 3.6938132417201994, 'val_loss:', 0.93133705139160161)
('epoch', 224, 'train_loss:', 3.6894779813289644, 'val_loss:', 0.92791864633560184)
('epoch', 225, 'train_loss:', 3.683002145290375, 'val_loss:', 0.92772091150283809)
('epoch', 226, 'train_loss:', 3.6789646172523498, 'val_loss:', 0.92602063059806827)
('epoch', 227, 'train_loss:', 3.6846952700614928, 'val_loss:', 0.92951308131217958)
('epoch', 228, 'train_loss:', 3.6816552984714508, 'val_loss:', 0.92818517088890073)
('epoch', 229, 'train_loss:', 3.6800161957740785, 'val_loss:', 0.93046785116195674)
('epoch', 230, 'train_loss:', 3.6730218040943146, 'val_loss:', 0.92916841626167301)
('epoch', 231, 'train_loss:', 3.6710102093219756, 'val_loss:', 0.92539474964141843)
('epoch', 232, 'train_loss:', 3.6725726318359375, 'val_loss:', 0.92808458209037781)
('epoch', 233, 'train_loss:', 3.6609184765815734, 'val_loss:', 0.92309433698654175)
('epoch', 234, 'train_loss:', 3.6646304094791411, 'val_loss:', 0.92350129485130306)
('epoch', 235, 'train_loss:', 3.6623869824409483, 'val_loss:', 0.92808763384819026)
('epoch', 236, 'train_loss:', 3.6628794860839844, 'val_loss:', 0.92219786882400512)
('epoch', 237, 'train_loss:', 3.6586977875232698, 'val_loss:', 0.92442839145660405)
('epoch', 238, 'train_loss:', 3.6567854940891267, 'val_loss:', 0.92142395973205571)
('epoch', 239, 'train_loss:', 3.661007961034775, 'val_loss:', 0.92213788390159612)
('epoch', 240, 'train_loss:', 3.6508102560043336, 'val_loss:', 0.92074901342391968)
('epoch', 241, 'train_loss:', 3.64461975812912, 'val_loss:', 0.92089679002761837)
('epoch', 242, 'train_loss:', 3.6461625874042509, 'val_loss:', 0.91983491420745844)
('epoch', 243, 'train_loss:', 3.6474200701713562, 'val_loss:', 0.9204001188278198)
('epoch', 244, 'train_loss:', 3.6431081449985503, 'val_loss:', 0.92051586270332342)
('epoch', 245, 'train_loss:', 3.6413889181613923, 'val_loss:', 0.92012747287750241)
('epoch', 246, 'train_loss:', 3.6400243890285493, 'val_loss:', 0.91689900994300844)
('epoch', 247, 'train_loss:', 3.6417984378337862, 'val_loss:', 0.91533677458763119)
('epoch', 248, 'train_loss:', 3.6334772694110868, 'val_loss:', 0.91673511266708374)
('epoch', 249, 'train_loss:', 3.6263851404190062, 'val_loss:', 0.91756108164787298)
('epoch', 250, 'train_loss:', 3.6303989589214325, 'val_loss:', 0.91375809550285336)
('epoch', 251, 'train_loss:', 3.6288454389572142, 'val_loss:', 0.91877114295959472)
('epoch', 252, 'train_loss:', 3.6201518189907076, 'val_loss:', 0.91640310168266292)
('epoch', 253, 'train_loss:', 3.6359947609901426, 'val_loss:', 0.91694917440414425)
('epoch', 254, 'train_loss:', 3.6228437340259552, 'val_loss:', 0.9148579692840576)
('epoch', 255, 'train_loss:', 3.6217848646640776, 'val_loss:', 0.91438917636871342)
('epoch', 256, 'train_loss:', 3.6188942408561706, 'val_loss:', 0.9124841630458832)
('epoch', 257, 'train_loss:', 3.6160620927810667, 'val_loss:', 0.91106151580810546)
('epoch', 258, 'train_loss:', 3.6204483723640442, 'val_loss:', 0.91391324520111084)
('epoch', 259, 'train_loss:', 3.6161318945884706, 'val_loss:', 0.90976348042488098)
('epoch', 260, 'train_loss:', 3.6072299587726593, 'val_loss:', 0.91250685572624202)
('epoch', 261, 'train_loss:', 3.6147619521617891, 'val_loss:', 0.91554674863815311)
('epoch', 262, 'train_loss:', 3.6077138495445253, 'val_loss:', 0.90775822997093203)
('epoch', 263, 'train_loss:', 3.609006555080414, 'val_loss:', 0.91275045156478885)
('epoch', 264, 'train_loss:', 3.6068083870410921, 'val_loss:', 0.91032676815986635)
('epoch', 265, 'train_loss:', 3.6057606494426726, 'val_loss:', 0.91126738786697392)
('epoch', 266, 'train_loss:', 3.6104074335098266, 'val_loss:', 0.90835190653800968)
('epoch', 267, 'train_loss:', 3.5976316845417022, 'val_loss:', 0.91143027901649476)
('epoch', 268, 'train_loss:', 3.6006127512454986, 'val_loss:', 0.90895952582359318)
('epoch', 269, 'train_loss:', 3.5945992517471312, 'val_loss:', 0.90872412085533139)
('epoch', 270, 'train_loss:', 3.5973468863964082, 'val_loss:', 0.90672026753425594)
('epoch', 271, 'train_loss:', 3.598339216709137, 'val_loss:', 0.90961665868759156)
('epoch', 272, 'train_loss:', 3.5940328145027163, 'val_loss:', 0.90793272137641912)
('epoch', 273, 'train_loss:', 3.582389818429947, 'val_loss:', 0.90613552212715154)
('epoch', 274, 'train_loss:', 3.5878032350540163, 'val_loss:', 0.9059312641620636)
('epoch', 275, 'train_loss:', 3.5872449696063997, 'val_loss:', 0.90577223420143127)
('epoch', 276, 'train_loss:', 3.5830365478992463, 'val_loss:', 0.90354143500328066)
('epoch', 277, 'train_loss:', 3.5802001404762267, 'val_loss:', 0.90526802182197574)
('epoch', 278, 'train_loss:', 3.578555476665497, 'val_loss:', 0.90362146615982053)
('epoch', 279, 'train_loss:', 3.5785474646091462, 'val_loss:', 0.90209620714187622)
('epoch', 280, 'train_loss:', 3.5756695568561554, 'val_loss:', 0.90835673928260807)
('epoch', 281, 'train_loss:', 3.5735863447189331, 'val_loss:', 0.9047907793521881)
('epoch', 282, 'train_loss:', 3.5792679822444917, 'val_loss:', 0.90431323051452639)
('epoch', 283, 'train_loss:', 3.5712807404994966, 'val_loss:', 0.90162087440490724)
('epoch', 284, 'train_loss:', 3.5683079314231874, 'val_loss:', 0.90351788997650151)
('epoch', 285, 'train_loss:', 3.5686089956760405, 'val_loss:', 0.90616990804672237)
('epoch', 286, 'train_loss:', 3.5684206843376161, 'val_loss:', 0.90556055307388306)
('epoch', 287, 'train_loss:', 3.568672435283661, 'val_loss:', 0.90175855040550235)
('epoch', 288, 'train_loss:', 3.5644333159923551, 'val_loss:', 0.90291301012039182)
('epoch', 289, 'train_loss:', 3.5638549268245696, 'val_loss:', 0.90308554172515865)
('epoch', 290, 'train_loss:', 3.5613284909725191, 'val_loss:', 0.90401048541069029)
('epoch', 291, 'train_loss:', 3.5519116520881653, 'val_loss:', 0.90139963030815129)
('epoch', 292, 'train_loss:', 3.5625441026687623, 'val_loss:', 0.90003870010375975)
('epoch', 293, 'train_loss:', 3.560234444141388, 'val_loss:', 0.89769967794418337)
('epoch', 294, 'train_loss:', 3.5538248407840727, 'val_loss:', 0.89848616361618039)
('epoch', 295, 'train_loss:', 3.5551307797431946, 'val_loss:', 0.89820785641670231)
('epoch', 296, 'train_loss:', 3.5496428143978118, 'val_loss:', 0.8955456686019897)
('epoch', 297, 'train_loss:', 3.5506596791744234, 'val_loss:', 0.90117769241333012)
('epoch', 298, 'train_loss:', 3.5511765897274019, 'val_loss:', 0.8981111407279968)
('epoch', 299, 'train_loss:', 3.5500978803634644, 'val_loss:', 0.89611803174018856)
('epoch', 300, 'train_loss:', 3.5493436896800996, 'val_loss:', 0.8969139766693115)
('epoch', 301, 'train_loss:', 3.5411428987979887, 'val_loss:', 0.8996618473529816)
('epoch', 302, 'train_loss:', 3.5447911584377287, 'val_loss:', 0.89819829583168032)
('epoch', 303, 'train_loss:', 3.5384348213672636, 'val_loss:', 0.89616498947143552)
('epoch', 304, 'train_loss:', 3.5432947885990145, 'val_loss:', 0.90044999718666074)
('epoch', 305, 'train_loss:', 3.5373589456081391, 'val_loss:', 0.89368201732635499)
('epoch', 306, 'train_loss:', 3.5375361359119415, 'val_loss:', 0.89672725200653081)
('epoch', 307, 'train_loss:', 3.5315381360054015, 'val_loss:', 0.89569743394851686)
('epoch', 308, 'train_loss:', 3.5342781126499174, 'val_loss:', 0.89593695878982549)
('epoch', 309, 'train_loss:', 3.5345615375041963, 'val_loss:', 0.89583374977111818)
('epoch', 310, 'train_loss:', 3.534420371055603, 'val_loss:', 0.89615612030029301)
('epoch', 311, 'train_loss:', 3.5331053400039671, 'val_loss:', 0.89627296447753901)
('epoch', 312, 'train_loss:', 3.5291197991371153, 'val_loss:', 0.892817759513855)
('epoch', 313, 'train_loss:', 3.5306949746608733, 'val_loss:', 0.89596673250198366)
('epoch', 314, 'train_loss:', 3.5295722389221194, 'val_loss:', 0.89323536276817317)
('epoch', 315, 'train_loss:', 3.519573450088501, 'val_loss:', 0.89964848160743716)
('epoch', 316, 'train_loss:', 3.5241206920146944, 'val_loss:', 0.89437397956848141)
('epoch', 317, 'train_loss:', 3.5176009976863862, 'val_loss:', 0.89419070005416867)
('epoch', 318, 'train_loss:', 3.5209595000743867, 'val_loss:', 0.89280085563659672)
('epoch', 319, 'train_loss:', 3.5201862394809722, 'val_loss:', 0.89142620205879208)
('epoch', 320, 'train_loss:', 3.5195884454250335, 'val_loss:', 0.89127015233039852)
('epoch', 321, 'train_loss:', 3.5166039133071898, 'val_loss:', 0.89065143585205075)
('epoch', 322, 'train_loss:', 3.5111173748970033, 'val_loss:', 0.89353133559226994)
('epoch', 323, 'train_loss:', 3.5124887728691103, 'val_loss:', 0.88861026048660274)
('epoch', 324, 'train_loss:', 3.5080744659900667, 'val_loss:', 0.8921541845798493)
('epoch', 325, 'train_loss:', 3.5095847654342651, 'val_loss:', 0.88686296820640564)
('epoch', 326, 'train_loss:', 3.5100719571113586, 'val_loss:', 0.89179351925849915)
('epoch', 327, 'train_loss:', 3.5147205471992491, 'val_loss:', 0.88874015569686893)
('epoch', 328, 'train_loss:', 3.5082584524154665, 'val_loss:', 0.89108716368675234)
('epoch', 329, 'train_loss:', 3.5096235823631288, 'val_loss:', 0.89184625625610348)
('epoch', 330, 'train_loss:', 3.5077937400341033, 'val_loss:', 0.8903088915348053)
('epoch', 331, 'train_loss:', 3.5017714607715607, 'val_loss:', 0.89016255617141726)
('epoch', 332, 'train_loss:', 3.5060961842536926, 'val_loss:', 0.88994625329971311)
('epoch', 333, 'train_loss:', 3.4973367393016814, 'val_loss:', 0.88947978138923645)
('epoch', 334, 'train_loss:', 3.5038938450813295, 'val_loss:', 0.88898225069046022)
('epoch', 335, 'train_loss:', 3.4969379794597626, 'val_loss:', 0.89132890105247498)
('epoch', 336, 'train_loss:', 3.4990659201145173, 'val_loss:', 0.88901925325393671)
('epoch', 337, 'train_loss:', 3.5051142179965975, 'val_loss:', 0.88835648298263548)
('epoch', 338, 'train_loss:', 3.4982938683032989, 'val_loss:', 0.89261107921600347)
('epoch', 339, 'train_loss:', 3.4936309742927549, 'val_loss:', 0.89025886774063112)
('epoch', 340, 'train_loss:', 3.4956755888462068, 'val_loss:', 0.89301034808158875)
('epoch', 341, 'train_loss:', 3.4912865746021269, 'val_loss:', 0.88837976813316344)
('epoch', 342, 'train_loss:', 3.4883288228511811, 'val_loss:', 0.88729672312736507)
('epoch', 343, 'train_loss:', 3.4894729006290435, 'val_loss:', 0.88458416938781737)
('epoch', 344, 'train_loss:', 3.4856079661846162, 'val_loss:', 0.8885463404655457)
('epoch', 345, 'train_loss:', 3.4854185140132903, 'val_loss:', 0.88372232198715206)
('epoch', 346, 'train_loss:', 3.4853412437438966, 'val_loss:', 0.88495161414146428)
('epoch', 347, 'train_loss:', 3.4896963322162629, 'val_loss:', 0.88576400279998779)
('epoch', 348, 'train_loss:', 3.4822810173034666, 'val_loss:', 0.88234502196311948)
('epoch', 349, 'train_loss:', 3.4840567910671236, 'val_loss:', 0.88482469916343687)
('epoch', 350, 'train_loss:', 3.4849385511875153, 'val_loss:', 0.88349287629127504)
('epoch', 351, 'train_loss:', 3.4792838811874391, 'val_loss:', 0.88467738866806034)
('epoch', 352, 'train_loss:', 3.4785112750530245, 'val_loss:', 0.88232014536857606)
('epoch', 353, 'train_loss:', 3.485444746017456, 'val_loss:', 0.88636901378631594)
('epoch', 354, 'train_loss:', 3.4826252400875091, 'val_loss:', 0.88224012017250064)
('epoch', 355, 'train_loss:', 3.4821714377403259, 'val_loss:', 0.88210801005363459)
('epoch', 356, 'train_loss:', 3.4780188429355623, 'val_loss:', 0.88136952877044683)
('epoch', 357, 'train_loss:', 3.4720524919033049, 'val_loss:', 0.88449134469032287)
('epoch', 358, 'train_loss:', 3.4726272642612459, 'val_loss:', 0.88006398797035212)
('epoch', 359, 'train_loss:', 3.465497019290924, 'val_loss:', 0.88155071139335628)
('epoch', 360, 'train_loss:', 3.4689564025402069, 'val_loss:', 0.88260650515556338)
('epoch', 361, 'train_loss:', 3.4664248239994051, 'val_loss:', 0.87994748115539556)
('epoch', 362, 'train_loss:', 3.4684359538555145, 'val_loss:', 0.87937408566474917)
('epoch', 363, 'train_loss:', 3.4672788739204408, 'val_loss:', 0.88057487726211547)
('epoch', 364, 'train_loss:', 3.4692417573928833, 'val_loss:', 0.8812279081344605)
('epoch', 365, 'train_loss:', 3.4649923563003542, 'val_loss:', 0.88293280005455022)
('epoch', 366, 'train_loss:', 3.4598378896713258, 'val_loss:', 0.88046371817588809)
('epoch', 367, 'train_loss:', 3.4626377093791962, 'val_loss:', 0.87995249032974243)
('epoch', 368, 'train_loss:', 3.4603803765773775, 'val_loss:', 0.87823105692863468)
('epoch', 369, 'train_loss:', 3.4600265526771548, 'val_loss:', 0.8774050271511078)
('epoch', 370, 'train_loss:', 3.4567635869979858, 'val_loss:', 0.88181599259376531)
('epoch', 371, 'train_loss:', 3.459501212835312, 'val_loss:', 0.88531870484352115)
('epoch', 372, 'train_loss:', 3.4634192585945129, 'val_loss:', 0.87752259254455567)
('epoch', 373, 'train_loss:', 3.4527468633651734, 'val_loss:', 0.87955542564392086)
('epoch', 374, 'train_loss:', 3.4527968764305115, 'val_loss:', 0.87943038821220398)
('epoch', 375, 'train_loss:', 3.4552583944797517, 'val_loss:', 0.88192474007606503)
('epoch', 376, 'train_loss:', 3.4480857121944428, 'val_loss:', 0.87765756964683528)
('epoch', 377, 'train_loss:', 3.4478049433231353, 'val_loss:', 0.87897011041641238)
('epoch', 378, 'train_loss:', 3.4503163933753966, 'val_loss:', 0.87809255957603449)
('epoch', 379, 'train_loss:', 3.4502393555641175, 'val_loss:', 0.87697689414024349)
('epoch', 380, 'train_loss:', 3.4453563523292541, 'val_loss:', 0.87615117669105524)
('epoch', 381, 'train_loss:', 3.446615322828293, 'val_loss:', 0.87350602746009831)
('epoch', 382, 'train_loss:', 3.44662246465683, 'val_loss:', 0.87627801656723026)
('epoch', 383, 'train_loss:', 3.4453628408908843, 'val_loss:', 0.87459364533424377)
('epoch', 384, 'train_loss:', 3.4431897413730623, 'val_loss:', 0.87539298653602604)
('epoch', 385, 'train_loss:', 3.4407425785064696, 'val_loss:', 0.87541428208351135)
('epoch', 386, 'train_loss:', 3.4341560816764831, 'val_loss:', 0.87716193795204167)
('epoch', 387, 'train_loss:', 3.4388838672637938, 'val_loss:', 0.87542064309120182)
('epoch', 388, 'train_loss:', 3.4368441712856295, 'val_loss:', 0.87669387817382816)
('epoch', 389, 'train_loss:', 3.4391883134841921, 'val_loss:', 0.87693969964981078)
('epoch', 390, 'train_loss:', 3.4363365209102632, 'val_loss:', 0.87655143380165101)
('epoch', 391, 'train_loss:', 3.4345130574703218, 'val_loss:', 0.87491026639938352)
('epoch', 392, 'train_loss:', 3.433337095975876, 'val_loss:', 0.87663381934165951)
('epoch', 393, 'train_loss:', 3.4384777343273161, 'val_loss:', 0.87480535984039309)
('epoch', 394, 'train_loss:', 3.4341750979423522, 'val_loss:', 0.87925515413284305)
('epoch', 395, 'train_loss:', 3.4329456865787504, 'val_loss:', 0.87741575956344608)
('epoch', 396, 'train_loss:', 3.4257377588748934, 'val_loss:', 0.87324516534805297)
('epoch', 397, 'train_loss:', 3.4327600169181824, 'val_loss:', 0.87554013609886172)
('epoch', 398, 'train_loss:', 3.4341477727890015, 'val_loss:', 0.87587187290191648)
('epoch', 399, 'train_loss:', 3.4357533848285673, 'val_loss:', 0.87751474976539612)
('epoch', 400, 'train_loss:', 3.4255192029476165, 'val_loss:', 0.87713253378868106)
('epoch', 401, 'train_loss:', 3.4217284011840818, 'val_loss:', 0.87745514512062073)
('epoch', 402, 'train_loss:', 3.4216397178173064, 'val_loss:', 0.87344479441642764)
('epoch', 403, 'train_loss:', 3.4231824028491973, 'val_loss:', 0.87671941041946411)
('epoch', 404, 'train_loss:', 3.4261967873573305, 'val_loss:', 0.8752523076534271)
('epoch', 405, 'train_loss:', 3.4211630415916443, 'val_loss:', 0.87150933980941769)
('epoch', 406, 'train_loss:', 3.4205603480339049, 'val_loss:', 0.87050915956497188)
('epoch', 407, 'train_loss:', 3.4197077226638792, 'val_loss:', 0.87262936711311345)
('epoch', 408, 'train_loss:', 3.4190295839309695, 'val_loss:', 0.87576544642448428)
('epoch', 409, 'train_loss:', 3.4183227288722993, 'val_loss:', 0.87095378518104549)
('epoch', 410, 'train_loss:', 3.4192932283878328, 'val_loss:', 0.87189662337303164)
('epoch', 411, 'train_loss:', 3.41576531291008, 'val_loss:', 0.86896115422248843)
('epoch', 412, 'train_loss:', 3.4147965943813325, 'val_loss:', 0.87098062276840205)
('epoch', 413, 'train_loss:', 3.4132750058174133, 'val_loss:', 0.86993809700012203)
('epoch', 414, 'train_loss:', 3.4124150288105013, 'val_loss:', 0.8713200128078461)
('epoch', 415, 'train_loss:', 3.4171292257308958, 'val_loss:', 0.86895737051963806)
('epoch', 416, 'train_loss:', 3.4125010764598844, 'val_loss:', 0.87320015668869022)
('epoch', 417, 'train_loss:', 3.4128002524375916, 'val_loss:', 0.86952366948127746)
('epoch', 418, 'train_loss:', 3.4075212311744689, 'val_loss:', 0.86893533825874325)
('epoch', 419, 'train_loss:', 3.4141659307479859, 'val_loss:', 0.87463421940803532)
('epoch', 420, 'train_loss:', 3.4121575248241425, 'val_loss:', 0.8726032531261444)
('epoch', 421, 'train_loss:', 3.4167589521408082, 'val_loss:', 0.87149970889091488)
('epoch', 422, 'train_loss:', 3.4058294856548308, 'val_loss:', 0.86843466281890869)
('epoch', 423, 'train_loss:', 3.4070923459529876, 'val_loss:', 0.87133799433708192)
('epoch', 424, 'train_loss:', 3.4099916470050813, 'val_loss:', 0.86956981182098392)
('epoch', 425, 'train_loss:', 3.4082195484638214, 'val_loss:', 0.87077405095100402)
('epoch', 426, 'train_loss:', 3.4097524678707121, 'val_loss:', 0.86832253575325014)
('epoch', 427, 'train_loss:', 3.4048404026031496, 'val_loss:', 0.8680720329284668)
('epoch', 428, 'train_loss:', 3.4034138882160185, 'val_loss:', 0.86729910731315618)
('epoch', 429, 'train_loss:', 3.4019898688793182, 'val_loss:', 0.86832629323005672)
('epoch', 430, 'train_loss:', 3.4033572626113893, 'val_loss:', 0.86719244241714477)
('epoch', 431, 'train_loss:', 3.399254833459854, 'val_loss:', 0.86684766173362737)
('epoch', 432, 'train_loss:', 3.4002696478366854, 'val_loss:', 0.86726432085037231)
('epoch', 433, 'train_loss:', 3.3987601566314698, 'val_loss:', 0.8697774147987366)
('epoch', 434, 'train_loss:', 3.3928875660896303, 'val_loss:', 0.86700650811195379)
('epoch', 435, 'train_loss:', 3.3963607060909271, 'val_loss:', 0.86882411241531377)
('epoch', 436, 'train_loss:', 3.389874759912491, 'val_loss:', 0.86809371113777156)
('epoch', 437, 'train_loss:', 3.3975961172580718, 'val_loss:', 0.86781506538391118)
('epoch', 438, 'train_loss:', 3.395723432302475, 'val_loss:', 0.86932522535324097)
('epoch', 439, 'train_loss:', 3.3944130396842955, 'val_loss:', 0.86832431793212894)
('epoch', 440, 'train_loss:', 3.3915293788909913, 'val_loss:', 0.86624996304512025)
('epoch', 441, 'train_loss:', 3.3855023407936096, 'val_loss:', 0.86636812329292301)
('epoch', 442, 'train_loss:', 3.3930933487415316, 'val_loss:', 0.86713723540306087)
('epoch', 443, 'train_loss:', 3.3906198227405548, 'val_loss:', 0.86507316112518307)
('epoch', 444, 'train_loss:', 3.3882349050045013, 'val_loss:', 0.86684244990348813)
('epoch', 445, 'train_loss:', 3.3935068738460541, 'val_loss:', 0.86758687496185305)
('epoch', 446, 'train_loss:', 3.3827746939659118, 'val_loss:', 0.86657306671142575)
('epoch', 447, 'train_loss:', 3.3919432950019837, 'val_loss:', 0.86971181869506831)
('epoch', 448, 'train_loss:', 3.381515688896179, 'val_loss:', 0.86240115284919738)
('epoch', 449, 'train_loss:', 3.3831191098690034, 'val_loss:', 0.86999575972557064)
('epoch', 450, 'train_loss:', 3.3817960011959074, 'val_loss:', 0.86689801931381227)
('epoch', 451, 'train_loss:', 3.3818533635139465, 'val_loss:', 0.87035670876502991)
('epoch', 452, 'train_loss:', 3.3852482295036315, 'val_loss:', 0.86341235995292664)
('epoch', 453, 'train_loss:', 3.3807802271842955, 'val_loss:', 0.86352138400077816)
('epoch', 454, 'train_loss:', 3.3817025291919709, 'val_loss:', 0.86731492161750789)
('epoch', 455, 'train_loss:', 3.3813087654113771, 'val_loss:', 0.86417817115783691)
('epoch', 456, 'train_loss:', 3.3749099624156953, 'val_loss:', 0.86619517922401423)
('epoch', 457, 'train_loss:', 3.3763329076766966, 'val_loss:', 0.86261648297309879)
('epoch', 458, 'train_loss:', 3.3779247522354128, 'val_loss:', 0.86395723342895503)
('epoch', 459, 'train_loss:', 3.3775547385215758, 'val_loss:', 0.86429866552352907)
('epoch', 460, 'train_loss:', 3.3782899487018585, 'val_loss:', 0.86517028093338011)
('epoch', 461, 'train_loss:', 3.3761058473587036, 'val_loss:', 0.86368349313735959)
('epoch', 462, 'train_loss:', 3.3712491703033449, 'val_loss:', 0.8647504115104675)
('epoch', 463, 'train_loss:', 3.3719987261295317, 'val_loss:', 0.86332776308059689)
('epoch', 464, 'train_loss:', 3.3733322501182554, 'val_loss:', 0.86282147288322453)
('epoch', 465, 'train_loss:', 3.3749033188819886, 'val_loss:', 0.86489217519760131)
('epoch', 466, 'train_loss:', 3.3716268491744996, 'val_loss:', 0.86453330516815186)
('epoch', 467, 'train_loss:', 3.3738087248802184, 'val_loss:', 0.86425957679748533)
('epoch', 468, 'train_loss:', 3.3730845201015471, 'val_loss:', 0.86298640489578249)
('epoch', 469, 'train_loss:', 3.3697154629230499, 'val_loss:', 0.86394780516624448)
('epoch', 470, 'train_loss:', 3.3654589796066285, 'val_loss:', 0.86013543844223017)
('epoch', 471, 'train_loss:', 3.3630519020557403, 'val_loss:', 0.85935249209403997)
('epoch', 472, 'train_loss:', 3.3674343323707578, 'val_loss:', 0.86302313446998591)
('epoch', 473, 'train_loss:', 3.3673132574558258, 'val_loss:', 0.86099212050437923)
('epoch', 474, 'train_loss:', 3.3620251309871674, 'val_loss:', 0.86302059292793276)
('epoch', 475, 'train_loss:', 3.3546380007267, 'val_loss:', 0.86080482721328733)
('epoch', 476, 'train_loss:', 3.3620239794254303, 'val_loss:', 0.86027445673942571)
('epoch', 477, 'train_loss:', 3.3613966858386992, 'val_loss:', 0.86313052773475651)
('epoch', 478, 'train_loss:', 3.3600290429592135, 'val_loss:', 0.86160109639167781)
('epoch', 479, 'train_loss:', 3.3599767363071442, 'val_loss:', 0.86188452363014223)
('epoch', 480, 'train_loss:', 3.3574115085601806, 'val_loss:', 0.86534229397773743)
('epoch', 481, 'train_loss:', 3.3599688148498537, 'val_loss:', 0.86300475358963014)
('epoch', 482, 'train_loss:', 3.3533770740032196, 'val_loss:', 0.8588049650192261)
('epoch', 483, 'train_loss:', 3.3552039492130281, 'val_loss:', 0.86171465396881108)
('epoch', 484, 'train_loss:', 3.3570115363597868, 'val_loss:', 0.85889190554618833)
('epoch', 485, 'train_loss:', 3.3483464503288269, 'val_loss:', 0.85905725240707398)
('epoch', 486, 'train_loss:', 3.3601140785217285, 'val_loss:', 0.86635838031768797)
('epoch', 487, 'train_loss:', 3.3587614738941194, 'val_loss:', 0.85852525591850282)
('epoch', 488, 'train_loss:', 3.3547168624401094, 'val_loss:', 0.86178711414337161)
('epoch', 489, 'train_loss:', 3.3468164443969726, 'val_loss:', 0.8625076544284821)
('epoch', 490, 'train_loss:', 3.3584705018997192, 'val_loss:', 0.86053496599197388)
('epoch', 491, 'train_loss:', 3.3512422502040864, 'val_loss:', 0.85938649058341976)
('epoch', 492, 'train_loss:', 3.3504714965820312, 'val_loss:', 0.85972917199134824)
('epoch', 493, 'train_loss:', 3.3503828072547912, 'val_loss:', 0.86074325323104861)
('epoch', 494, 'train_loss:', 3.3430902910232545, 'val_loss:', 0.86011347293853757)
('epoch', 495, 'train_loss:', 3.3586117351055145, 'val_loss:', 0.85754218459129339)
('epoch', 496, 'train_loss:', 3.3483400547504427, 'val_loss:', 0.85698049306869506)
('epoch', 497, 'train_loss:', 3.3463711893558501, 'val_loss:', 0.86266175150871272)
('epoch', 498, 'train_loss:', 3.3507497656345366, 'val_loss:', 0.86503350615501406)
('epoch', 499, 'train_loss:', 3.3475984358787536, 'val_loss:', 0.86007475733757022)

