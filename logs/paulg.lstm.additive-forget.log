(keras)skoppula@sls-sm-9:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=1 python lstm-additive-forget.py -t
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
('fetched data. trn/test data shape: ', (63514, 30), (63514, 30), (15879, 30), (15879, 30))
('num steps in trn and val epochs', 248, 62)
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3154 get requests, put_count=2815 evicted_count=1000 eviction_rate=0.35524 and unsatisfied allocation rate=0.456246
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3333 get requests, put_count=3491 evicted_count=1000 eviction_rate=0.286451 and unsatisfied allocation rate=0.259526
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
('epoch', 0, 'train_loss:', 9.3218508648872369, 'val_loss:', 2.0026559662818908)
('epoch', 1, 'train_loss:', 7.3292249488830565, 'val_loss:', 1.7199088430404663)
('epoch', 2, 'train_loss:', 6.7032526445388791, 'val_loss:', 1.6461724328994751)
('epoch', 3, 'train_loss:', 6.4914068531990052, 'val_loss:', 1.6025504755973816)
('epoch', 4, 'train_loss:', 6.3591096258163455, 'val_loss:', 1.5718808078765869)
('epoch', 5, 'train_loss:', 6.2610313987731931, 'val_loss:', 1.5540801501274109)
('epoch', 6, 'train_loss:', 6.1771776008605954, 'val_loss:', 1.5331328988075257)
('epoch', 7, 'train_loss:', 6.1089252591133114, 'val_loss:', 1.5193619489669801)
('epoch', 8, 'train_loss:', 6.0417332792282101, 'val_loss:', 1.503378849029541)
('epoch', 9, 'train_loss:', 5.9810462260246275, 'val_loss:', 1.4849123072624206)
('epoch', 10, 'train_loss:', 5.9250285243988037, 'val_loss:', 1.4753077960014342)
('epoch', 11, 'train_loss:', 5.8650679636001586, 'val_loss:', 1.4571383118629455)
('epoch', 12, 'train_loss:', 5.8119500303268437, 'val_loss:', 1.4441572308540345)
('epoch', 13, 'train_loss:', 5.7614681005477903, 'val_loss:', 1.4336408853530884)
('epoch', 14, 'train_loss:', 5.71061794757843, 'val_loss:', 1.4205084228515625)
('epoch', 15, 'train_loss:', 5.6613184165954591, 'val_loss:', 1.4078262138366699)
('epoch', 16, 'train_loss:', 5.6143905305862427, 'val_loss:', 1.3975881099700929)
('epoch', 17, 'train_loss:', 5.572780213356018, 'val_loss:', 1.386789801120758)
('epoch', 18, 'train_loss:', 5.5330331230163576, 'val_loss:', 1.3758368754386903)
('epoch', 19, 'train_loss:', 5.4944734692573549, 'val_loss:', 1.3656644010543824)
('epoch', 20, 'train_loss:', 5.4563295483589176, 'val_loss:', 1.3598231387138366)
('epoch', 21, 'train_loss:', 5.4177084827423094, 'val_loss:', 1.34926828622818)
('epoch', 22, 'train_loss:', 5.3762105584144591, 'val_loss:', 1.338062391281128)
('epoch', 23, 'train_loss:', 5.344595575332642, 'val_loss:', 1.3292865276336669)
('epoch', 24, 'train_loss:', 5.3093445444107052, 'val_loss:', 1.3235738563537598)
('epoch', 25, 'train_loss:', 5.2750919699668888, 'val_loss:', 1.3133863949775695)
('epoch', 26, 'train_loss:', 5.2456880617141728, 'val_loss:', 1.3013656449317932)
('epoch', 27, 'train_loss:', 5.2072139930725099, 'val_loss:', 1.2953823065757752)
('epoch', 28, 'train_loss:', 5.1716286635398863, 'val_loss:', 1.28851726770401)
('epoch', 29, 'train_loss:', 5.1346746587753298, 'val_loss:', 1.2811818742752075)
('epoch', 30, 'train_loss:', 5.1038858175277708, 'val_loss:', 1.2708880043029784)
('epoch', 31, 'train_loss:', 5.077032899856567, 'val_loss:', 1.2637152445316315)
('epoch', 32, 'train_loss:', 5.0438410151004796, 'val_loss:', 1.2575998294353485)
('epoch', 33, 'train_loss:', 5.0129889130592344, 'val_loss:', 1.2456273901462556)
('epoch', 34, 'train_loss:', 4.9822949159145358, 'val_loss:', 1.2390983748435973)
('epoch', 35, 'train_loss:', 4.9482918632030488, 'val_loss:', 1.2324905717372894)
('epoch', 36, 'train_loss:', 4.9252966606616972, 'val_loss:', 1.2275162613391877)
('epoch', 37, 'train_loss:', 4.8905823779106141, 'val_loss:', 1.2188664257526398)
('epoch', 38, 'train_loss:', 4.8573853802680969, 'val_loss:', 1.2108305418491363)
('epoch', 39, 'train_loss:', 4.8351782119274143, 'val_loss:', 1.2049655485153199)
('epoch', 40, 'train_loss:', 4.8016194903850558, 'val_loss:', 1.1957475006580354)
('epoch', 41, 'train_loss:', 4.7799467444419861, 'val_loss:', 1.1920022869110107)
('epoch', 42, 'train_loss:', 4.7494752848148343, 'val_loss:', 1.1894157707691193)
('epoch', 43, 'train_loss:', 4.7225965654850004, 'val_loss:', 1.1764283990859985)
('epoch', 44, 'train_loss:', 4.6973650288581847, 'val_loss:', 1.1703061270713806)
('epoch', 45, 'train_loss:', 4.6800897395610805, 'val_loss:', 1.1690732908248902)
('epoch', 46, 'train_loss:', 4.6550134956836704, 'val_loss:', 1.1630180883407593)
('epoch', 47, 'train_loss:', 4.628629781007767, 'val_loss:', 1.1574276113510131)
('epoch', 48, 'train_loss:', 4.6055086708068851, 'val_loss:', 1.1520962297916413)
('epoch', 49, 'train_loss:', 4.5822053313255307, 'val_loss:', 1.1403119313716887)
('epoch', 50, 'train_loss:', 4.5580240535736083, 'val_loss:', 1.1409901297092437)
('epoch', 51, 'train_loss:', 4.5410427868366243, 'val_loss:', 1.1350933516025543)
('epoch', 52, 'train_loss:', 4.5201141715049742, 'val_loss:', 1.1278204655647277)
('epoch', 53, 'train_loss:', 4.4939540076255797, 'val_loss:', 1.1245368063449859)
('epoch', 54, 'train_loss:', 4.4771936750411987, 'val_loss:', 1.1173026394844054)
('epoch', 55, 'train_loss:', 4.4561934280395512, 'val_loss:', 1.1139544558525085)
('epoch', 56, 'train_loss:', 4.434993481636047, 'val_loss:', 1.1092307448387146)
('epoch', 57, 'train_loss:', 4.4179124569892885, 'val_loss:', 1.1033748269081116)
('epoch', 58, 'train_loss:', 4.3984601032733917, 'val_loss:', 1.0982265532016755)
('epoch', 59, 'train_loss:', 4.3828346383571626, 'val_loss:', 1.0965469408035278)
('epoch', 60, 'train_loss:', 4.3605240035057067, 'val_loss:', 1.0897383284568787)
('epoch', 61, 'train_loss:', 4.3487945067882539, 'val_loss:', 1.0863326156139375)
('epoch', 62, 'train_loss:', 4.3327811729907992, 'val_loss:', 1.0846727120876312)
('epoch', 63, 'train_loss:', 4.3084518015384674, 'val_loss:', 1.0788183808326721)
('epoch', 64, 'train_loss:', 4.2941798818111421, 'val_loss:', 1.0728112041950226)
('epoch', 65, 'train_loss:', 4.2786721360683444, 'val_loss:', 1.0724479687213897)
('epoch', 66, 'train_loss:', 4.2688898742198944, 'val_loss:', 1.0652575707435608)
('epoch', 67, 'train_loss:', 4.250147407054901, 'val_loss:', 1.06403449177742)
('epoch', 68, 'train_loss:', 4.2432656931877135, 'val_loss:', 1.0606809866428375)
('epoch', 69, 'train_loss:', 4.2225582873821255, 'val_loss:', 1.0523003005981446)
('epoch', 70, 'train_loss:', 4.2090372884273526, 'val_loss:', 1.0537107956409455)
('epoch', 71, 'train_loss:', 4.1919006848335263, 'val_loss:', 1.0519905161857606)
('epoch', 72, 'train_loss:', 4.1945999646186829, 'val_loss:', 1.045183789730072)
('epoch', 73, 'train_loss:', 4.1621851849555966, 'val_loss:', 1.0425818574428558)
('epoch', 74, 'train_loss:', 4.1585065853595733, 'val_loss:', 1.0382908332347869)
('epoch', 75, 'train_loss:', 4.1495429766178127, 'val_loss:', 1.0369399356842042)
('epoch', 76, 'train_loss:', 4.1386101818084713, 'val_loss:', 1.0356659996509552)
('epoch', 77, 'train_loss:', 4.1237913990020756, 'val_loss:', 1.0322651982307434)
('epoch', 78, 'train_loss:', 4.1128825259208677, 'val_loss:', 1.0309302842617034)
('epoch', 79, 'train_loss:', 4.102228045463562, 'val_loss:', 1.0254503858089448)
('epoch', 80, 'train_loss:', 4.0867361342906952, 'val_loss:', 1.0219924724102021)
('epoch', 81, 'train_loss:', 4.0759938406944274, 'val_loss:', 1.0244322013854981)
('epoch', 82, 'train_loss:', 4.0734414100646976, 'val_loss:', 1.0201748323440551)
('epoch', 83, 'train_loss:', 4.0554209613800047, 'val_loss:', 1.0157023882865905)
('epoch', 84, 'train_loss:', 4.04209912776947, 'val_loss:', 1.013856028318405)
('epoch', 85, 'train_loss:', 4.0467053043842318, 'val_loss:', 1.0151587200164796)
('epoch', 86, 'train_loss:', 4.0285034394264221, 'val_loss:', 1.0098731970787049)
('epoch', 87, 'train_loss:', 4.0218562269210816, 'val_loss:', 1.0070028245449065)
('epoch', 88, 'train_loss:', 4.008330142498016, 'val_loss:', 1.0060755288600922)
('epoch', 89, 'train_loss:', 3.9950892698764799, 'val_loss:', 1.0055634629726411)
('epoch', 90, 'train_loss:', 3.9907698059082031, 'val_loss:', 1.0006480622291565)
('epoch', 91, 'train_loss:', 3.977147789001465, 'val_loss:', 1.000810455083847)
('epoch', 92, 'train_loss:', 3.975251431465149, 'val_loss:', 0.99609914541244504)
('epoch', 93, 'train_loss:', 3.9652147614955902, 'val_loss:', 0.99504180669784548)
('epoch', 94, 'train_loss:', 3.9616432344913481, 'val_loss:', 0.99228539586067199)
('epoch', 95, 'train_loss:', 3.9508497357368468, 'val_loss:', 0.98843830585479742)
('epoch', 96, 'train_loss:', 3.9440873992443084, 'val_loss:', 0.98918598890304565)
('epoch', 97, 'train_loss:', 3.9368866455554961, 'val_loss:', 0.98686845183372496)
('epoch', 98, 'train_loss:', 3.9224356520175934, 'val_loss:', 0.98401749730110166)
('epoch', 99, 'train_loss:', 3.9222270715236665, 'val_loss:', 0.98237905144691462)
('epoch', 100, 'train_loss:', 3.9145473897457124, 'val_loss:', 0.98216866135597225)
('epoch', 101, 'train_loss:', 3.9124960267543791, 'val_loss:', 0.98281830310821539)
('epoch', 102, 'train_loss:', 3.8911163544654848, 'val_loss:', 0.98137450218200684)
('epoch', 103, 'train_loss:', 3.8905854094028474, 'val_loss:', 0.9771307194232941)
('epoch', 104, 'train_loss:', 3.8832434737682342, 'val_loss:', 0.97634216904640203)
('epoch', 105, 'train_loss:', 3.8754908931255341, 'val_loss:', 0.97477953791618344)
('epoch', 106, 'train_loss:', 3.872294090986252, 'val_loss:', 0.97001867413520815)
('epoch', 107, 'train_loss:', 3.8575540792942049, 'val_loss:', 0.97199128031730653)
('epoch', 108, 'train_loss:', 3.857129806280136, 'val_loss:', 0.97173764944076535)
('epoch', 109, 'train_loss:', 3.851645600795746, 'val_loss:', 0.96517132997512822)
('epoch', 110, 'train_loss:', 3.8446850931644438, 'val_loss:', 0.96425801992416382)
('epoch', 111, 'train_loss:', 3.8457053673267363, 'val_loss:', 0.96358584046363827)
('epoch', 112, 'train_loss:', 3.8305209612846376, 'val_loss:', 0.96940016508102422)
('epoch', 113, 'train_loss:', 3.8207271850109099, 'val_loss:', 0.96295192122459417)
('epoch', 114, 'train_loss:', 3.8194926452636717, 'val_loss:', 0.95730569720268255)
('epoch', 115, 'train_loss:', 3.8133629143238066, 'val_loss:', 0.95509311676025388)
('epoch', 116, 'train_loss:', 3.8063444101810457, 'val_loss:', 0.95579295873641967)
('epoch', 117, 'train_loss:', 3.8120194590091705, 'val_loss:', 0.94977258205413817)
('epoch', 118, 'train_loss:', 3.7967719888687133, 'val_loss:', 0.95635188937187199)
('epoch', 119, 'train_loss:', 3.7923624587059019, 'val_loss:', 0.95461541652679438)
('epoch', 120, 'train_loss:', 3.7801473104953764, 'val_loss:', 0.95191080927848815)
('epoch', 121, 'train_loss:', 3.7738687837123872, 'val_loss:', 0.95182244777679448)
('epoch', 122, 'train_loss:', 3.7760855233669282, 'val_loss:', 0.95255237698554995)
('epoch', 123, 'train_loss:', 3.7658933889865875, 'val_loss:', 0.94727230548858643)
('epoch', 124, 'train_loss:', 3.7645913386344909, 'val_loss:', 0.94859242320060733)
('epoch', 125, 'train_loss:', 3.7623893702030182, 'val_loss:', 0.94626266598701481)
('epoch', 126, 'train_loss:', 3.7560953855514527, 'val_loss:', 0.9457021200656891)
('epoch', 127, 'train_loss:', 3.7498274993896485, 'val_loss:', 0.9431826961040497)
('epoch', 128, 'train_loss:', 3.7487625277042387, 'val_loss:', 0.94286609053611758)
('epoch', 129, 'train_loss:', 3.746227136850357, 'val_loss:', 0.93914155840873714)
('epoch', 130, 'train_loss:', 3.7385578525066374, 'val_loss:', 0.93951724171638484)
('epoch', 131, 'train_loss:', 3.7268921172618867, 'val_loss:', 0.939787734746933)
('epoch', 132, 'train_loss:', 3.7258082735538482, 'val_loss:', 0.93962699413299555)
('epoch', 133, 'train_loss:', 3.7217529189586638, 'val_loss:', 0.93685565471649168)
('epoch', 134, 'train_loss:', 3.713875902891159, 'val_loss:', 0.93700835943222049)
('epoch', 135, 'train_loss:', 3.7142152547836305, 'val_loss:', 0.93473890423774719)
('epoch', 136, 'train_loss:', 3.7118927085399629, 'val_loss:', 0.93487213373184208)
('epoch', 137, 'train_loss:', 3.7061195921897889, 'val_loss:', 0.93439641952514652)
('epoch', 138, 'train_loss:', 3.7061028468608854, 'val_loss:', 0.93334371209144595)
('epoch', 139, 'train_loss:', 3.7015593862533569, 'val_loss:', 0.92889693975448606)
('epoch', 140, 'train_loss:', 3.6845562696456908, 'val_loss:', 0.93248880743980411)
('epoch', 141, 'train_loss:', 3.6862104225158689, 'val_loss:', 0.92976407527923588)
('epoch', 142, 'train_loss:', 3.682196309566498, 'val_loss:', 0.92738045334815977)
('epoch', 143, 'train_loss:', 3.680117166042328, 'val_loss:', 0.92973699569702151)
('epoch', 144, 'train_loss:', 3.6744862318038942, 'val_loss:', 0.92598116397857666)
('epoch', 145, 'train_loss:', 3.6717937338352202, 'val_loss:', 0.92563323259353636)
('epoch', 146, 'train_loss:', 3.66550728559494, 'val_loss:', 0.92623759865760802)
('epoch', 147, 'train_loss:', 3.6623693943023681, 'val_loss:', 0.92178675532341003)
('epoch', 148, 'train_loss:', 3.6620160531997681, 'val_loss:', 0.92287678599357603)
('epoch', 149, 'train_loss:', 3.6555911552906037, 'val_loss:', 0.92448244571685789)
('epoch', 150, 'train_loss:', 3.6469325375556947, 'val_loss:', 0.92212802767753599)
('epoch', 151, 'train_loss:', 3.6483488750457762, 'val_loss:', 0.92412659883499149)
('epoch', 152, 'train_loss:', 3.6424379777908324, 'val_loss:', 0.92110301256179805)
('epoch', 153, 'train_loss:', 3.6335273635387422, 'val_loss:', 0.91803978323936464)
('epoch', 154, 'train_loss:', 3.644068682193756, 'val_loss:', 0.91957977890968323)
('epoch', 155, 'train_loss:', 3.6378273046016694, 'val_loss:', 0.92138103723525999)
('epoch', 156, 'train_loss:', 3.6311614453792571, 'val_loss:', 0.91394294023513789)
('epoch', 157, 'train_loss:', 3.6285730636119844, 'val_loss:', 0.9191540193557739)
('epoch', 158, 'train_loss:', 3.620819364786148, 'val_loss:', 0.91287747144699094)
('epoch', 159, 'train_loss:', 3.6220848548412321, 'val_loss:', 0.91327322959899904)
('epoch', 160, 'train_loss:', 3.6177294814586638, 'val_loss:', 0.91324311852455142)
('epoch', 161, 'train_loss:', 3.6106883239746095, 'val_loss:', 0.9104911231994629)
('epoch', 162, 'train_loss:', 3.6133207118511201, 'val_loss:', 0.91124963045120244)
('epoch', 163, 'train_loss:', 3.6121948456764219, 'val_loss:', 0.91071789383888246)
('epoch', 164, 'train_loss:', 3.6031267678737642, 'val_loss:', 0.91313657164573669)
('epoch', 165, 'train_loss:', 3.5994185030460359, 'val_loss:', 0.91087154388427738)
('epoch', 166, 'train_loss:', 3.5885854387283325, 'val_loss:', 0.91068199157714846)
('epoch', 167, 'train_loss:', 3.5904727852344513, 'val_loss:', 0.90753997445106505)
('epoch', 168, 'train_loss:', 3.5836122751235964, 'val_loss:', 0.90816639184951786)
('epoch', 169, 'train_loss:', 3.5851698923110962, 'val_loss:', 0.90951400756835943)
('epoch', 170, 'train_loss:', 3.5831644701957703, 'val_loss:', 0.91135450601577761)
('epoch', 171, 'train_loss:', 3.5795364665985105, 'val_loss:', 0.9089354336261749)
('epoch', 172, 'train_loss:', 3.5839066553115844, 'val_loss:', 0.90423599481582639)
('epoch', 173, 'train_loss:', 3.5730238568782808, 'val_loss:', 0.90757046461105351)
('epoch', 174, 'train_loss:', 3.5728627002239226, 'val_loss:', 0.90274310469627383)
('epoch', 175, 'train_loss:', 3.5676715338230132, 'val_loss:', 0.90362812399864201)
('epoch', 176, 'train_loss:', 3.5700167584419251, 'val_loss:', 0.90291064858436587)
('epoch', 177, 'train_loss:', 3.5629783797264101, 'val_loss:', 0.90453692436218258)
('epoch', 178, 'train_loss:', 3.5626003897190093, 'val_loss:', 0.90032839894294736)
('epoch', 179, 'train_loss:', 3.5604358541965486, 'val_loss:', 0.90368998527526856)
('epoch', 180, 'train_loss:', 3.5506357920169829, 'val_loss:', 0.90227731823921209)
('epoch', 181, 'train_loss:', 3.5525691258907317, 'val_loss:', 0.90141676664352421)
('epoch', 182, 'train_loss:', 3.5519158709049226, 'val_loss:', 0.90224298954010007)
('epoch', 183, 'train_loss:', 3.5480039083957671, 'val_loss:', 0.90020612835884095)
('epoch', 184, 'train_loss:', 3.5449254035949709, 'val_loss:', 0.89756788372993468)
('epoch', 185, 'train_loss:', 3.5389744162559511, 'val_loss:', 0.89649902462959286)
('epoch', 186, 'train_loss:', 3.5459278702735899, 'val_loss:', 0.89681397080421443)
('epoch', 187, 'train_loss:', 3.5293795406818389, 'val_loss:', 0.89574790000915527)
('epoch', 188, 'train_loss:', 3.5328338873386382, 'val_loss:', 0.89927835345268248)
('epoch', 189, 'train_loss:', 3.5284995472431184, 'val_loss:', 0.89746047258377071)
('epoch', 190, 'train_loss:', 3.531965512037277, 'val_loss:', 0.89506019473075871)
('epoch', 191, 'train_loss:', 3.5293099141120909, 'val_loss:', 0.89272336602211)
('epoch', 192, 'train_loss:', 3.5248923730850219, 'val_loss:', 0.89490866661071777)
('epoch', 193, 'train_loss:', 3.5224224400520323, 'val_loss:', 0.89576796412467952)
('epoch', 194, 'train_loss:', 3.5134336459636688, 'val_loss:', 0.89231053233146662)
('epoch', 195, 'train_loss:', 3.5165307307243348, 'val_loss:', 0.89698346495628356)
('epoch', 196, 'train_loss:', 3.5176650643348695, 'val_loss:', 0.89276350975036622)
('epoch', 197, 'train_loss:', 3.5097986018657683, 'val_loss:', 0.89062168240547179)
('epoch', 198, 'train_loss:', 3.5068121123313905, 'val_loss:', 0.89169117331504821)
('epoch', 199, 'train_loss:', 3.509166306257248, 'val_loss:', 0.89057962298393245)
('epoch', 200, 'train_loss:', 3.510461584329605, 'val_loss:', 0.89153061509132381)
('epoch', 201, 'train_loss:', 3.5043422651290892, 'val_loss:', 0.88881937861442561)
('epoch', 202, 'train_loss:', 3.5007518422603607, 'val_loss:', 0.89095326900482175)
('epoch', 203, 'train_loss:', 3.502299190759659, 'val_loss:', 0.88601559281349185)
('epoch', 204, 'train_loss:', 3.4954437649250032, 'val_loss:', 0.88653375864028927)
('epoch', 205, 'train_loss:', 3.4940240347385405, 'val_loss:', 0.88862426757812496)
('epoch', 206, 'train_loss:', 3.505478049516678, 'val_loss:', 0.88860717535018918)
('epoch', 207, 'train_loss:', 3.4903072595596312, 'val_loss:', 0.88815819621086123)
('epoch', 208, 'train_loss:', 3.4938813102245332, 'val_loss:', 0.88419745922088622)
('epoch', 209, 'train_loss:', 3.4856422996520995, 'val_loss:', 0.88476654410362243)
('epoch', 210, 'train_loss:', 3.4888189673423766, 'val_loss:', 0.8837551140785217)
('epoch', 211, 'train_loss:', 3.4834434998035433, 'val_loss:', 0.88503841996192933)
('epoch', 212, 'train_loss:', 3.4797956717014311, 'val_loss:', 0.88452084779739382)
('epoch', 213, 'train_loss:', 3.4788133108615877, 'val_loss:', 0.88304520726203917)
('epoch', 214, 'train_loss:', 3.4766242372989655, 'val_loss:', 0.88316412925720211)
('epoch', 215, 'train_loss:', 3.4776456284523012, 'val_loss:', 0.88419531822204589)
('epoch', 216, 'train_loss:', 3.4733772456645964, 'val_loss:', 0.88412474036216737)
('epoch', 217, 'train_loss:', 3.467604331970215, 'val_loss:', 0.88285300731658933)
('epoch', 218, 'train_loss:', 3.4618519389629365, 'val_loss:', 0.87965020179748532)
('epoch', 219, 'train_loss:', 3.4680131292343139, 'val_loss:', 0.88377029657363892)
('epoch', 220, 'train_loss:', 3.4643182194232942, 'val_loss:', 0.88301845788955691)
('epoch', 221, 'train_loss:', 3.4654174065589904, 'val_loss:', 0.88062483549118042)
('epoch', 222, 'train_loss:', 3.4591042721271514, 'val_loss:', 0.88382282733917239)
('epoch', 223, 'train_loss:', 3.4614118814468382, 'val_loss:', 0.87895091533660885)
('epoch', 224, 'train_loss:', 3.4580406355857849, 'val_loss:', 0.87573700666427612)
('epoch', 225, 'train_loss:', 3.4469411563873291, 'val_loss:', 0.88000976204872128)
('epoch', 226, 'train_loss:', 3.4501503562927245, 'val_loss:', 0.88147781014442439)
('epoch', 227, 'train_loss:', 3.4481568348407747, 'val_loss:', 0.87949214935302733)
('epoch', 228, 'train_loss:', 3.4452143120765686, 'val_loss:', 0.88260145545005797)
('epoch', 229, 'train_loss:', 3.4397411584854125, 'val_loss:', 0.88044505000114437)
('epoch', 230, 'train_loss:', 3.4425641965866087, 'val_loss:', 0.87817153692245487)
('epoch', 231, 'train_loss:', 3.4446403205394747, 'val_loss:', 0.87813151836395265)
('epoch', 232, 'train_loss:', 3.4453441572189329, 'val_loss:', 0.87821756362915038)
('epoch', 233, 'train_loss:', 3.4355906510353087, 'val_loss:', 0.8751914191246033)
('epoch', 234, 'train_loss:', 3.4327862071990967, 'val_loss:', 0.87704202651977536)
('epoch', 235, 'train_loss:', 3.4300262284278871, 'val_loss:', 0.87554165840148923)
('epoch', 236, 'train_loss:', 3.4311246621608733, 'val_loss:', 0.87482733011245728)
('epoch', 237, 'train_loss:', 3.4337761867046357, 'val_loss:', 0.87526291847228999)
('epoch', 238, 'train_loss:', 3.423162475824356, 'val_loss:', 0.87487621665000914)
('epoch', 239, 'train_loss:', 3.4306085908412935, 'val_loss:', 0.87425343394279476)
('epoch', 240, 'train_loss:', 3.4234367299079893, 'val_loss:', 0.87319335341453552)
('epoch', 241, 'train_loss:', 3.428032078742981, 'val_loss:', 0.87567886471748357)
('epoch', 242, 'train_loss:', 3.42628729224205, 'val_loss:', 0.87442420125007625)
('epoch', 243, 'train_loss:', 3.426255521774292, 'val_loss:', 0.87300166487693787)
('epoch', 244, 'train_loss:', 3.4223574721813201, 'val_loss:', 0.87245707035064701)
('epoch', 245, 'train_loss:', 3.414638055562973, 'val_loss:', 0.87315747737884519)
('epoch', 246, 'train_loss:', 3.4232753622531891, 'val_loss:', 0.8710710561275482)
('epoch', 247, 'train_loss:', 3.4125048315525053, 'val_loss:', 0.87816929578781133)
('epoch', 248, 'train_loss:', 3.4053661167621612, 'val_loss:', 0.87081459522247318)
('epoch', 249, 'train_loss:', 3.4166253614425659, 'val_loss:', 0.87024172306060787)
('epoch', 250, 'train_loss:', 3.4052425396442412, 'val_loss:', 0.87205764532089236)
('epoch', 251, 'train_loss:', 3.4076180684566499, 'val_loss:', 0.87120326519012448)
('epoch', 252, 'train_loss:', 3.4062994778156281, 'val_loss:', 0.87117451548576352)
('epoch', 253, 'train_loss:', 3.406286358833313, 'val_loss:', 0.86930935382843022)
('epoch', 254, 'train_loss:', 3.3999132168293, 'val_loss:', 0.86930982947349544)
('epoch', 255, 'train_loss:', 3.4023116338253021, 'val_loss:', 0.87235033631324765)
('epoch', 256, 'train_loss:', 3.3986170113086702, 'val_loss:', 0.86999462008476258)
('epoch', 257, 'train_loss:', 3.3942124557495119, 'val_loss:', 0.86940768957138059)
('epoch', 258, 'train_loss:', 3.3933616912364961, 'val_loss:', 0.86730788946151738)
('epoch', 259, 'train_loss:', 3.3900807070732117, 'val_loss:', 0.87012020587921146)
('epoch', 260, 'train_loss:', 3.3928522408008575, 'val_loss:', 0.86726402878761288)
('epoch', 261, 'train_loss:', 3.3905237460136415, 'val_loss:', 0.86793806433677678)
('epoch', 262, 'train_loss:', 3.3933934473991396, 'val_loss:', 0.86559071660041809)
('epoch', 263, 'train_loss:', 3.3929277801513673, 'val_loss:', 0.86821105003356935)
('epoch', 264, 'train_loss:', 3.3877013123035429, 'val_loss:', 0.86829752802848814)
('epoch', 265, 'train_loss:', 3.3885681664943696, 'val_loss:', 0.86623344063758845)
('epoch', 266, 'train_loss:', 3.3809056055545805, 'val_loss:', 0.86155358910560609)
('epoch', 267, 'train_loss:', 3.379541313648224, 'val_loss:', 0.86349710941314695)
('epoch', 268, 'train_loss:', 3.3808573913574218, 'val_loss:', 0.86723404645919799)
('epoch', 269, 'train_loss:', 3.3753120124340059, 'val_loss:', 0.86749638319015498)
('epoch', 270, 'train_loss:', 3.3781135797500612, 'val_loss:', 0.86346939682960511)
('epoch', 271, 'train_loss:', 3.3766626524925232, 'val_loss:', 0.86472836136817932)
('epoch', 272, 'train_loss:', 3.3700205087661743, 'val_loss:', 0.86401650309562683)
('epoch', 273, 'train_loss:', 3.372809545993805, 'val_loss:', 0.8627165830135346)
('epoch', 274, 'train_loss:', 3.3759263348579407, 'val_loss:', 0.86368150830268864)
('epoch', 275, 'train_loss:', 3.3703720033168794, 'val_loss:', 0.86514647245407106)
('epoch', 276, 'train_loss:', 3.3712260591983796, 'val_loss:', 0.86428490996360774)
('epoch', 277, 'train_loss:', 3.3676862001419066, 'val_loss:', 0.86199815392494206)
('epoch', 278, 'train_loss:', 3.3685844886302947, 'val_loss:', 0.86112085461616517)
('epoch', 279, 'train_loss:', 3.3655938255786895, 'val_loss:', 0.86539531946182247)
('epoch', 280, 'train_loss:', 3.362111282348633, 'val_loss:', 0.86519412755966185)
('epoch', 281, 'train_loss:', 3.3659014761447907, 'val_loss:', 0.86054015159606934)
('epoch', 282, 'train_loss:', 3.3585123133659365, 'val_loss:', 0.86255668759346005)
('epoch', 283, 'train_loss:', 3.3589584422111511, 'val_loss:', 0.86192589879035952)
('epoch', 284, 'train_loss:', 3.3626489794254302, 'val_loss:', 0.8624012732505798)
('epoch', 285, 'train_loss:', 3.3594726276397706, 'val_loss:', 0.86345784664154057)
('epoch', 286, 'train_loss:', 3.3623195910453796, 'val_loss:', 0.86028573393821717)
('epoch', 287, 'train_loss:', 3.350914855003357, 'val_loss:', 0.85844275116920477)
('epoch', 288, 'train_loss:', 3.3556327342987062, 'val_loss:', 0.85953281760215761)
('epoch', 289, 'train_loss:', 3.3512723863124849, 'val_loss:', 0.85891626715660097)
('epoch', 290, 'train_loss:', 3.3512168192863463, 'val_loss:', 0.86117717266082761)
('epoch', 291, 'train_loss:', 3.3501634454727172, 'val_loss:', 0.85754915237426754)
('epoch', 292, 'train_loss:', 3.3488946771621704, 'val_loss:', 0.86224979281425473)
('epoch', 293, 'train_loss:', 3.3494772565364839, 'val_loss:', 0.85846447587013242)
('epoch', 294, 'train_loss:', 3.3457625687122343, 'val_loss:', 0.85323737382888798)
('epoch', 295, 'train_loss:', 3.3469171917438505, 'val_loss:', 0.85830244064331052)
('epoch', 296, 'train_loss:', 3.3425497448444368, 'val_loss:', 0.85741959810256962)
('epoch', 297, 'train_loss:', 3.3299561798572541, 'val_loss:', 0.85846754908561707)
('epoch', 298, 'train_loss:', 3.3435510361194609, 'val_loss:', 0.85864118933677669)
('epoch', 299, 'train_loss:', 3.3324687922000886, 'val_loss:', 0.85663738608360296)
('epoch', 300, 'train_loss:', 3.3387876701354982, 'val_loss:', 0.85694149494171146)
('epoch', 301, 'train_loss:', 3.3345435380935671, 'val_loss:', 0.85790082097053533)
('epoch', 302, 'train_loss:', 3.3359739851951598, 'val_loss:', 0.8527948927879333)
('epoch', 303, 'train_loss:', 3.3324231374263764, 'val_loss:', 0.85502469539642334)
('epoch', 304, 'train_loss:', 3.3306645691394805, 'val_loss:', 0.8602636623382568)
('epoch', 305, 'train_loss:', 3.3277566778659819, 'val_loss:', 0.85555325746536259)
('epoch', 306, 'train_loss:', 3.3306592392921446, 'val_loss:', 0.85506744265556334)
('epoch', 307, 'train_loss:', 3.3208450794219972, 'val_loss:', 0.85928533077239988)
('epoch', 308, 'train_loss:', 3.327178372144699, 'val_loss:', 0.85663013696670531)
('epoch', 309, 'train_loss:', 3.3180237782001494, 'val_loss:', 0.85247193098068241)
('epoch', 310, 'train_loss:', 3.322165129184723, 'val_loss:', 0.85587015748023987)
('epoch', 311, 'train_loss:', 3.3231362807750702, 'val_loss:', 0.85397415757179262)
('epoch', 312, 'train_loss:', 3.3177711236476899, 'val_loss:', 0.85380717158317565)
('epoch', 313, 'train_loss:', 3.3171096181869508, 'val_loss:', 0.85439535140991207)
('epoch', 314, 'train_loss:', 3.3191882252693174, 'val_loss:', 0.85632790923118596)
('epoch', 315, 'train_loss:', 3.3187241375446321, 'val_loss:', 0.85437511563301083)
('epoch', 316, 'train_loss:', 3.3144751548767091, 'val_loss:', 0.8521358323097229)
('epoch', 317, 'train_loss:', 3.3135445666313172, 'val_loss:', 0.85500342130661011)
('epoch', 318, 'train_loss:', 3.312886062860489, 'val_loss:', 0.85404346346855164)
('epoch', 319, 'train_loss:', 3.3182808864116669, 'val_loss:', 0.84984658598899843)
('epoch', 320, 'train_loss:', 3.3208521401882169, 'val_loss:', 0.85321114301681522)
('epoch', 321, 'train_loss:', 3.3179449999332427, 'val_loss:', 0.852539392709732)
('epoch', 322, 'train_loss:', 3.3008948516845704, 'val_loss:', 0.85562308430671696)
('epoch', 323, 'train_loss:', 3.3058288931846618, 'val_loss:', 0.85199727416038518)
('epoch', 324, 'train_loss:', 3.3059932827949523, 'val_loss:', 0.85171509861946104)
('epoch', 325, 'train_loss:', 3.3102203071117402, 'val_loss:', 0.851288548707962)
('epoch', 326, 'train_loss:', 3.3111056852340699, 'val_loss:', 0.85160942435264586)
('epoch', 327, 'train_loss:', 3.3043191707134247, 'val_loss:', 0.85365389347076415)
('epoch', 328, 'train_loss:', 3.3028971934318543, 'val_loss:', 0.85370518326759337)
('epoch', 329, 'train_loss:', 3.3042985713481903, 'val_loss:', 0.85067999958992002)
('epoch', 330, 'train_loss:', 3.3026564514636991, 'val_loss:', 0.85268669366836547)
('epoch', 331, 'train_loss:', 3.3000759863853455, 'val_loss:', 0.84956666231155398)
('epoch', 332, 'train_loss:', 3.3020320320129395, 'val_loss:', 0.85102614521980291)
('epoch', 333, 'train_loss:', 3.2950794887542725, 'val_loss:', 0.85081593155860902)
('epoch', 334, 'train_loss:', 3.2929911565780641, 'val_loss:', 0.8512405800819397)
('epoch', 335, 'train_loss:', 3.2990492343902589, 'val_loss:', 0.84970018625259403)
('epoch', 336, 'train_loss:', 3.2938106071949007, 'val_loss:', 0.8478833913803101)
('epoch', 337, 'train_loss:', 3.2906191766262056, 'val_loss:', 0.84880818486213683)
('epoch', 338, 'train_loss:', 3.2923283028602599, 'val_loss:', 0.84751221895217899)
('epoch', 339, 'train_loss:', 3.2898607516288756, 'val_loss:', 0.8509893560409546)
('epoch', 340, 'train_loss:', 3.2813520467281343, 'val_loss:', 0.85262716293334961)
('epoch', 341, 'train_loss:', 3.2872266793251037, 'val_loss:', 0.84686175584793089)
('epoch', 342, 'train_loss:', 3.2817914545536042, 'val_loss:', 0.84735795974731443)
('epoch', 343, 'train_loss:', 3.2911508977413177, 'val_loss:', 0.85148056983947751)
('epoch', 344, 'train_loss:', 3.283272649049759, 'val_loss:', 0.85036589264869689)
('epoch', 345, 'train_loss:', 3.2828940403461457, 'val_loss:', 0.85014250397682189)
('epoch', 346, 'train_loss:', 3.2803165102005005, 'val_loss:', 0.84781699895858764)
('epoch', 347, 'train_loss:', 3.2778448009490968, 'val_loss:', 0.8488779699802399)
('epoch', 348, 'train_loss:', 3.2828460836410525, 'val_loss:', 0.84800756692886348)
('epoch', 349, 'train_loss:', 3.2876299846172334, 'val_loss:', 0.85098654508590699)
('epoch', 350, 'train_loss:', 3.2807618534564971, 'val_loss:', 0.84734255552291871)
('epoch', 351, 'train_loss:', 3.2699815189838408, 'val_loss:', 0.84874026656150814)
('epoch', 352, 'train_loss:', 3.2754321920871736, 'val_loss:', 0.84198976039886475)
('epoch', 353, 'train_loss:', 3.277065851688385, 'val_loss:', 0.85072949767112727)
('epoch', 354, 'train_loss:', 3.2729505586624144, 'val_loss:', 0.84971934199333188)
('epoch', 355, 'train_loss:', 3.2765242552757261, 'val_loss:', 0.84833183526992795)
('epoch', 356, 'train_loss:', 3.2775492537021638, 'val_loss:', 0.84943992853164674)
('epoch', 357, 'train_loss:', 3.2689764630794524, 'val_loss:', 0.84633854866027836)
('epoch', 358, 'train_loss:', 3.269215887784958, 'val_loss:', 0.84650118231773375)
('epoch', 359, 'train_loss:', 3.2714197862148287, 'val_loss:', 0.84598884820938114)
('epoch', 360, 'train_loss:', 3.2699838030338286, 'val_loss:', 0.84581434011459355)
('epoch', 361, 'train_loss:', 3.264918564558029, 'val_loss:', 0.84633950114250178)
('epoch', 362, 'train_loss:', 3.2644055938720702, 'val_loss:', 0.84540630340576173)
('epoch', 363, 'train_loss:', 3.2683975219726564, 'val_loss:', 0.84400962233543397)
('epoch', 364, 'train_loss:', 3.2625700783729554, 'val_loss:', 0.84327034711837767)
('epoch', 365, 'train_loss:', 3.2641190934181212, 'val_loss:', 0.84417142510414123)
('epoch', 366, 'train_loss:', 3.2555508363246917, 'val_loss:', 0.84294163942337041)
('epoch', 367, 'train_loss:', 3.2651329088211059, 'val_loss:', 0.84628986835479736)
('epoch', 368, 'train_loss:', 3.2621373677253724, 'val_loss:', 0.84448570013046265)
('epoch', 369, 'train_loss:', 3.2608444464206694, 'val_loss:', 0.84661216616630552)
('epoch', 370, 'train_loss:', 3.2651651334762573, 'val_loss:', 0.84427190661430362)
('epoch', 371, 'train_loss:', 3.2629528176784515, 'val_loss:', 0.84317095279693599)
('epoch', 372, 'train_loss:', 3.257411003112793, 'val_loss:', 0.84103823781013487)
('epoch', 373, 'train_loss:', 3.2568921637535095, 'val_loss:', 0.84258179664611821)
('epoch', 374, 'train_loss:', 3.252927267551422, 'val_loss:', 0.84573825240135192)
('epoch', 375, 'train_loss:', 3.2556718170642851, 'val_loss:', 0.84254872918128965)
('epoch', 376, 'train_loss:', 3.2531800913810729, 'val_loss:', 0.84134099125862116)
('epoch', 377, 'train_loss:', 3.2502854812145232, 'val_loss:', 0.84242113113403316)
('epoch', 378, 'train_loss:', 3.2539167714118959, 'val_loss:', 0.84177590966224669)
('epoch', 379, 'train_loss:', 3.2506708061695098, 'val_loss:', 0.84428493261337278)
('epoch', 380, 'train_loss:', 3.2475875973701478, 'val_loss:', 0.84243416428565976)
('epoch', 381, 'train_loss:', 3.2482083415985108, 'val_loss:', 0.84439054489135745)
('epoch', 382, 'train_loss:', 3.2498283839225768, 'val_loss:', 0.84194518804550167)
('epoch', 383, 'train_loss:', 3.2444973027706148, 'val_loss:', 0.84199179410934444)
('epoch', 384, 'train_loss:', 3.2409694743156434, 'val_loss:', 0.83988038301467893)
('epoch', 385, 'train_loss:', 3.2497297883033753, 'val_loss:', 0.84117605805397033)
('epoch', 386, 'train_loss:', 3.2483844578266146, 'val_loss:', 0.83835870265960688)
('epoch', 387, 'train_loss:', 3.242310427427292, 'val_loss:', 0.84293171882629392)
('epoch', 388, 'train_loss:', 3.236200610399246, 'val_loss:', 0.84307595968246463)
('epoch', 389, 'train_loss:', 3.2428069937229158, 'val_loss:', 0.84534628748893736)
('epoch', 390, 'train_loss:', 3.2375337576866148, 'val_loss:', 0.84042454004287714)
('epoch', 391, 'train_loss:', 3.2388020515441895, 'val_loss:', 0.83754941463470456)
('epoch', 392, 'train_loss:', 3.2394431376457216, 'val_loss:', 0.84080857396125797)
('epoch', 393, 'train_loss:', 3.2334412169456481, 'val_loss:', 0.8399088227748871)
('epoch', 394, 'train_loss:', 3.2393001019954681, 'val_loss:', 0.84022771120071416)
('epoch', 395, 'train_loss:', 3.2381871056556704, 'val_loss:', 0.83974493265151973)
('epoch', 396, 'train_loss:', 3.2359715759754182, 'val_loss:', 0.84341094136238093)
('epoch', 397, 'train_loss:', 3.2353898870944975, 'val_loss:', 0.84106494545936583)
('epoch', 398, 'train_loss:', 3.234031014442444, 'val_loss:', 0.83684551239013671)
('epoch', 399, 'train_loss:', 3.231887094974518, 'val_loss:', 0.83679732441902166)
('epoch', 400, 'train_loss:', 3.2304321551322936, 'val_loss:', 0.84159591436386105)
('epoch', 401, 'train_loss:', 3.2349544084072113, 'val_loss:', 0.83721823692321773)
('epoch', 402, 'train_loss:', 3.2295405972003937, 'val_loss:', 0.84049429059028624)
('epoch', 403, 'train_loss:', 3.2321573626995086, 'val_loss:', 0.83964353919029233)
('epoch', 404, 'train_loss:', 3.232700707912445, 'val_loss:', 0.8383914244174957)
('epoch', 405, 'train_loss:', 3.2253999245166778, 'val_loss:', 0.83889117956161496)
('epoch', 406, 'train_loss:', 3.2265421724319459, 'val_loss:', 0.83787618160247801)
('epoch', 407, 'train_loss:', 3.2226745331287385, 'val_loss:', 0.83763170480728144)
('epoch', 408, 'train_loss:', 3.2235943865776062, 'val_loss:', 0.84105680346488954)
('epoch', 409, 'train_loss:', 3.2276954448223112, 'val_loss:', 0.8374208450317383)
('epoch', 410, 'train_loss:', 3.2159985756874083, 'val_loss:', 0.83870446801185605)
('epoch', 411, 'train_loss:', 3.219885678291321, 'val_loss:', 0.83892627477645876)
('epoch', 412, 'train_loss:', 3.2233113670349121, 'val_loss:', 0.84043311238288876)
('epoch', 413, 'train_loss:', 3.2094543087482452, 'val_loss:', 0.83998238325119023)
('epoch', 414, 'train_loss:', 3.218198536634445, 'val_loss:', 0.83806897521018986)
('epoch', 415, 'train_loss:', 3.2247824895381929, 'val_loss:', 0.83548337817192075)
('epoch', 416, 'train_loss:', 3.2190181005001066, 'val_loss:', 0.83609719753265377)
('epoch', 417, 'train_loss:', 3.2201481580734255, 'val_loss:', 0.83638089537620541)
('epoch', 418, 'train_loss:', 3.2179940807819367, 'val_loss:', 0.83552559256553649)
('epoch', 419, 'train_loss:', 3.2170499551296232, 'val_loss:', 0.83919378042221071)
('epoch', 420, 'train_loss:', 3.2126555037498474, 'val_loss:', 0.83551426410675045)
('epoch', 421, 'train_loss:', 3.2120006334781648, 'val_loss:', 0.8375285685062408)
('epoch', 422, 'train_loss:', 3.2114413738250733, 'val_loss:', 0.83698407530784602)
('epoch', 423, 'train_loss:', 3.2155968940258024, 'val_loss:', 0.83902268052101137)
('epoch', 424, 'train_loss:', 3.2107309019565582, 'val_loss:', 0.83732158303260806)
('epoch', 425, 'train_loss:', 3.2064364182949068, 'val_loss:', 0.83478406071662903)
('epoch', 426, 'train_loss:', 3.2106033265590668, 'val_loss:', 0.83253473401069644)
('epoch', 427, 'train_loss:', 3.2047319269180297, 'val_loss:', 0.83544147253036494)
('epoch', 428, 'train_loss:', 3.2015065753459933, 'val_loss:', 0.83806660771369934)
('epoch', 429, 'train_loss:', 3.2028753221035005, 'val_loss:', 0.83712466239929195)
('epoch', 430, 'train_loss:', 3.2071871495246889, 'val_loss:', 0.83762806534767154)
('epoch', 431, 'train_loss:', 3.2049976122379302, 'val_loss:', 0.83718079090118414)
('epoch', 432, 'train_loss:', 3.2012983679771425, 'val_loss:', 0.83638371229171748)
('epoch', 433, 'train_loss:', 3.2066890454292296, 'val_loss:', 0.8418484461307526)
('epoch', 434, 'train_loss:', 3.2036394262313843, 'val_loss:', 0.83466170430183406)
('epoch', 435, 'train_loss:', 3.2031618738174439, 'val_loss:', 0.83704920649528503)
('epoch', 436, 'train_loss:', 3.199826134443283, 'val_loss:', 0.83564517855644227)
('epoch', 437, 'train_loss:', 3.1986398303508761, 'val_loss:', 0.83548900961875916)
('epoch', 438, 'train_loss:', 3.2002041935920715, 'val_loss:', 0.83371931791305542)
('epoch', 439, 'train_loss:', 3.1949161171913145, 'val_loss:', 0.83616324067115788)
('epoch', 440, 'train_loss:', 3.1988062703609468, 'val_loss:', 0.83738555908203127)
('epoch', 441, 'train_loss:', 3.1991609036922455, 'val_loss:', 0.8350384867191315)
('epoch', 442, 'train_loss:', 3.2011483311653137, 'val_loss:', 0.83741388201713562)
('epoch', 443, 'train_loss:', 3.1960546445846556, 'val_loss:', 0.83668556928634641)
('epoch', 444, 'train_loss:', 3.1924212121963502, 'val_loss:', 0.83315528750419621)
('epoch', 445, 'train_loss:', 3.1901601088047027, 'val_loss:', 0.8313187396526337)
('epoch', 446, 'train_loss:', 3.1941545438766479, 'val_loss:', 0.8342581379413605)
('epoch', 447, 'train_loss:', 3.1977127277851105, 'val_loss:', 0.83605013728141786)
('epoch', 448, 'train_loss:', 3.1903617465496064, 'val_loss:', 0.83427489519119258)
('epoch', 449, 'train_loss:', 3.1875306069850922, 'val_loss:', 0.83325389266014094)
('epoch', 450, 'train_loss:', 3.1900673019886017, 'val_loss:', 0.83361183285713192)
('epoch', 451, 'train_loss:', 3.1906673920154573, 'val_loss:', 0.83269492864608763)
('epoch', 452, 'train_loss:', 3.1879488468170165, 'val_loss:', 0.83673531651496891)
('epoch', 453, 'train_loss:', 3.1857216811180114, 'val_loss:', 0.83416996836662294)
('epoch', 454, 'train_loss:', 3.1842970287799837, 'val_loss:', 0.83467988729476927)
('epoch', 455, 'train_loss:', 3.1859281957149506, 'val_loss:', 0.83055709838867187)
('epoch', 456, 'train_loss:', 3.1883653020858764, 'val_loss:', 0.83318770766258243)
('epoch', 457, 'train_loss:', 3.1845408725738524, 'val_loss:', 0.83337522506713868)
('epoch', 458, 'train_loss:', 3.1796672749519348, 'val_loss:', 0.83259930014610295)
('epoch', 459, 'train_loss:', 3.1788309955596925, 'val_loss:', 0.83036791682243349)
('epoch', 460, 'train_loss:', 3.184924716949463, 'val_loss:', 0.83296490430831904)
('epoch', 461, 'train_loss:', 3.1784836030006409, 'val_loss:', 0.83349402189254762)
('epoch', 462, 'train_loss:', 3.1785909557342529, 'val_loss:', 0.83331336379051213)
('epoch', 463, 'train_loss:', 3.1794458281993867, 'val_loss:', 0.83290190577507017)
('epoch', 464, 'train_loss:', 3.177118068933487, 'val_loss:', 0.83007025957107539)
('epoch', 465, 'train_loss:', 3.1797901546955107, 'val_loss:', 0.83223576903343199)
('epoch', 466, 'train_loss:', 3.1779174673557282, 'val_loss:', 0.83035161375999456)
('epoch', 467, 'train_loss:', 3.1757878994941713, 'val_loss:', 0.8328359198570251)
('epoch', 468, 'train_loss:', 3.1729755663871764, 'val_loss:', 0.82988948345184321)
('epoch', 469, 'train_loss:', 3.1710889303684233, 'val_loss:', 0.83122765064239501)
('epoch', 470, 'train_loss:', 3.1749415254592894, 'val_loss:', 0.83101316690444949)
('epoch', 471, 'train_loss:', 3.1764789092540742, 'val_loss:', 0.83044030547142034)
('epoch', 472, 'train_loss:', 3.1716876792907716, 'val_loss:', 0.83081284046173098)
('epoch', 473, 'train_loss:', 3.1695572113990784, 'val_loss:', 0.83110733628273015)
('epoch', 474, 'train_loss:', 3.1732426083087919, 'val_loss:', 0.82597891807556156)
('epoch', 475, 'train_loss:', 3.1720851576328277, 'val_loss:', 0.83573716402053833)
('epoch', 476, 'train_loss:', 3.1698673391342163, 'val_loss:', 0.8303354954719544)
('epoch', 477, 'train_loss:', 3.1694550585746764, 'val_loss:', 0.82764667868614195)
('epoch', 478, 'train_loss:', 3.1685383057594301, 'val_loss:', 0.82918657064437862)
('epoch', 479, 'train_loss:', 3.1730864119529723, 'val_loss:', 0.83046222567558292)
('epoch', 480, 'train_loss:', 3.1690504705905913, 'val_loss:', 0.83087896347045898)
('epoch', 481, 'train_loss:', 3.1660704410076139, 'val_loss:', 0.83442551970481871)
('epoch', 482, 'train_loss:', 3.1673740243911741, 'val_loss:', 0.83136232972145085)
('epoch', 483, 'train_loss:', 3.165718752145767, 'val_loss:', 0.82860531330108644)
('epoch', 484, 'train_loss:', 3.1644366645812987, 'val_loss:', 0.83009085059165955)
('epoch', 485, 'train_loss:', 3.1616744577884672, 'val_loss:', 0.83179153203964229)
('epoch', 486, 'train_loss:', 3.1644828653335573, 'val_loss:', 0.83293704867362972)
('epoch', 487, 'train_loss:', 3.1615801966190338, 'val_loss:', 0.82988805890083317)
('epoch', 488, 'train_loss:', 3.1614466059207915, 'val_loss:', 0.83250674009323122)
('epoch', 489, 'train_loss:', 3.1572860300540926, 'val_loss:', 0.83012012958526615)
('epoch', 490, 'train_loss:', 3.1625004935264589, 'val_loss:', 0.8319992887973785)
('epoch', 491, 'train_loss:', 3.1624254453182221, 'val_loss:', 0.82872075676918033)
('epoch', 492, 'train_loss:', 3.1586199283599852, 'val_loss:', 0.827902569770813)
('epoch', 493, 'train_loss:', 3.1561880302429199, 'val_loss:', 0.83091656923294066)
('epoch', 494, 'train_loss:', 3.1524771034717558, 'val_loss:', 0.83007361412048342)
('epoch', 495, 'train_loss:', 3.1552839851379395, 'val_loss:', 0.82725669741630559)
('epoch', 496, 'train_loss:', 3.1550275647640227, 'val_loss:', 0.8297022366523743)
('epoch', 497, 'train_loss:', 3.1545124328136445, 'val_loss:', 0.82852352261543272)
('epoch', 498, 'train_loss:', 3.1502680683135988, 'val_loss:', 0.82882405281066895)
('epoch', 499, 'train_loss:', 3.150167940855026, 'val_loss:', 0.82867048740386962)

