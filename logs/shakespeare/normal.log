(keras)skoppula@sls-sm-8:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=0 python lstm_all_variants.py -t -d shakespeare -v normal
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'shakespeare', 'train': True, 'variant': 'normal', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (29743, 30), (29743, 30), (7436, 30), (7436, 30))
('num steps in trn and val epochs', 116, 29)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2280 get requests, put_count=2269 evicted_count=1000 eviction_rate=0.440723 and unsatisfied allocation rate=0.487281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3416 get requests, put_count=3395 evicted_count=1000 eviction_rate=0.294551 and unsatisfied allocation rate=0.305621
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
('epoch', 0, 'train_loss:', 4.3039435648918154, 'val_loss:', 0.96659195899963379)
('epoch', 1, 'train_loss:', 3.7508165884017943, 'val_loss:', 0.91136424779891967)
('epoch', 2, 'train_loss:', 3.5522956299781798, 'val_loss:', 0.8646565198898315)
('epoch', 3, 'train_loss:', 3.3623592448234558, 'val_loss:', 0.81682407140731816)
('epoch', 4, 'train_loss:', 3.1922185564041139, 'val_loss:', 0.78298949241638183)
('epoch', 5, 'train_loss:', 3.080996639728546, 'val_loss:', 0.76306086540222173)
('epoch', 6, 'train_loss:', 3.013530902862549, 'val_loss:', 0.7459148240089416)
('epoch', 7, 'train_loss:', 2.9536565780639648, 'val_loss:', 0.73371165275573735)
('epoch', 8, 'train_loss:', 2.904707067012787, 'val_loss:', 0.7196969866752625)
('epoch', 9, 'train_loss:', 2.8624615812301637, 'val_loss:', 0.71122310161590574)
('epoch', 10, 'train_loss:', 2.827447772026062, 'val_loss:', 0.70015054941177368)
('epoch', 11, 'train_loss:', 2.7885969734191893, 'val_loss:', 0.69798150062561037)
('epoch', 12, 'train_loss:', 2.7564124202728273, 'val_loss:', 0.68785186290740963)
('epoch', 13, 'train_loss:', 2.7281634616851806, 'val_loss:', 0.67865519046783451)
('epoch', 14, 'train_loss:', 2.6911331892013548, 'val_loss:', 0.67626165151596074)
('epoch', 15, 'train_loss:', 2.6678962516784668, 'val_loss:', 0.664988796710968)
('epoch', 16, 'train_loss:', 2.640997278690338, 'val_loss:', 0.66772536277771)
('epoch', 17, 'train_loss:', 2.6182757568359376, 'val_loss:', 0.65006198883056643)
('epoch', 18, 'train_loss:', 2.5948596739768983, 'val_loss:', 0.64440821409225468)
('epoch', 19, 'train_loss:', 2.5712817287445069, 'val_loss:', 0.64470352888107296)
('epoch', 20, 'train_loss:', 2.5462824392318724, 'val_loss:', 0.63521562576293944)
('epoch', 21, 'train_loss:', 2.5277742218971251, 'val_loss:', 0.6321993660926819)
('epoch', 22, 'train_loss:', 2.5047528314590455, 'val_loss:', 0.62746953010559081)
('epoch', 23, 'train_loss:', 2.4851663899421692, 'val_loss:', 0.61909502506256109)
('epoch', 24, 'train_loss:', 2.4664770555496216, 'val_loss:', 0.61490557909011845)
('epoch', 25, 'train_loss:', 2.4496358895301817, 'val_loss:', 0.61141540527343752)
('epoch', 26, 'train_loss:', 2.4231316590309144, 'val_loss:', 0.60325307607650758)
('epoch', 27, 'train_loss:', 2.4096387696266173, 'val_loss:', 0.60012573242187495)
('epoch', 28, 'train_loss:', 2.3903863263130187, 'val_loss:', 0.59923418045043941)
('epoch', 29, 'train_loss:', 2.3711414420604706, 'val_loss:', 0.59185339689254757)
('epoch', 30, 'train_loss:', 2.3554161596298218, 'val_loss:', 0.58813288092613225)
('epoch', 31, 'train_loss:', 2.3385071492195131, 'val_loss:', 0.58408800482749934)
('epoch', 32, 'train_loss:', 2.3234111428260804, 'val_loss:', 0.58337834000587463)
('epoch', 33, 'train_loss:', 2.3050241780281069, 'val_loss:', 0.57592415690422061)
('epoch', 34, 'train_loss:', 2.2928361308574678, 'val_loss:', 0.57295632243156436)
('epoch', 35, 'train_loss:', 2.2725000953674317, 'val_loss:', 0.56903960466384884)
('epoch', 36, 'train_loss:', 2.2603667616844176, 'val_loss:', 0.56761024713516239)
('epoch', 37, 'train_loss:', 2.2457003271579743, 'val_loss:', 0.56047256350517272)
('epoch', 38, 'train_loss:', 2.2331775319576264, 'val_loss:', 0.55916221261024479)
('epoch', 39, 'train_loss:', 2.2153766357898714, 'val_loss:', 0.55693125367164609)
('epoch', 40, 'train_loss:', 2.1998477017879488, 'val_loss:', 0.55006393551826482)
('epoch', 41, 'train_loss:', 2.1857233250141146, 'val_loss:', 0.55177390575408936)
('epoch', 42, 'train_loss:', 2.1763554084300996, 'val_loss:', 0.54352828383445739)
('epoch', 43, 'train_loss:', 2.1608657836914062, 'val_loss:', 0.54187170743942259)
('epoch', 44, 'train_loss:', 2.1501013231277466, 'val_loss:', 0.54118102431297299)
('epoch', 45, 'train_loss:', 2.1395617246627809, 'val_loss:', 0.53492849469184878)
('epoch', 46, 'train_loss:', 2.1257232296466828, 'val_loss:', 0.53178364396095279)
('epoch', 47, 'train_loss:', 2.1154469907283784, 'val_loss:', 0.53133515596389769)
('epoch', 48, 'train_loss:', 2.1032156884670257, 'val_loss:', 0.52751349091529842)
('epoch', 49, 'train_loss:', 2.091952600479126, 'val_loss:', 0.52705117821693426)
('epoch', 50, 'train_loss:', 2.0833219897747042, 'val_loss:', 0.52314970374107361)
('epoch', 51, 'train_loss:', 2.0707255256175996, 'val_loss:', 0.52436186671257023)
('epoch', 52, 'train_loss:', 2.0623345160484314, 'val_loss:', 0.52126982212066653)
('epoch', 53, 'train_loss:', 2.0536239385604858, 'val_loss:', 0.51746828198432926)
('epoch', 54, 'train_loss:', 2.0402546298503874, 'val_loss:', 0.51375792264938358)
('epoch', 55, 'train_loss:', 2.0295602130889892, 'val_loss:', 0.51297196030616765)
('epoch', 56, 'train_loss:', 2.0203368210792543, 'val_loss:', 0.50871255993843079)
('epoch', 57, 'train_loss:', 2.0130663883686064, 'val_loss:', 0.51021639108657835)
('epoch', 58, 'train_loss:', 2.0082459580898284, 'val_loss:', 0.50665609240531917)
('epoch', 59, 'train_loss:', 1.993691337108612, 'val_loss:', 0.5031356370449066)
('epoch', 60, 'train_loss:', 1.9849345552921296, 'val_loss:', 0.49951241493225096)
('epoch', 61, 'train_loss:', 1.9822666573524474, 'val_loss:', 0.49918927073478697)
('epoch', 62, 'train_loss:', 1.9740623724460602, 'val_loss:', 0.49960897088050843)
('epoch', 63, 'train_loss:', 1.9632257640361785, 'val_loss:', 0.50001059055328367)
('epoch', 64, 'train_loss:', 1.9482663702964782, 'val_loss:', 0.49743144512176513)
('epoch', 65, 'train_loss:', 1.9452288401126863, 'val_loss:', 0.49402535915374757)
('epoch', 66, 'train_loss:', 1.941912453174591, 'val_loss:', 0.49257202744483947)
('epoch', 67, 'train_loss:', 1.9294807565212251, 'val_loss:', 0.4871817636489868)
('epoch', 68, 'train_loss:', 1.9217212700843811, 'val_loss:', 0.48849544763565061)
('epoch', 69, 'train_loss:', 1.9196394467353821, 'val_loss:', 0.48742602348327635)
('epoch', 70, 'train_loss:', 1.9089154160022737, 'val_loss:', 0.48503860354423523)
('epoch', 71, 'train_loss:', 1.9059278023242952, 'val_loss:', 0.48602020502090454)
('epoch', 72, 'train_loss:', 1.896217029094696, 'val_loss:', 0.48244151711463928)
('epoch', 73, 'train_loss:', 1.8925598454475403, 'val_loss:', 0.48139806509017946)
('epoch', 74, 'train_loss:', 1.8886770415306091, 'val_loss:', 0.47848660826683043)
('epoch', 75, 'train_loss:', 1.878711484670639, 'val_loss:', 0.47950794458389284)
('epoch', 76, 'train_loss:', 1.8715995264053344, 'val_loss:', 0.47702967166900634)
('epoch', 77, 'train_loss:', 1.8682269442081452, 'val_loss:', 0.47852299571037293)
('epoch', 78, 'train_loss:', 1.8672129333019256, 'val_loss:', 0.47709266066551209)
('epoch', 79, 'train_loss:', 1.8573849749565126, 'val_loss:', 0.47242367863655088)
('epoch', 80, 'train_loss:', 1.848539456129074, 'val_loss:', 0.47294835925102235)
('epoch', 81, 'train_loss:', 1.8489358091354371, 'val_loss:', 0.46962183117866518)
('epoch', 82, 'train_loss:', 1.8405385196208954, 'val_loss:', 0.4715892434120178)
('epoch', 83, 'train_loss:', 1.839148803949356, 'val_loss:', 0.46818382382392881)
('epoch', 84, 'train_loss:', 1.8272609066963197, 'val_loss:', 0.46860292911529539)
('epoch', 85, 'train_loss:', 1.8274767243862151, 'val_loss:', 0.46607921600341795)
('epoch', 86, 'train_loss:', 1.824683655500412, 'val_loss:', 0.46819692373275756)
('epoch', 87, 'train_loss:', 1.8189458560943603, 'val_loss:', 0.4647074520587921)
('epoch', 88, 'train_loss:', 1.8150186157226562, 'val_loss:', 0.46347266793251035)
('epoch', 89, 'train_loss:', 1.8081026244163514, 'val_loss:', 0.46427179694175719)
('epoch', 90, 'train_loss:', 1.8067266416549683, 'val_loss:', 0.46443571686744689)
('epoch', 91, 'train_loss:', 1.7972492420673369, 'val_loss:', 0.46205157399177549)
('epoch', 92, 'train_loss:', 1.800158168077469, 'val_loss:', 0.46226303458213808)
('epoch', 93, 'train_loss:', 1.7885925662517548, 'val_loss:', 0.45925620555877683)
('epoch', 94, 'train_loss:', 1.7869385933876039, 'val_loss:', 0.46238745331764219)
('epoch', 95, 'train_loss:', 1.7816018688678741, 'val_loss:', 0.45688035964965823)
('epoch', 96, 'train_loss:', 1.780863344669342, 'val_loss:', 0.45961110830307006)
('epoch', 97, 'train_loss:', 1.7779139673709869, 'val_loss:', 0.46006989717483521)
('epoch', 98, 'train_loss:', 1.7713003528118134, 'val_loss:', 0.45878408432006834)
('epoch', 99, 'train_loss:', 1.7673868680000304, 'val_loss:', 0.45948023796081544)
('epoch', 100, 'train_loss:', 1.7657281076908111, 'val_loss:', 0.45498783946037291)
('epoch', 101, 'train_loss:', 1.7597318887710571, 'val_loss:', 0.45682489991188047)
('epoch', 102, 'train_loss:', 1.7600812339782714, 'val_loss:', 0.45244749426841735)
('epoch', 103, 'train_loss:', 1.7596698558330537, 'val_loss:', 0.45075469970703125)
('epoch', 104, 'train_loss:', 1.7489909613132477, 'val_loss:', 0.45402613162994387)
('epoch', 105, 'train_loss:', 1.7488048303127288, 'val_loss:', 0.45537680029869082)
('epoch', 106, 'train_loss:', 1.7482238268852235, 'val_loss:', 0.45229134917259217)
('epoch', 107, 'train_loss:', 1.7406433451175689, 'val_loss:', 0.45322277784347537)
('epoch', 108, 'train_loss:', 1.7364215660095215, 'val_loss:', 0.44843028306961058)
('epoch', 109, 'train_loss:', 1.738736299276352, 'val_loss:', 0.45166169404983519)
('epoch', 110, 'train_loss:', 1.7331857681274414, 'val_loss:', 0.45058689951896669)
('epoch', 111, 'train_loss:', 1.7311360228061676, 'val_loss:', 0.44944432616233826)
('epoch', 112, 'train_loss:', 1.7325718939304351, 'val_loss:', 0.44996613740921021)
('epoch', 113, 'train_loss:', 1.7199256110191345, 'val_loss:', 0.44885720968246462)
('epoch', 114, 'train_loss:', 1.72111456990242, 'val_loss:', 0.44669545888900758)
('epoch', 115, 'train_loss:', 1.7139614307880402, 'val_loss:', 0.44705904006958008)
('epoch', 116, 'train_loss:', 1.7146293592453004, 'val_loss:', 0.45033501863479614)
('epoch', 117, 'train_loss:', 1.7117213475704194, 'val_loss:', 0.44553163886070252)
('epoch', 118, 'train_loss:', 1.7061665916442872, 'val_loss:', 0.44735204339027407)
('epoch', 119, 'train_loss:', 1.7036338829994202, 'val_loss:', 0.44672056794166565)
('epoch', 120, 'train_loss:', 1.7061696243286133, 'val_loss:', 0.44236541509628297)
('epoch', 121, 'train_loss:', 1.6987389957904815, 'val_loss:', 0.44413341164588926)
('epoch', 122, 'train_loss:', 1.6993192613124848, 'val_loss:', 0.44567009687423709)
('epoch', 123, 'train_loss:', 1.6983944523334502, 'val_loss:', 0.44244376659393309)
('epoch', 124, 'train_loss:', 1.6913963544368744, 'val_loss:', 0.44314535617828371)
('epoch', 125, 'train_loss:', 1.6908517205715179, 'val_loss:', 0.44260367274284362)
('epoch', 126, 'train_loss:', 1.6852710080146789, 'val_loss:', 0.44260482549667357)
('epoch', 127, 'train_loss:', 1.6851647341251372, 'val_loss:', 0.44323477268218991)
('epoch', 128, 'train_loss:', 1.6825464236736298, 'val_loss:', 0.44529958128929137)
('epoch', 129, 'train_loss:', 1.6801503908634186, 'val_loss:', 0.44254443168640134)
('epoch', 130, 'train_loss:', 1.6799008786678313, 'val_loss:', 0.44115615010261533)
('epoch', 131, 'train_loss:', 1.6730233263969421, 'val_loss:', 0.44298094987869263)
('epoch', 132, 'train_loss:', 1.6741049313545227, 'val_loss:', 0.43915480494499204)
('epoch', 133, 'train_loss:', 1.6699646508693695, 'val_loss:', 0.44112755298614503)
('epoch', 134, 'train_loss:', 1.6668125450611115, 'val_loss:', 0.44188602328300475)
('epoch', 135, 'train_loss:', 1.6675207567214967, 'val_loss:', 0.43919795632362368)
('epoch', 136, 'train_loss:', 1.6627818536758423, 'val_loss:', 0.44129702091217043)
('epoch', 137, 'train_loss:', 1.6600119721889497, 'val_loss:', 0.43897501707077025)
('epoch', 138, 'train_loss:', 1.6549479579925537, 'val_loss:', 0.4398027467727661)
('epoch', 139, 'train_loss:', 1.6597067213058472, 'val_loss:', 0.43974457502365111)
('epoch', 140, 'train_loss:', 1.6553705513477326, 'val_loss:', 0.43779496192932127)
('epoch', 141, 'train_loss:', 1.6527367889881135, 'val_loss:', 0.43514631628990175)
('epoch', 142, 'train_loss:', 1.6499191594123841, 'val_loss:', 0.43922988772392274)
('epoch', 143, 'train_loss:', 1.6470831072330474, 'val_loss:', 0.43863377690315247)
('epoch', 144, 'train_loss:', 1.642428708076477, 'val_loss:', 0.43722837924957275)
('epoch', 145, 'train_loss:', 1.6417007863521575, 'val_loss:', 0.436083083152771)
('epoch', 146, 'train_loss:', 1.6435110104084014, 'val_loss:', 0.43832628488540648)
('epoch', 147, 'train_loss:', 1.6409481704235076, 'val_loss:', 0.43878663063049317)
('epoch', 148, 'train_loss:', 1.6352312982082366, 'val_loss:', 0.43600369453430177)
('epoch', 149, 'train_loss:', 1.6356423413753509, 'val_loss:', 0.43475982189178469)
('epoch', 150, 'train_loss:', 1.6356036674976349, 'val_loss:', 0.43482516050338743)
('epoch', 151, 'train_loss:', 1.6315133810043334, 'val_loss:', 0.43637390494346617)
('epoch', 152, 'train_loss:', 1.6280901026725769, 'val_loss:', 0.4377263903617859)
('epoch', 153, 'train_loss:', 1.6260073292255401, 'val_loss:', 0.43724262118339541)
('epoch', 154, 'train_loss:', 1.6250114381313323, 'val_loss:', 0.43528151512145996)
('epoch', 155, 'train_loss:', 1.6227066707611084, 'val_loss:', 0.43727436423301697)
('epoch', 156, 'train_loss:', 1.6209663772583007, 'val_loss:', 0.43775788187980652)
('epoch', 157, 'train_loss:', 1.6207929039001465, 'val_loss:', 0.43558452963829042)
('epoch', 158, 'train_loss:', 1.6134411144256591, 'val_loss:', 0.43471205234527588)
('epoch', 159, 'train_loss:', 1.6162098562717437, 'val_loss:', 0.43456913113594053)
('epoch', 160, 'train_loss:', 1.6140883290767669, 'val_loss:', 0.43329123377799988)
('epoch', 161, 'train_loss:', 1.6080120539665221, 'val_loss:', 0.43352092146873472)
('epoch', 162, 'train_loss:', 1.6117799699306488, 'val_loss:', 0.43522632122039795)
('epoch', 163, 'train_loss:', 1.6067294239997865, 'val_loss:', 0.43705544233322141)
('epoch', 164, 'train_loss:', 1.6079286599159242, 'val_loss:', 0.43417524456977846)
('epoch', 165, 'train_loss:', 1.607199159860611, 'val_loss:', 0.43382007718086241)
('epoch', 166, 'train_loss:', 1.605116993188858, 'val_loss:', 0.43487966895103453)
('epoch', 167, 'train_loss:', 1.6019202899932861, 'val_loss:', 0.4322610807418823)
('epoch', 168, 'train_loss:', 1.5955101764202118, 'val_loss:', 0.43136256575584414)
('epoch', 169, 'train_loss:', 1.5989695167541504, 'val_loss:', 0.43184363484382632)
('epoch', 170, 'train_loss:', 1.5953516221046449, 'val_loss:', 0.4327681648731232)
('epoch', 171, 'train_loss:', 1.5913561308383941, 'val_loss:', 0.43292643904685973)
('epoch', 172, 'train_loss:', 1.5900261723995208, 'val_loss:', 0.43311753392219543)
('epoch', 173, 'train_loss:', 1.5896782279014587, 'val_loss:', 0.43336092710494994)
('epoch', 174, 'train_loss:', 1.5870185339450835, 'val_loss:', 0.4321684694290161)
('epoch', 175, 'train_loss:', 1.5855311882495879, 'val_loss:', 0.43511866092681883)
('epoch', 176, 'train_loss:', 1.582790275812149, 'val_loss:', 0.43077266335487363)
('epoch', 177, 'train_loss:', 1.5819246685504913, 'val_loss:', 0.43346167922019957)
('epoch', 178, 'train_loss:', 1.5761859452724456, 'val_loss:', 0.43212504863739015)
('epoch', 179, 'train_loss:', 1.581371042728424, 'val_loss:', 0.43106420397758483)
('epoch', 180, 'train_loss:', 1.5778985166549682, 'val_loss:', 0.43194956064224244)
('epoch', 181, 'train_loss:', 1.5742273998260499, 'val_loss:', 0.43239496827125551)
('epoch', 182, 'train_loss:', 1.5761875569820405, 'val_loss:', 0.4331360113620758)
('epoch', 183, 'train_loss:', 1.5723051536083221, 'val_loss:', 0.43205877423286437)
('epoch', 184, 'train_loss:', 1.571473274230957, 'val_loss:', 0.431172593832016)
('epoch', 185, 'train_loss:', 1.5686445713043213, 'val_loss:', 0.43124970078468322)
('epoch', 186, 'train_loss:', 1.5663823688030243, 'val_loss:', 0.43151199221611025)
('epoch', 187, 'train_loss:', 1.5641840088367462, 'val_loss:', 0.43073438048362733)
('epoch', 188, 'train_loss:', 1.5632537066936494, 'val_loss:', 0.43211588144302371)
('epoch', 189, 'train_loss:', 1.5634055519104004, 'val_loss:', 0.43279818177223206)
('epoch', 190, 'train_loss:', 1.5589707040786742, 'val_loss:', 0.43142792105674743)
('epoch', 191, 'train_loss:', 1.5595095181465148, 'val_loss:', 0.43282912135124207)
('epoch', 192, 'train_loss:', 1.5565027940273284, 'val_loss:', 0.43282857537269592)
('epoch', 193, 'train_loss:', 1.5573087978363036, 'val_loss:', 0.43335413932800293)
('epoch', 194, 'train_loss:', 1.5533500516414642, 'val_loss:', 0.43275603652000427)
('epoch', 195, 'train_loss:', 1.5506021368503571, 'val_loss:', 0.43327242255210874)
('epoch', 196, 'train_loss:', 1.5514542555809021, 'val_loss:', 0.43294575929641721)
('epoch', 197, 'train_loss:', 1.5487143635749816, 'val_loss:', 0.42941523909568785)
('epoch', 198, 'train_loss:', 1.5473108899593353, 'val_loss:', 0.43161774039268491)
('epoch', 199, 'train_loss:', 1.5463326621055602, 'val_loss:', 0.43347182035446169)
('epoch', 200, 'train_loss:', 1.5449545073509217, 'val_loss:', 0.43111679315567014)
('epoch', 201, 'train_loss:', 1.5393693351745605, 'val_loss:', 0.43026521325111389)
('epoch', 202, 'train_loss:', 1.542827558517456, 'val_loss:', 0.43255038261413575)
('epoch', 203, 'train_loss:', 1.539655201435089, 'val_loss:', 0.43458524823188782)
('epoch', 204, 'train_loss:', 1.5390207576751709, 'val_loss:', 0.43456189036369325)
('epoch', 205, 'train_loss:', 1.5390122687816621, 'val_loss:', 0.43124398469924929)
('epoch', 206, 'train_loss:', 1.5304566204547883, 'val_loss:', 0.43164409518241881)
('epoch', 207, 'train_loss:', 1.5294379591941833, 'val_loss:', 0.43199015855789186)
('epoch', 208, 'train_loss:', 1.5318530082702637, 'val_loss:', 0.43298510313034055)
('epoch', 209, 'train_loss:', 1.5278599262237549, 'val_loss:', 0.4338101303577423)
('epoch', 210, 'train_loss:', 1.5253191947937013, 'val_loss:', 0.43336468219757079)
('epoch', 211, 'train_loss:', 1.5253912782669068, 'val_loss:', 0.43476438283920288)
('epoch', 212, 'train_loss:', 1.5239865076541901, 'val_loss:', 0.43398607611656187)
('epoch', 213, 'train_loss:', 1.5237203717231751, 'val_loss:', 0.43534319043159486)
('epoch', 214, 'train_loss:', 1.5196113669872284, 'val_loss:', 0.43421634674072268)
('epoch', 215, 'train_loss:', 1.516949589252472, 'val_loss:', 0.43301245212554934)
('epoch', 216, 'train_loss:', 1.5171878063678741, 'val_loss:', 0.43333781361579893)
('epoch', 217, 'train_loss:', 1.5183505654335021, 'val_loss:', 0.43732696652412417)
('epoch', 218, 'train_loss:', 1.5140861749649048, 'val_loss:', 0.43460418820381164)
('epoch', 219, 'train_loss:', 1.5090827417373658, 'val_loss:', 0.43233929634094237)
('epoch', 220, 'train_loss:', 1.5124187493324279, 'val_loss:', 0.43507807016372679)
('epoch', 221, 'train_loss:', 1.5130052137374879, 'val_loss:', 0.43165305018424988)
('epoch', 222, 'train_loss:', 1.5071183013916016, 'val_loss:', 0.43298955202102662)
('epoch', 223, 'train_loss:', 1.5086937248706818, 'val_loss:', 0.43235839962959288)
('epoch', 224, 'train_loss:', 1.5062965977191924, 'val_loss:', 0.43365537881851196)
('epoch', 225, 'train_loss:', 1.5027660763263702, 'val_loss:', 0.43359220504760743)
('epoch', 226, 'train_loss:', 1.502117508649826, 'val_loss:', 0.43345866918563841)
('epoch', 227, 'train_loss:', 1.5019098937511444, 'val_loss:', 0.43295154571533201)
('epoch', 228, 'train_loss:', 1.5008033013343811, 'val_loss:', 0.43449745893478392)
('epoch', 229, 'train_loss:', 1.5013182830810547, 'val_loss:', 0.43294787883758545)
('epoch', 230, 'train_loss:', 1.4927217483520507, 'val_loss:', 0.43321775555610659)
('epoch', 231, 'train_loss:', 1.4940489280223845, 'val_loss:', 0.43428264021873475)
('epoch', 232, 'train_loss:', 1.4931508266925813, 'val_loss:', 0.43464755296707153)
('epoch', 233, 'train_loss:', 1.4938138329982757, 'val_loss:', 0.43597907066345215)
('epoch', 234, 'train_loss:', 1.4897216391563415, 'val_loss:', 0.43544442534446715)
('epoch', 235, 'train_loss:', 1.4864675223827362, 'val_loss:', 0.43433043003082278)
('epoch', 236, 'train_loss:', 1.4873077034950257, 'val_loss:', 0.4340998315811157)
('epoch', 237, 'train_loss:', 1.4868294167518616, 'val_loss:', 0.43549803614616395)
('epoch', 238, 'train_loss:', 1.4818828117847442, 'val_loss:', 0.43297504067420961)
('epoch', 239, 'train_loss:', 1.4807641613483429, 'val_loss:', 0.4358828592300415)
('epoch', 240, 'train_loss:', 1.4827173352241516, 'val_loss:', 0.43513070821762084)
('epoch', 241, 'train_loss:', 1.4801835453510284, 'val_loss:', 0.4369273102283478)
('epoch', 242, 'train_loss:', 1.483245313167572, 'val_loss:', 0.43581650972366331)
('epoch', 243, 'train_loss:', 1.4774097645282744, 'val_loss:', 0.43640599608421327)
('epoch', 244, 'train_loss:', 1.4750914406776428, 'val_loss:', 0.43473928093910219)
('epoch', 245, 'train_loss:', 1.475417104959488, 'val_loss:', 0.4390103042125702)
('epoch', 246, 'train_loss:', 1.4696845984458924, 'val_loss:', 0.43675318479537961)
('epoch', 247, 'train_loss:', 1.4729316127300263, 'val_loss:', 0.43798696637153628)
('epoch', 248, 'train_loss:', 1.4731159830093383, 'val_loss:', 0.43573202729225158)
('epoch', 249, 'train_loss:', 1.4699712991714478, 'val_loss:', 0.43728877186775206)
('epoch', 250, 'train_loss:', 1.4660939574241638, 'val_loss:', 0.43615382194519042)
('epoch', 251, 'train_loss:', 1.4667473638057709, 'val_loss:', 0.43703191637992861)
('epoch', 252, 'train_loss:', 1.4643444693088532, 'val_loss:', 0.43852240800857545)
('epoch', 253, 'train_loss:', 1.4636656558513641, 'val_loss:', 0.43887866139411924)
('epoch', 254, 'train_loss:', 1.461462390422821, 'val_loss:', 0.4367496395111084)
('epoch', 255, 'train_loss:', 1.4574262368679047, 'val_loss:', 0.43870983004570008)
('epoch', 256, 'train_loss:', 1.4586271154880524, 'val_loss:', 0.43731515049934389)
('epoch', 257, 'train_loss:', 1.4536835360527038, 'val_loss:', 0.43609219908714292)
('epoch', 258, 'train_loss:', 1.4535793244838715, 'val_loss:', 0.43971520900726319)
('epoch', 259, 'train_loss:', 1.4563617599010468, 'val_loss:', 0.43865304231643676)
('epoch', 260, 'train_loss:', 1.4523561799526215, 'val_loss:', 0.44038018226623538)
('epoch', 261, 'train_loss:', 1.4482663404941558, 'val_loss:', 0.43997049212455752)
('epoch', 262, 'train_loss:', 1.4486722695827483, 'val_loss:', 0.43875658273696899)
('epoch', 263, 'train_loss:', 1.4462199831008911, 'val_loss:', 0.43673988103866579)
('epoch', 264, 'train_loss:', 1.4501412081718446, 'val_loss:', 0.43910426020622251)
('epoch', 265, 'train_loss:', 1.4453457605838775, 'val_loss:', 0.43943784236907957)
('epoch', 266, 'train_loss:', 1.4422238719463349, 'val_loss:', 0.44096724748611449)
('epoch', 267, 'train_loss:', 1.4423123383522034, 'val_loss:', 0.44050782442092895)
('epoch', 268, 'train_loss:', 1.4380471515655517, 'val_loss:', 0.44012985706329344)
('epoch', 269, 'train_loss:', 1.4363092756271363, 'val_loss:', 0.4375960063934326)
('epoch', 270, 'train_loss:', 1.436120218038559, 'val_loss:', 0.44099243998527526)
('epoch', 271, 'train_loss:', 1.4322772240638733, 'val_loss:', 0.44032992839813234)
('epoch', 272, 'train_loss:', 1.4285697746276855, 'val_loss:', 0.44139283180236816)
('epoch', 273, 'train_loss:', 1.4335878050327302, 'val_loss:', 0.44065359234809875)
('epoch', 274, 'train_loss:', 1.4309135830402375, 'val_loss:', 0.44177165031433108)
('epoch', 275, 'train_loss:', 1.4313075304031373, 'val_loss:', 0.44183694720268252)
('epoch', 276, 'train_loss:', 1.4273878824710846, 'val_loss:', 0.4435620653629303)
('epoch', 277, 'train_loss:', 1.4228424036502838, 'val_loss:', 0.44411499023437501)
('epoch', 278, 'train_loss:', 1.4265228223800659, 'val_loss:', 0.44212866067886353)
('epoch', 279, 'train_loss:', 1.420233758687973, 'val_loss:', 0.44204477667808534)
('epoch', 280, 'train_loss:', 1.4191196072101593, 'val_loss:', 0.44197762727737427)
('epoch', 281, 'train_loss:', 1.4189564454555512, 'val_loss:', 0.44276915550231932)
('epoch', 282, 'train_loss:', 1.4202132964134215, 'val_loss:', 0.44346409440040591)
('epoch', 283, 'train_loss:', 1.4183676731586456, 'val_loss:', 0.44465937018394469)
('epoch', 284, 'train_loss:', 1.4157798480987549, 'val_loss:', 0.44426276087760924)
('epoch', 285, 'train_loss:', 1.4169503605365754, 'val_loss:', 0.44490367770195005)
('epoch', 286, 'train_loss:', 1.4132212603092194, 'val_loss:', 0.44633534908294675)
('epoch', 287, 'train_loss:', 1.4137576925754547, 'val_loss:', 0.44338259220123288)
('epoch', 288, 'train_loss:', 1.4080632102489472, 'val_loss:', 0.44504219055175781)
('epoch', 289, 'train_loss:', 1.4102166032791137, 'val_loss:', 0.44593866109848024)
('epoch', 290, 'train_loss:', 1.4066746401786805, 'val_loss:', 0.44539334177970885)
('epoch', 291, 'train_loss:', 1.404783775806427, 'val_loss:', 0.44418244123458861)
('epoch', 292, 'train_loss:', 1.4038864445686341, 'val_loss:', 0.44536856293678284)
('epoch', 293, 'train_loss:', 1.4009744691848756, 'val_loss:', 0.44734001755714414)
('epoch', 294, 'train_loss:', 1.4005979013442993, 'val_loss:', 0.44668414950370788)
('epoch', 295, 'train_loss:', 1.3970323979854584, 'val_loss:', 0.44735017061233523)
('epoch', 296, 'train_loss:', 1.4015374290943146, 'val_loss:', 0.445877468585968)
('epoch', 297, 'train_loss:', 1.3972877538204194, 'val_loss:', 0.44697382926940921)
('epoch', 298, 'train_loss:', 1.3946242344379425, 'val_loss:', 0.44705256104469299)
('epoch', 299, 'train_loss:', 1.3927433705329895, 'val_loss:', 0.44675598382949827)
('epoch', 300, 'train_loss:', 1.3945240354537964, 'val_loss:', 0.44826936841011045)
('epoch', 301, 'train_loss:', 1.391070500612259, 'val_loss:', 0.44887376308441163)
('epoch', 302, 'train_loss:', 1.3887864875793456, 'val_loss:', 0.44893558740615847)
('epoch', 303, 'train_loss:', 1.3900275146961212, 'val_loss:', 0.44763670563697816)
('epoch', 304, 'train_loss:', 1.3833106482028961, 'val_loss:', 0.44813621163368222)
('epoch', 305, 'train_loss:', 1.3830464816093444, 'val_loss:', 0.4498379671573639)
('epoch', 306, 'train_loss:', 1.3804203927516938, 'val_loss:', 0.45085107207298281)
('epoch', 307, 'train_loss:', 1.3769770729541779, 'val_loss:', 0.45044010758399966)
('epoch', 308, 'train_loss:', 1.3786872112751007, 'val_loss:', 0.45054675459861754)
('epoch', 309, 'train_loss:', 1.3746181535720825, 'val_loss:', 0.44930444955825805)
('epoch', 310, 'train_loss:', 1.3770864617824554, 'val_loss:', 0.44907049655914305)
('epoch', 311, 'train_loss:', 1.3708275198936462, 'val_loss:', 0.45196159243583678)
('epoch', 312, 'train_loss:', 1.3736363244056702, 'val_loss:', 0.44948931097984313)
('epoch', 313, 'train_loss:', 1.3729205822944641, 'val_loss:', 0.45430698752403259)
('epoch', 314, 'train_loss:', 1.3695736467838286, 'val_loss:', 0.45338343501091005)
('epoch', 315, 'train_loss:', 1.3671037423610688, 'val_loss:', 0.4511037063598633)
('epoch', 316, 'train_loss:', 1.3679711127281189, 'val_loss:', 0.45229855298995969)
('epoch', 317, 'train_loss:', 1.3642909944057464, 'val_loss:', 0.45184847354888918)
('epoch', 318, 'train_loss:', 1.3648585522174834, 'val_loss:', 0.45788951039314268)
('epoch', 319, 'train_loss:', 1.3629163599014282, 'val_loss:', 0.45337374925613405)
('epoch', 320, 'train_loss:', 1.3617507863044738, 'val_loss:', 0.45411576390266417)
('epoch', 321, 'train_loss:', 1.3596684014797211, 'val_loss:', 0.45297770380973817)
('epoch', 322, 'train_loss:', 1.3556064343452454, 'val_loss:', 0.45614327669143678)
('epoch', 323, 'train_loss:', 1.3502925980091094, 'val_loss:', 0.456221969127655)
('epoch', 324, 'train_loss:', 1.3543748807907106, 'val_loss:', 0.4552549922466278)
('epoch', 325, 'train_loss:', 1.3498370921611786, 'val_loss:', 0.45854470968246458)
('epoch', 326, 'train_loss:', 1.3544490706920624, 'val_loss:', 0.45524870872497558)
('epoch', 327, 'train_loss:', 1.3495755481719971, 'val_loss:', 0.45773390412330628)
('epoch', 328, 'train_loss:', 1.3456726968288422, 'val_loss:', 0.45615626215934751)
('epoch', 329, 'train_loss:', 1.34427468419075, 'val_loss:', 0.45509794116020202)
('epoch', 330, 'train_loss:', 1.3434559607505798, 'val_loss:', 0.45687041521072386)
('epoch', 331, 'train_loss:', 1.3455703008174895, 'val_loss:', 0.46062000989913943)
('epoch', 332, 'train_loss:', 1.337924907207489, 'val_loss:', 0.45745965480804446)
('epoch', 333, 'train_loss:', 1.3394820916652679, 'val_loss:', 0.45971132278442384)
('epoch', 334, 'train_loss:', 1.3386275148391724, 'val_loss:', 0.46196480989456179)
('epoch', 335, 'train_loss:', 1.3364215528964996, 'val_loss:', 0.45900427460670473)
('epoch', 336, 'train_loss:', 1.3314273715019227, 'val_loss:', 0.46043308019638063)
('epoch', 337, 'train_loss:', 1.3334387934207916, 'val_loss:', 0.45926068425178529)
('epoch', 338, 'train_loss:', 1.3317331659793854, 'val_loss:', 0.45986855745315552)
('epoch', 339, 'train_loss:', 1.3335125601291657, 'val_loss:', 0.46061703801155091)
('epoch', 340, 'train_loss:', 1.3302467465400696, 'val_loss:', 0.46232744097709655)
('epoch', 341, 'train_loss:', 1.3283728504180907, 'val_loss:', 0.46114099860191343)
('epoch', 342, 'train_loss:', 1.3250335443019867, 'val_loss:', 0.46190968990325926)
('epoch', 343, 'train_loss:', 1.3233173871040345, 'val_loss:', 0.46274704575538633)
('epoch', 344, 'train_loss:', 1.3247579038143158, 'val_loss:', 0.4645706522464752)
('epoch', 345, 'train_loss:', 1.3225845134258269, 'val_loss:', 0.46197401762008666)
('epoch', 346, 'train_loss:', 1.3199167954921722, 'val_loss:', 0.46181818008422854)
('epoch', 347, 'train_loss:', 1.3167948353290557, 'val_loss:', 0.46386983275413513)
('epoch', 348, 'train_loss:', 1.3149389481544496, 'val_loss:', 0.46600713491439821)
('epoch', 349, 'train_loss:', 1.3129120004177093, 'val_loss:', 0.46512223601341246)
('epoch', 350, 'train_loss:', 1.3117883777618409, 'val_loss:', 0.46301242828369138)
('epoch', 351, 'train_loss:', 1.3063626706600189, 'val_loss:', 0.46656650900840757)
('epoch', 352, 'train_loss:', 1.3099708819389344, 'val_loss:', 0.46612557291984558)
('epoch', 353, 'train_loss:', 1.3105006086826325, 'val_loss:', 0.46472623229026794)
('epoch', 354, 'train_loss:', 1.3074570691585541, 'val_loss:', 0.46777396440505981)
('epoch', 355, 'train_loss:', 1.3036747896671295, 'val_loss:', 0.46790596723556521)
('epoch', 356, 'train_loss:', 1.3049831855297089, 'val_loss:', 0.46701788783073428)
('epoch', 357, 'train_loss:', 1.2980498933792115, 'val_loss:', 0.46828376293182372)
('epoch', 358, 'train_loss:', 1.3032315516471862, 'val_loss:', 0.47027645707130433)
('epoch', 359, 'train_loss:', 1.2971957004070283, 'val_loss:', 0.46819292187690736)
('epoch', 360, 'train_loss:', 1.2972617626190186, 'val_loss:', 0.47005585908889769)
('epoch', 361, 'train_loss:', 1.2951499259471893, 'val_loss:', 0.46815746545791626)
('epoch', 362, 'train_loss:', 1.296943542957306, 'val_loss:', 0.47157868385314944)
('epoch', 363, 'train_loss:', 1.2931456363201141, 'val_loss:', 0.4704395627975464)
('epoch', 364, 'train_loss:', 1.2892938005924224, 'val_loss:', 0.47116646647453309)
('epoch', 365, 'train_loss:', 1.2896078658103942, 'val_loss:', 0.47024630188941957)
('epoch', 366, 'train_loss:', 1.2903456974029541, 'val_loss:', 0.47199249148368838)
('epoch', 367, 'train_loss:', 1.2867193269729613, 'val_loss:', 0.4731164300441742)
('epoch', 368, 'train_loss:', 1.283980724811554, 'val_loss:', 0.47371168613433839)
('epoch', 369, 'train_loss:', 1.2796345448493958, 'val_loss:', 0.47575504541397096)
('epoch', 370, 'train_loss:', 1.2847151660919189, 'val_loss:', 0.47297775983810425)
('epoch', 371, 'train_loss:', 1.2788236820697785, 'val_loss:', 0.47423109889030457)
('epoch', 372, 'train_loss:', 1.2803764688968657, 'val_loss:', 0.47099876999855039)
('epoch', 373, 'train_loss:', 1.2775931131839753, 'val_loss:', 0.47245808482170104)
('epoch', 374, 'train_loss:', 1.2751021111011505, 'val_loss:', 0.47713168621063234)
('epoch', 375, 'train_loss:', 1.2747043621540071, 'val_loss:', 0.47332316517829898)
('epoch', 376, 'train_loss:', 1.2726423943042755, 'val_loss:', 0.47473977804183959)
('epoch', 377, 'train_loss:', 1.2706609570980072, 'val_loss:', 0.47709995388984683)
('epoch', 378, 'train_loss:', 1.2691632163524629, 'val_loss:', 0.47792120456695558)
('epoch', 379, 'train_loss:', 1.2686668872833251, 'val_loss:', 0.4759372556209564)
('epoch', 380, 'train_loss:', 1.2646697998046874, 'val_loss:', 0.47830105900764464)
('epoch', 381, 'train_loss:', 1.2641856408119201, 'val_loss:', 0.47894028306007386)
('epoch', 382, 'train_loss:', 1.2630059742927551, 'val_loss:', 0.47693073272705078)
('epoch', 383, 'train_loss:', 1.261010011434555, 'val_loss:', 0.48026485681533815)
('epoch', 384, 'train_loss:', 1.259605859518051, 'val_loss:', 0.47848691105842589)
('epoch', 385, 'train_loss:', 1.2575880765914917, 'val_loss:', 0.48173924088478087)
('epoch', 386, 'train_loss:', 1.2604108333587647, 'val_loss:', 0.48045220494270324)
('epoch', 387, 'train_loss:', 1.2527081704139709, 'val_loss:', 0.47724604368209839)
('epoch', 388, 'train_loss:', 1.2517889499664308, 'val_loss:', 0.47995373725891111)
('epoch', 389, 'train_loss:', 1.251738737821579, 'val_loss:', 0.48046736240386961)
('epoch', 390, 'train_loss:', 1.247394142150879, 'val_loss:', 0.4812881934642792)
('epoch', 391, 'train_loss:', 1.2499172365665436, 'val_loss:', 0.4803389036655426)
('epoch', 392, 'train_loss:', 1.2455172193050386, 'val_loss:', 0.48355841398239136)
('epoch', 393, 'train_loss:', 1.2457780873775481, 'val_loss:', 0.48541845321655275)
('epoch', 394, 'train_loss:', 1.2459147071838379, 'val_loss:', 0.48258490204811094)
('epoch', 395, 'train_loss:', 1.2418622303009033, 'val_loss:', 0.48947464942932128)
('epoch', 396, 'train_loss:', 1.2384946501255036, 'val_loss:', 0.48544638633728027)
('epoch', 397, 'train_loss:', 1.2424453544616698, 'val_loss:', 0.48786369323730466)
('epoch', 398, 'train_loss:', 1.2397043490409851, 'val_loss:', 0.48456804513931273)
('epoch', 399, 'train_loss:', 1.2334729552268981, 'val_loss:', 0.48657876849174497)
('epoch', 400, 'train_loss:', 1.2374727296829224, 'val_loss:', 0.48846229910850525)
('epoch', 401, 'train_loss:', 1.2351544940471648, 'val_loss:', 0.48840119719505309)
('epoch', 402, 'train_loss:', 1.2330736017227173, 'val_loss:', 0.49101630449295042)
('epoch', 403, 'train_loss:', 1.22864226937294, 'val_loss:', 0.48585254311561582)
('epoch', 404, 'train_loss:', 1.2262198340892791, 'val_loss:', 0.48774752616882322)
('epoch', 405, 'train_loss:', 1.2264424407482146, 'val_loss:', 0.48811133265495299)
('epoch', 406, 'train_loss:', 1.2238402700424194, 'val_loss:', 0.48981899619102476)
('epoch', 407, 'train_loss:', 1.2243191206455231, 'val_loss:', 0.48617690443992617)
('epoch', 408, 'train_loss:', 1.2174827337265015, 'val_loss:', 0.49164948701858519)
('epoch', 409, 'train_loss:', 1.2214019346237182, 'val_loss:', 0.48843997240066528)
('epoch', 410, 'train_loss:', 1.2179747521877289, 'val_loss:', 0.49073974609374998)
('epoch', 411, 'train_loss:', 1.2165790045261382, 'val_loss:', 0.4899472737312317)
('epoch', 412, 'train_loss:', 1.215145194530487, 'val_loss:', 0.49107686400413514)
('epoch', 413, 'train_loss:', 1.2171037828922271, 'val_loss:', 0.49298982143402098)
('epoch', 414, 'train_loss:', 1.2120911693572998, 'val_loss:', 0.49756546854972838)
('epoch', 415, 'train_loss:', 1.2108025431632996, 'val_loss:', 0.49475218296051027)
('epoch', 416, 'train_loss:', 1.2129543149471282, 'val_loss:', 0.49298595786094668)
('epoch', 417, 'train_loss:', 1.2059612357616425, 'val_loss:', 0.49555824995040892)
('epoch', 418, 'train_loss:', 1.2069999635219575, 'val_loss:', 0.49671820044517517)
('epoch', 419, 'train_loss:', 1.20718954205513, 'val_loss:', 0.49408605694770813)
('epoch', 420, 'train_loss:', 1.2010386788845062, 'val_loss:', 0.49654605627059939)
('epoch', 421, 'train_loss:', 1.2021849834918976, 'val_loss:', 0.49633266568183898)
('epoch', 422, 'train_loss:', 1.1977951228618622, 'val_loss:', 0.49561784625053407)
('epoch', 423, 'train_loss:', 1.1980724143981933, 'val_loss:', 0.49701424241065978)
('epoch', 424, 'train_loss:', 1.2017644023895264, 'val_loss:', 0.49963780403137209)
('epoch', 425, 'train_loss:', 1.1959887880086899, 'val_loss:', 0.49938491582870481)
('epoch', 426, 'train_loss:', 1.1940095901489258, 'val_loss:', 0.50003880381584165)
('epoch', 427, 'train_loss:', 1.1914349913597106, 'val_loss:', 0.49896666049957278)
('epoch', 428, 'train_loss:', 1.1902781581878663, 'val_loss:', 0.50266044259071352)
('epoch', 429, 'train_loss:', 1.1938496708869935, 'val_loss:', 0.49913960695266724)
('epoch', 430, 'train_loss:', 1.1906378930807113, 'val_loss:', 0.50053302645683284)
('epoch', 431, 'train_loss:', 1.1913062489032746, 'val_loss:', 0.5056643450260162)
('epoch', 432, 'train_loss:', 1.1839146918058396, 'val_loss:', 0.50503120660781864)
('epoch', 433, 'train_loss:', 1.185438711643219, 'val_loss:', 0.50284296870231626)
('epoch', 434, 'train_loss:', 1.1819086903333664, 'val_loss:', 0.50669845461845398)
('epoch', 435, 'train_loss:', 1.1835664433240891, 'val_loss:', 0.50757614254951477)
('epoch', 436, 'train_loss:', 1.1799405497312545, 'val_loss:', 0.50895292878150944)
('epoch', 437, 'train_loss:', 1.174239227771759, 'val_loss:', 0.5063147079944611)
('epoch', 438, 'train_loss:', 1.177158784866333, 'val_loss:', 0.50623906970024113)
('epoch', 439, 'train_loss:', 1.1775482416152954, 'val_loss:', 0.50614270806312556)
('epoch', 440, 'train_loss:', 1.1745287013053893, 'val_loss:', 0.50507809996604924)
('epoch', 441, 'train_loss:', 1.1738800919055938, 'val_loss:', 0.50731749176979068)
('epoch', 442, 'train_loss:', 1.1670361405611038, 'val_loss:', 0.50534147739410396)
('epoch', 443, 'train_loss:', 1.1733426910638809, 'val_loss:', 0.50759213447570806)
('epoch', 444, 'train_loss:', 1.1694343119859696, 'val_loss:', 0.50817086935043332)
('epoch', 445, 'train_loss:', 1.1659409505128862, 'val_loss:', 0.50741416811943052)
('epoch', 446, 'train_loss:', 1.1625538861751556, 'val_loss:', 0.50662699460983274)
('epoch', 447, 'train_loss:', 1.1609909850358964, 'val_loss:', 0.51268422245979306)
('epoch', 448, 'train_loss:', 1.1626744776964189, 'val_loss:', 0.50755093216896052)
('epoch', 449, 'train_loss:', 1.1577070200443267, 'val_loss:', 0.50838024854660036)
('epoch', 450, 'train_loss:', 1.1615741473436356, 'val_loss:', 0.5137087857723236)
('epoch', 451, 'train_loss:', 1.159625443816185, 'val_loss:', 0.50941404342651364)
('epoch', 452, 'train_loss:', 1.1542439794540404, 'val_loss:', 0.51373428344726557)
('epoch', 453, 'train_loss:', 1.1530087363719941, 'val_loss:', 0.51498422622680662)
('epoch', 454, 'train_loss:', 1.1528855895996093, 'val_loss:', 0.51450106263160711)
('epoch', 455, 'train_loss:', 1.1531496655941009, 'val_loss:', 0.51385238289833068)
('epoch', 456, 'train_loss:', 1.1494092184305191, 'val_loss:', 0.51703856706619267)
('epoch', 457, 'train_loss:', 1.1504087895154953, 'val_loss:', 0.51510997772216793)
('epoch', 458, 'train_loss:', 1.1466571331024169, 'val_loss:', 0.51771360397338873)
('epoch', 459, 'train_loss:', 1.147611613869667, 'val_loss:', 0.51761358499526977)
('epoch', 460, 'train_loss:', 1.1405639910697938, 'val_loss:', 0.51485285401344294)
('epoch', 461, 'train_loss:', 1.1422278237342836, 'val_loss:', 0.51554886221885676)
('epoch', 462, 'train_loss:', 1.1402961444854736, 'val_loss:', 0.51813196539878847)
('epoch', 463, 'train_loss:', 1.1387431389093399, 'val_loss:', 0.51414755821228031)
('epoch', 464, 'train_loss:', 1.1370452904701234, 'val_loss:', 0.51898589491844183)
('epoch', 465, 'train_loss:', 1.1315174549818039, 'val_loss:', 0.51854058027267458)
('epoch', 466, 'train_loss:', 1.1357412034273147, 'val_loss:', 0.5188572943210602)
('epoch', 467, 'train_loss:', 1.1307842129468917, 'val_loss:', 0.52246408700942992)
('epoch', 468, 'train_loss:', 1.1352537262439728, 'val_loss:', 0.51845747828483579)
('epoch', 469, 'train_loss:', 1.134501788020134, 'val_loss:', 0.52190768480300909)
('epoch', 470, 'train_loss:', 1.1278648501634598, 'val_loss:', 0.5250141882896423)
('epoch', 471, 'train_loss:', 1.126964007616043, 'val_loss:', 0.52263105034828183)
('epoch', 472, 'train_loss:', 1.122958573102951, 'val_loss:', 0.52124176383018495)
('epoch', 473, 'train_loss:', 1.1273303669691086, 'val_loss:', 0.5213876509666443)
('epoch', 474, 'train_loss:', 1.124029683470726, 'val_loss:', 0.5225649666786194)
('epoch', 475, 'train_loss:', 1.1199255323410033, 'val_loss:', 0.5219457995891571)
('epoch', 476, 'train_loss:', 1.1216757184267043, 'val_loss:', 0.52476298093795781)
('epoch', 477, 'train_loss:', 1.1186182355880738, 'val_loss:', 0.52496117472648618)
('epoch', 478, 'train_loss:', 1.1223441219329835, 'val_loss:', 0.52541152715682982)
('epoch', 479, 'train_loss:', 1.118002740740776, 'val_loss:', 0.52471099495887752)
('epoch', 480, 'train_loss:', 1.1181020635366439, 'val_loss:', 0.52686212897300722)
('epoch', 481, 'train_loss:', 1.1104397284984588, 'val_loss:', 0.52677325844764711)
('epoch', 482, 'train_loss:', 1.1151880043745042, 'val_loss:', 0.52575864911079406)
('epoch', 483, 'train_loss:', 1.1116105103492737, 'val_loss:', 0.52906652212142946)
('epoch', 484, 'train_loss:', 1.1082856315374374, 'val_loss:', 0.5299555087089538)
('epoch', 485, 'train_loss:', 1.1145661497116088, 'val_loss:', 0.5304086828231811)
('epoch', 486, 'train_loss:', 1.10655543923378, 'val_loss:', 0.52872583031654363)
('epoch', 487, 'train_loss:', 1.1085670363903046, 'val_loss:', 0.52640642285346984)
('epoch', 488, 'train_loss:', 1.1038986426591872, 'val_loss:', 0.53132436633110047)
('epoch', 489, 'train_loss:', 1.1045670402050018, 'val_loss:', 0.53311125397682191)
('epoch', 490, 'train_loss:', 1.1045145559310914, 'val_loss:', 0.53047338247299192)
('epoch', 491, 'train_loss:', 1.0999398994445801, 'val_loss:', 0.53401588559150692)
('epoch', 492, 'train_loss:', 1.0967399191856384, 'val_loss:', 0.53355155587196346)
('epoch', 493, 'train_loss:', 1.0980611723661422, 'val_loss:', 0.533793774843216)
('epoch', 494, 'train_loss:', 1.0952805584669114, 'val_loss:', 0.53408493399620061)
('epoch', 495, 'train_loss:', 1.0966425621509552, 'val_loss:', 0.53511607885360712)
('epoch', 496, 'train_loss:', 1.0915239536762238, 'val_loss:', 0.5381042003631592)
('epoch', 497, 'train_loss:', 1.0938042044639587, 'val_loss:', 0.53746730446815494)
('epoch', 498, 'train_loss:', 1.0890400898456574, 'val_loss:', 0.53437124967575078)
('epoch', 499, 'train_loss:', 1.0903658241033554, 'val_loss:', 0.53247982978820796)
('epoch', 500, 'train_loss:', 1.0867635715007782, 'val_loss:', 0.53850145220756529)
('epoch', 501, 'train_loss:', 1.086100823879242, 'val_loss:', 0.53989658951759334)
('epoch', 502, 'train_loss:', 1.0911783599853515, 'val_loss:', 0.5385666418075562)
('epoch', 503, 'train_loss:', 1.0861022174358368, 'val_loss:', 0.53890063166618352)
('epoch', 504, 'train_loss:', 1.0872435331344605, 'val_loss:', 0.5380586123466492)
('epoch', 505, 'train_loss:', 1.0809415572881698, 'val_loss:', 0.53787244200706485)
('epoch', 506, 'train_loss:', 1.0809444034099578, 'val_loss:', 0.53883976101875308)
('epoch', 507, 'train_loss:', 1.0765629363059999, 'val_loss:', 0.53991095662117006)
('epoch', 508, 'train_loss:', 1.0824622404575348, 'val_loss:', 0.53957018852233884)
('epoch', 509, 'train_loss:', 1.0819642966985703, 'val_loss:', 0.54215046048164373)
('epoch', 510, 'train_loss:', 1.0735862702131271, 'val_loss:', 0.54294692993164062)
('epoch', 511, 'train_loss:', 1.0777001261711121, 'val_loss:', 0.54290401577949521)
('epoch', 512, 'train_loss:', 1.0724908077716828, 'val_loss:', 0.5419765102863312)
('epoch', 513, 'train_loss:', 1.0677516734600068, 'val_loss:', 0.54189385414123536)
('epoch', 514, 'train_loss:', 1.0734734368324279, 'val_loss:', 0.54084569573402408)
('epoch', 515, 'train_loss:', 1.0737852323055268, 'val_loss:', 0.54542553186416631)
('epoch', 516, 'train_loss:', 1.0693708115816116, 'val_loss:', 0.54657489299774165)
('epoch', 517, 'train_loss:', 1.0634582906961441, 'val_loss:', 0.54915933132171635)
('epoch', 518, 'train_loss:', 1.0711271804571152, 'val_loss:', 0.54459679961204532)
('epoch', 519, 'train_loss:', 1.0576983487606049, 'val_loss:', 0.54395458698272703)
('epoch', 520, 'train_loss:', 1.0655632352828979, 'val_loss:', 0.54425395846366886)
('epoch', 521, 'train_loss:', 1.06379409968853, 'val_loss:', 0.54630567073822023)
('epoch', 522, 'train_loss:', 1.0599193662405013, 'val_loss:', 0.54916059494018554)
('epoch', 523, 'train_loss:', 1.0566108000278474, 'val_loss:', 0.54861203789710999)
('epoch', 524, 'train_loss:', 1.0603231132030486, 'val_loss:', 0.55389086723327641)
('epoch', 525, 'train_loss:', 1.0542684870958328, 'val_loss:', 0.55125093102455136)
('epoch', 526, 'train_loss:', 1.0557177114486693, 'val_loss:', 0.55027245044708251)
('epoch', 527, 'train_loss:', 1.0549396872520447, 'val_loss:', 0.54784928202629091)
('epoch', 528, 'train_loss:', 1.0550880485773086, 'val_loss:', 0.55100963115692136)
('epoch', 529, 'train_loss:', 1.0494836443662643, 'val_loss:', 0.5517714858055115)
('epoch', 530, 'train_loss:', 1.0544107115268708, 'val_loss:', 0.55441628694534306)
('epoch', 531, 'train_loss:', 1.045918081998825, 'val_loss:', 0.55177072167396546)
('epoch', 532, 'train_loss:', 1.0492213380336761, 'val_loss:', 0.55210862040519715)
('epoch', 533, 'train_loss:', 1.047577605843544, 'val_loss:', 0.55319862604141234)
('epoch', 534, 'train_loss:', 1.0444340389966964, 'val_loss:', 0.5520722305774689)
('epoch', 535, 'train_loss:', 1.0459034329652785, 'val_loss:', 0.55541981816291808)
('epoch', 536, 'train_loss:', 1.0460076159238816, 'val_loss:', 0.55562704682350161)
('epoch', 537, 'train_loss:', 1.0416407918930053, 'val_loss:', 0.56080114722251895)
('epoch', 538, 'train_loss:', 1.0364236342906952, 'val_loss:', 0.55529085874557493)
('epoch', 539, 'train_loss:', 1.0367245131731033, 'val_loss:', 0.55773290157318112)
('epoch', 540, 'train_loss:', 1.0397232240438461, 'val_loss:', 0.56100937366485593)
('epoch', 541, 'train_loss:', 1.0347479873895644, 'val_loss:', 0.55951740503311154)
('epoch', 542, 'train_loss:', 1.0362915676832198, 'val_loss:', 0.5564375162124634)
('epoch', 543, 'train_loss:', 1.0392918944358827, 'val_loss:', 0.55700056076049809)
('epoch', 544, 'train_loss:', 1.0342919033765794, 'val_loss:', 0.55861737847328186)
('epoch', 545, 'train_loss:', 1.0319152832031251, 'val_loss:', 0.56149172663688662)
('epoch', 546, 'train_loss:', 1.0324680185317994, 'val_loss:', 0.55706823706626896)
('epoch', 547, 'train_loss:', 1.0311017227172852, 'val_loss:', 0.56246887683868407)
('epoch', 548, 'train_loss:', 1.0268069344758988, 'val_loss:', 0.55995585441589357)
('epoch', 549, 'train_loss:', 1.0273667567968368, 'val_loss:', 0.56289302110672002)
('epoch', 550, 'train_loss:', 1.0245411062240601, 'val_loss:', 0.56108160376548766)
('epoch', 551, 'train_loss:', 1.0246649265289307, 'val_loss:', 0.55953223705291744)
('epoch', 552, 'train_loss:', 1.025637510418892, 'val_loss:', 0.56225568652153013)
('epoch', 553, 'train_loss:', 1.0227145451307296, 'val_loss:', 0.56486976146697998)
('epoch', 554, 'train_loss:', 1.0210467159748078, 'val_loss:', 0.56309028506278991)
('epoch', 555, 'train_loss:', 1.0223539155721664, 'val_loss:', 0.56318907976150512)
('epoch', 556, 'train_loss:', 1.0192307877540587, 'val_loss:', 0.56243945121765138)
('epoch', 557, 'train_loss:', 1.0213916838169097, 'val_loss:', 0.5660382151603699)
('epoch', 558, 'train_loss:', 1.0158053958415985, 'val_loss:', 0.5673881721496582)
('epoch', 559, 'train_loss:', 1.0128934359550477, 'val_loss:', 0.56683501839637751)
('epoch', 560, 'train_loss:', 1.0149024999141694, 'val_loss:', 0.5651919484138489)
('epoch', 561, 'train_loss:', 1.0136121398210525, 'val_loss:', 0.56913115262985225)
('epoch', 562, 'train_loss:', 1.0181178832054139, 'val_loss:', 0.56676480054855349)
('epoch', 563, 'train_loss:', 1.0148684072494507, 'val_loss:', 0.56539509773254393)
('epoch', 564, 'train_loss:', 1.0111049997806549, 'val_loss:', 0.56846570491790771)
('epoch', 565, 'train_loss:', 1.0038317275047302, 'val_loss:', 0.57195352554321288)
('epoch', 566, 'train_loss:', 1.0063361585140229, 'val_loss:', 0.57123519062995909)
('epoch', 567, 'train_loss:', 1.0126412755250931, 'val_loss:', 0.57016946792602541)
('epoch', 568, 'train_loss:', 1.0075336015224456, 'val_loss:', 0.57327476024627688)
('epoch', 569, 'train_loss:', 1.0085759419202804, 'val_loss:', 0.57170591711997987)
('epoch', 570, 'train_loss:', 0.99875483393669129, 'val_loss:', 0.57002359271049496)
('epoch', 571, 'train_loss:', 0.99857560217380525, 'val_loss:', 0.57427221536636353)
('epoch', 572, 'train_loss:', 1.0065068626403808, 'val_loss:', 0.56810038685798647)
('epoch', 573, 'train_loss:', 0.99615474283695216, 'val_loss:', 0.5726204514503479)
('epoch', 574, 'train_loss:', 0.99491177737712855, 'val_loss:', 0.57824378013610844)
('epoch', 575, 'train_loss:', 1.0044244068861008, 'val_loss:', 0.5725594329833984)
('epoch', 576, 'train_loss:', 0.99570973396301266, 'val_loss:', 0.57324000835418698)
('epoch', 577, 'train_loss:', 0.99516184747219083, 'val_loss:', 0.58138988494873045)
('epoch', 578, 'train_loss:', 0.99669688284397129, 'val_loss:', 0.57783354163169864)
('epoch', 579, 'train_loss:', 0.99367525577545168, 'val_loss:', 0.57323197484016419)
('epoch', 580, 'train_loss:', 0.99500882804393764, 'val_loss:', 0.57596385598182676)
('epoch', 581, 'train_loss:', 0.9937821358442307, 'val_loss:', 0.57326331019401555)
('epoch', 582, 'train_loss:', 0.98796149134635924, 'val_loss:', 0.57574459433555603)
('epoch', 583, 'train_loss:', 0.9846666157245636, 'val_loss:', 0.5810323035717011)
('epoch', 584, 'train_loss:', 0.98392780780792233, 'val_loss:', 0.58081068754196163)
('epoch', 585, 'train_loss:', 0.99193795323371892, 'val_loss:', 0.58056031346321102)
('epoch', 586, 'train_loss:', 0.98446493923664091, 'val_loss:', 0.57816941976547243)
('epoch', 587, 'train_loss:', 0.98576509535312651, 'val_loss:', 0.58115158438682557)
('epoch', 588, 'train_loss:', 0.98469183027744289, 'val_loss:', 0.57888949036598203)
('epoch', 589, 'train_loss:', 0.98087440848350527, 'val_loss:', 0.5835972821712494)
('epoch', 590, 'train_loss:', 0.98253376483917232, 'val_loss:', 0.57931993365287782)
('epoch', 591, 'train_loss:', 0.98481765210628514, 'val_loss:', 0.57897586107254029)
('epoch', 592, 'train_loss:', 0.98087274551391601, 'val_loss:', 0.58053343176841732)
('epoch', 593, 'train_loss:', 0.9747747510671616, 'val_loss:', 0.58296298623085019)
('epoch', 594, 'train_loss:', 0.97707552731037139, 'val_loss:', 0.58359742879867549)
('epoch', 595, 'train_loss:', 0.9786742794513702, 'val_loss:', 0.58606026887893681)
('epoch', 596, 'train_loss:', 0.97398764371871949, 'val_loss:', 0.58172743678092953)
('epoch', 597, 'train_loss:', 0.97407834470272059, 'val_loss:', 0.58710947990417484)
('epoch', 598, 'train_loss:', 0.97052621006965634, 'val_loss:', 0.58571924209594728)
('epoch', 599, 'train_loss:', 0.97282733798027043, 'val_loss:', 0.58787078857421871)
