(keras)skoppula@sls-sm-8:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=3 python lstm_all_variants.py -t -d shakespeare -v additive_no_sigmoid_linear
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'shakespeare', 'train': True, 'variant': 'additive_no_sigmoid_linear', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (29743, 30), (29743, 30), (7436, 30), (7436, 30))
('num steps in trn and val epochs', 116, 29)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:82:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2390 get requests, put_count=2256 evicted_count=1000 eviction_rate=0.443262 and unsatisfied allocation rate=0.516318
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2444 get requests, put_count=2480 evicted_count=1000 eviction_rate=0.403226 and unsatisfied allocation rate=0.403846
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4777 get requests, put_count=4680 evicted_count=1000 eviction_rate=0.213675 and unsatisfied allocation rate=0.241993
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 0, 'train_loss:', 3.9546739864349365, 'val_loss:', 0.96165108203887939)
('epoch', 1, 'train_loss:', 3.842860336303711, 'val_loss:', 0.95870637416839599)
('epoch', 2, 'train_loss:', 3.8146035313606261, 'val_loss:', 0.95233358144760127)
('epoch', 3, 'train_loss:', 3.7942737388610839, 'val_loss:', 0.94402368783950807)
('epoch', 4, 'train_loss:', 3.7550070166587828, 'val_loss:', 0.93324975013732914)
('epoch', 5, 'train_loss:', 3.6937792110443115, 'val_loss:', 0.91335881233215332)
('epoch', 6, 'train_loss:', 3.6142705416679384, 'val_loss:', 0.89368871212005618)
('epoch', 7, 'train_loss:', 3.529729266166687, 'val_loss:', 0.87207508325576777)
('epoch', 8, 'train_loss:', 3.4443016386032106, 'val_loss:', 0.85159527540206914)
('epoch', 9, 'train_loss:', 3.3737116718292235, 'val_loss:', 0.83420172214508059)
('epoch', 10, 'train_loss:', 3.3125781846046447, 'val_loss:', 0.8237059760093689)
('epoch', 11, 'train_loss:', 3.2558723640441896, 'val_loss:', 0.80821066379547124)
('epoch', 12, 'train_loss:', 3.2106952786445619, 'val_loss:', 0.79765785694122315)
('epoch', 13, 'train_loss:', 3.1681543946266175, 'val_loss:', 0.78697999238967897)
('epoch', 14, 'train_loss:', 3.1253193807601929, 'val_loss:', 0.77920648574829099)
('epoch', 15, 'train_loss:', 3.0924576067924501, 'val_loss:', 0.7686348557472229)
('epoch', 16, 'train_loss:', 3.0586089944839476, 'val_loss:', 0.76239858388900761)
('epoch', 17, 'train_loss:', 3.0306348967552186, 'val_loss:', 0.75696482181549074)
('epoch', 18, 'train_loss:', 3.0101250672340392, 'val_loss:', 0.74933861970901494)
('epoch', 19, 'train_loss:', 2.9899834871292112, 'val_loss:', 0.74659772634506227)
('epoch', 20, 'train_loss:', 2.9662566566467286, 'val_loss:', 0.74094555616378788)
('epoch', 21, 'train_loss:', 2.9542868113517762, 'val_loss:', 0.7381903791427612)
('epoch', 22, 'train_loss:', 2.9400425434112547, 'val_loss:', 0.73282272815704341)
('epoch', 23, 'train_loss:', 2.9293115139007568, 'val_loss:', 0.73291132450103758)
('epoch', 24, 'train_loss:', 2.9141458415985109, 'val_loss:', 0.72957822561264041)
('epoch', 25, 'train_loss:', 2.9039080667495729, 'val_loss:', 0.72727299451827998)
('epoch', 26, 'train_loss:', 2.8946727538108825, 'val_loss:', 0.72426360368728637)
('epoch', 27, 'train_loss:', 2.8838827753067018, 'val_loss:', 0.72181994676589967)
('epoch', 28, 'train_loss:', 2.8740867543220521, 'val_loss:', 0.71927131652832033)
('epoch', 29, 'train_loss:', 2.869975905418396, 'val_loss:', 0.7170702195167542)
('epoch', 30, 'train_loss:', 2.8583819174766543, 'val_loss:', 0.71536882638931276)
('epoch', 31, 'train_loss:', 2.8508864712715147, 'val_loss:', 0.71398391723632815)
('epoch', 32, 'train_loss:', 2.8413205838203428, 'val_loss:', 0.71286762237548829)
('epoch', 33, 'train_loss:', 2.8368916440010072, 'val_loss:', 0.71075505018234253)
('epoch', 34, 'train_loss:', 2.8302508878707884, 'val_loss:', 0.70710078001022336)
('epoch', 35, 'train_loss:', 2.8185110044479371, 'val_loss:', 0.70634830951690675)
('epoch', 36, 'train_loss:', 2.8172831153869629, 'val_loss:', 0.70246614694595333)
('epoch', 37, 'train_loss:', 2.804596230983734, 'val_loss:', 0.7021953725814819)
('epoch', 38, 'train_loss:', 2.7988636422157289, 'val_loss:', 0.69923496961593623)
('epoch', 39, 'train_loss:', 2.7912363314628603, 'val_loss:', 0.69740340471267703)
('epoch', 40, 'train_loss:', 2.7855531930923463, 'val_loss:', 0.69655388116836547)
('epoch', 41, 'train_loss:', 2.7802477025985719, 'val_loss:', 0.69433588027954096)
('epoch', 42, 'train_loss:', 2.769601583480835, 'val_loss:', 0.69363399744033816)
('epoch', 43, 'train_loss:', 2.7726679039001465, 'val_loss:', 0.69342488050460815)
('epoch', 44, 'train_loss:', 2.7621193289756776, 'val_loss:', 0.69142555236816405)
('epoch', 45, 'train_loss:', 2.7611272001266478, 'val_loss:', 0.68955734729766849)
('epoch', 46, 'train_loss:', 2.7588813948631286, 'val_loss:', 0.68715309858322149)
('epoch', 47, 'train_loss:', 2.7534043765068055, 'val_loss:', 0.68875198125839232)
('epoch', 48, 'train_loss:', 2.7459675455093384, 'val_loss:', 0.68738204956054683)
('epoch', 49, 'train_loss:', 2.7430337524414061, 'val_loss:', 0.68725938320159907)
('epoch', 50, 'train_loss:', 2.7376224064826964, 'val_loss:', 0.68582065105438228)
('epoch', 51, 'train_loss:', 2.7338008975982664, 'val_loss:', 0.68509310007095336)
('epoch', 52, 'train_loss:', 2.7327061629295351, 'val_loss:', 0.68399243593215941)
('epoch', 53, 'train_loss:', 2.7291211128234862, 'val_loss:', 0.68269171476364132)
('epoch', 54, 'train_loss:', 2.7260952496528628, 'val_loss:', 0.68115767002105709)
('epoch', 55, 'train_loss:', 2.7212202334403992, 'val_loss:', 0.683163788318634)
('epoch', 56, 'train_loss:', 2.7186477518081666, 'val_loss:', 0.68060166120529175)
('epoch', 57, 'train_loss:', 2.7176820564270021, 'val_loss:', 0.67974680662155151)
('epoch', 58, 'train_loss:', 2.7089264583587647, 'val_loss:', 0.67823092222213743)
('epoch', 59, 'train_loss:', 2.7097964310646057, 'val_loss:', 0.67956126928329463)
('epoch', 60, 'train_loss:', 2.7052390742301942, 'val_loss:', 0.67816259622573849)
('epoch', 61, 'train_loss:', 2.70366272687912, 'val_loss:', 0.67709852218627931)
('epoch', 62, 'train_loss:', 2.69877863407135, 'val_loss:', 0.67545110702514644)
('epoch', 63, 'train_loss:', 2.6996879601478576, 'val_loss:', 0.67470928430557253)
('epoch', 64, 'train_loss:', 2.6950035357475279, 'val_loss:', 0.67451621055603028)
('epoch', 65, 'train_loss:', 2.6923277473449705, 'val_loss:', 0.67320103168487544)
('epoch', 66, 'train_loss:', 2.6869046139717101, 'val_loss:', 0.67269219398498536)
('epoch', 67, 'train_loss:', 2.6875181460380553, 'val_loss:', 0.67226814031600957)
('epoch', 68, 'train_loss:', 2.6838168931007385, 'val_loss:', 0.67189797878265378)
('epoch', 69, 'train_loss:', 2.6818270421028139, 'val_loss:', 0.67135881662368779)
('epoch', 70, 'train_loss:', 2.676641433238983, 'val_loss:', 0.6721789956092834)
('epoch', 71, 'train_loss:', 2.6776964855194092, 'val_loss:', 0.67177945852279664)
('epoch', 72, 'train_loss:', 2.6726619076728819, 'val_loss:', 0.66904790878295894)
('epoch', 73, 'train_loss:', 2.6684206128120422, 'val_loss:', 0.66890396595001222)
('epoch', 74, 'train_loss:', 2.6688276505470276, 'val_loss:', 0.66952321529388426)
('epoch', 75, 'train_loss:', 2.6673598289489746, 'val_loss:', 0.6682171416282654)
('epoch', 76, 'train_loss:', 2.6637938618659973, 'val_loss:', 0.66667697668075565)
('epoch', 77, 'train_loss:', 2.6622621107101438, 'val_loss:', 0.66542312860488895)
('epoch', 78, 'train_loss:', 2.6620374655723573, 'val_loss:', 0.66387051343917847)
('epoch', 79, 'train_loss:', 2.6571447157859804, 'val_loss:', 0.66491728544235229)
('epoch', 80, 'train_loss:', 2.6529614758491515, 'val_loss:', 0.665164680480957)
('epoch', 81, 'train_loss:', 2.6507639455795289, 'val_loss:', 0.66374665498733521)
('epoch', 82, 'train_loss:', 2.6493339061737062, 'val_loss:', 0.6646928548812866)
('epoch', 83, 'train_loss:', 2.6479232597351072, 'val_loss:', 0.66175640106201172)
('epoch', 84, 'train_loss:', 2.6483068227767945, 'val_loss:', 0.66160954475402833)
('epoch', 85, 'train_loss:', 2.6401939153671266, 'val_loss:', 0.66071719646453853)
('epoch', 86, 'train_loss:', 2.6388380336761474, 'val_loss:', 0.66216541290283204)
('epoch', 87, 'train_loss:', 2.636339478492737, 'val_loss:', 0.66012446165084837)
('epoch', 88, 'train_loss:', 2.6367585158348081, 'val_loss:', 0.65982320308685305)
('epoch', 89, 'train_loss:', 2.6346610093116762, 'val_loss:', 0.65957354545593261)
('epoch', 90, 'train_loss:', 2.6322895526885985, 'val_loss:', 0.65812895298004148)
('epoch', 91, 'train_loss:', 2.6291957736015319, 'val_loss:', 0.657741231918335)
('epoch', 92, 'train_loss:', 2.6260634279251098, 'val_loss:', 0.65686692476272579)
('epoch', 93, 'train_loss:', 2.6268052887916564, 'val_loss:', 0.65733697652816769)
('epoch', 94, 'train_loss:', 2.6236551976203919, 'val_loss:', 0.65659755706787104)
('epoch', 95, 'train_loss:', 2.6213461804389953, 'val_loss:', 0.65623310089111331)
('epoch', 96, 'train_loss:', 2.6187171077728273, 'val_loss:', 0.65619243383407588)
('epoch', 97, 'train_loss:', 2.6154584550857543, 'val_loss:', 0.65499362230300906)
('epoch', 98, 'train_loss:', 2.6164327979087831, 'val_loss:', 0.65531987667083735)
('epoch', 99, 'train_loss:', 2.6106435728073119, 'val_loss:', 0.65285986900329585)
('epoch', 100, 'train_loss:', 2.6117165470123291, 'val_loss:', 0.65338334083557126)
('epoch', 101, 'train_loss:', 2.6093840932846071, 'val_loss:', 0.65439386844635006)
('epoch', 102, 'train_loss:', 2.6031456017494201, 'val_loss:', 0.65208339214324951)
('epoch', 103, 'train_loss:', 2.6018148946762083, 'val_loss:', 0.65163403511047369)
('epoch', 104, 'train_loss:', 2.599469223022461, 'val_loss:', 0.65177248239517216)
('epoch', 105, 'train_loss:', 2.5983269166946412, 'val_loss:', 0.65149297714233401)
('epoch', 106, 'train_loss:', 2.6026723647117613, 'val_loss:', 0.65028750658035284)
('epoch', 107, 'train_loss:', 2.5941426444053648, 'val_loss:', 0.65143402814865115)
('epoch', 108, 'train_loss:', 2.5961183404922483, 'val_loss:', 0.64898063182830812)
('epoch', 109, 'train_loss:', 2.5937931871414186, 'val_loss:', 0.65016307830810549)
('epoch', 110, 'train_loss:', 2.5900337934494018, 'val_loss:', 0.64643328666687017)
('epoch', 111, 'train_loss:', 2.5828099131584166, 'val_loss:', 0.64812177658081049)
('epoch', 112, 'train_loss:', 2.5882897281646731, 'val_loss:', 0.64839927434921263)
('epoch', 113, 'train_loss:', 2.5822001051902772, 'val_loss:', 0.64502085685729982)
('epoch', 114, 'train_loss:', 2.5795403051376344, 'val_loss:', 0.64512851953506467)
('epoch', 115, 'train_loss:', 2.5809810829162596, 'val_loss:', 0.64707542419433595)
('epoch', 116, 'train_loss:', 2.5760610055923463, 'val_loss:', 0.64520156145095831)
('epoch', 117, 'train_loss:', 2.5736935138702393, 'val_loss:', 0.64522182464599609)
('epoch', 118, 'train_loss:', 2.5766211342811585, 'val_loss:', 0.64458675861358639)
('epoch', 119, 'train_loss:', 2.5759055137634279, 'val_loss:', 0.64257332086563113)
('epoch', 120, 'train_loss:', 2.5691599488258361, 'val_loss:', 0.64400175809860227)
('epoch', 121, 'train_loss:', 2.5662244892120363, 'val_loss:', 0.64298239231109622)
('epoch', 122, 'train_loss:', 2.5673875665664672, 'val_loss:', 0.64267833709716793)
('epoch', 123, 'train_loss:', 2.5662406063079835, 'val_loss:', 0.6430194711685181)
('epoch', 124, 'train_loss:', 2.5622377872467039, 'val_loss:', 0.64097030639648434)
('epoch', 125, 'train_loss:', 2.5589934682846067, 'val_loss:', 0.64092885494232177)
('epoch', 126, 'train_loss:', 2.5591221785545351, 'val_loss:', 0.64106271505355839)
('epoch', 127, 'train_loss:', 2.5568096756935121, 'val_loss:', 0.63984791755676274)
('epoch', 128, 'train_loss:', 2.5577593207359315, 'val_loss:', 0.63898728609085087)
('epoch', 129, 'train_loss:', 2.5533485531806948, 'val_loss:', 0.64017088174819947)
('epoch', 130, 'train_loss:', 2.5526289463043215, 'val_loss:', 0.63768952846527105)
('epoch', 131, 'train_loss:', 2.5505605649948122, 'val_loss:', 0.63902649879455564)
('epoch', 132, 'train_loss:', 2.5499405765533449, 'val_loss:', 0.63707968711853025)
('epoch', 133, 'train_loss:', 2.5449871921539309, 'val_loss:', 0.63715702533721918)
('epoch', 134, 'train_loss:', 2.5444549703598023, 'val_loss:', 0.63575837850570682)
('epoch', 135, 'train_loss:', 2.5415247797966005, 'val_loss:', 0.63469588994979853)
('epoch', 136, 'train_loss:', 2.5393828177452087, 'val_loss:', 0.63684789180755619)
('epoch', 137, 'train_loss:', 2.5403343391418458, 'val_loss:', 0.63707805633544923)
('epoch', 138, 'train_loss:', 2.536911702156067, 'val_loss:', 0.63567981958389286)
('epoch', 139, 'train_loss:', 2.5333767724037171, 'val_loss:', 0.63612618207931515)
('epoch', 140, 'train_loss:', 2.5336430168151853, 'val_loss:', 0.6352186441421509)
('epoch', 141, 'train_loss:', 2.5358425784111023, 'val_loss:', 0.63230255365371701)
('epoch', 142, 'train_loss:', 2.5300452589988707, 'val_loss:', 0.63263988018035888)
('epoch', 143, 'train_loss:', 2.5310995864868162, 'val_loss:', 0.63320419788360593)
('epoch', 144, 'train_loss:', 2.524541163444519, 'val_loss:', 0.63232543230056759)
('epoch', 145, 'train_loss:', 2.5234971356391909, 'val_loss:', 0.63110292911529542)
('epoch', 146, 'train_loss:', 2.5211673212051391, 'val_loss:', 0.63021506786346437)
('epoch', 147, 'train_loss:', 2.5189585018157961, 'val_loss:', 0.62912867784500126)
('epoch', 148, 'train_loss:', 2.5214524030685426, 'val_loss:', 0.63152291059494015)
('epoch', 149, 'train_loss:', 2.5184255266189575, 'val_loss:', 0.63016146898269654)
('epoch', 150, 'train_loss:', 2.5122673082351685, 'val_loss:', 0.62855939149856566)
('epoch', 151, 'train_loss:', 2.5132531785964964, 'val_loss:', 0.62935468912124637)
('epoch', 152, 'train_loss:', 2.5128347611427309, 'val_loss:', 0.62883461952209474)
('epoch', 153, 'train_loss:', 2.5090155053138732, 'val_loss:', 0.62824454307556155)
('epoch', 154, 'train_loss:', 2.5103871870040892, 'val_loss:', 0.62723563194274901)
('epoch', 155, 'train_loss:', 2.5043519139289856, 'val_loss:', 0.62564440965652468)
('epoch', 156, 'train_loss:', 2.5019927668571471, 'val_loss:', 0.626305410861969)
('epoch', 157, 'train_loss:', 2.5044621396064759, 'val_loss:', 0.62670710563659671)
('epoch', 158, 'train_loss:', 2.4965254449844361, 'val_loss:', 0.62603379249572755)
('epoch', 159, 'train_loss:', 2.4982445287704467, 'val_loss:', 0.62612369775772092)
('epoch', 160, 'train_loss:', 2.4948139858245848, 'val_loss:', 0.62516772508621221)
('epoch', 161, 'train_loss:', 2.4957502269744873, 'val_loss:', 0.62407242536544805)
('epoch', 162, 'train_loss:', 2.4930663204193113, 'val_loss:', 0.62404886007308957)
('epoch', 163, 'train_loss:', 2.4940724492073061, 'val_loss:', 0.62313244819641112)
('epoch', 164, 'train_loss:', 2.4862689852714537, 'val_loss:', 0.62348319768905636)
('epoch', 165, 'train_loss:', 2.4896406197547911, 'val_loss:', 0.62301943778991697)
('epoch', 166, 'train_loss:', 2.4861022663116454, 'val_loss:', 0.62145514249801637)
('epoch', 167, 'train_loss:', 2.4830391263961791, 'val_loss:', 0.62108024835586551)
('epoch', 168, 'train_loss:', 2.4794255805015566, 'val_loss:', 0.6206836581230164)
('epoch', 169, 'train_loss:', 2.4801691126823426, 'val_loss:', 0.61910917282104494)
('epoch', 170, 'train_loss:', 2.478754460811615, 'val_loss:', 0.61982467412948605)
('epoch', 171, 'train_loss:', 2.4734662103652956, 'val_loss:', 0.6189609503746033)
('epoch', 172, 'train_loss:', 2.4772846508026123, 'val_loss:', 0.61902051925659185)
('epoch', 173, 'train_loss:', 2.4734283709526061, 'val_loss:', 0.61850486993789677)
('epoch', 174, 'train_loss:', 2.4674615788459779, 'val_loss:', 0.61772692918777461)
('epoch', 175, 'train_loss:', 2.4682132983207703, 'val_loss:', 0.61715372085571285)
('epoch', 176, 'train_loss:', 2.4678095221519469, 'val_loss:', 0.61772239685058594)
('epoch', 177, 'train_loss:', 2.4648886203765867, 'val_loss:', 0.6174227833747864)
('epoch', 178, 'train_loss:', 2.4634375691413881, 'val_loss:', 0.61518237352371219)
('epoch', 179, 'train_loss:', 2.4608326125144959, 'val_loss:', 0.61513030290603643)
('epoch', 180, 'train_loss:', 2.4589191412925722, 'val_loss:', 0.61591529130935674)
('epoch', 181, 'train_loss:', 2.4578490209579469, 'val_loss:', 0.61450169563293455)
('epoch', 182, 'train_loss:', 2.4552760076522828, 'val_loss:', 0.61437481403350835)
('epoch', 183, 'train_loss:', 2.4560459327697752, 'val_loss:', 0.61279186248779294)
('epoch', 184, 'train_loss:', 2.4540193843841553, 'val_loss:', 0.61239961385726926)
('epoch', 185, 'train_loss:', 2.4518198251724241, 'val_loss:', 0.61283739566802975)
('epoch', 186, 'train_loss:', 2.4510053324699403, 'val_loss:', 0.61296617031097411)
('epoch', 187, 'train_loss:', 2.4480898404121398, 'val_loss:', 0.61115912675857542)
('epoch', 188, 'train_loss:', 2.4464213013648988, 'val_loss:', 0.61252817869186404)
('epoch', 189, 'train_loss:', 2.4461685156822206, 'val_loss:', 0.61147839784622193)
('epoch', 190, 'train_loss:', 2.4427776598930357, 'val_loss:', 0.61029575586318974)
('epoch', 191, 'train_loss:', 2.4416574144363405, 'val_loss:', 0.61069539308547971)
('epoch', 192, 'train_loss:', 2.437238965034485, 'val_loss:', 0.61175923347473149)
('epoch', 193, 'train_loss:', 2.4399050974845888, 'val_loss:', 0.61042204856872562)
('epoch', 194, 'train_loss:', 2.4361065053939819, 'val_loss:', 0.61055063486099248)
('epoch', 195, 'train_loss:', 2.4359985947608949, 'val_loss:', 0.60949051380157471)
('epoch', 196, 'train_loss:', 2.4320755410194397, 'val_loss:', 0.60957785844802859)
('epoch', 197, 'train_loss:', 2.4324880075454711, 'val_loss:', 0.60763303279876713)
('epoch', 198, 'train_loss:', 2.4299039387702943, 'val_loss:', 0.60840976715087891)
('epoch', 199, 'train_loss:', 2.4296877074241636, 'val_loss:', 0.60878143072128299)
('epoch', 200, 'train_loss:', 2.4259853696823122, 'val_loss:', 0.60740544795989992)
('epoch', 201, 'train_loss:', 2.4273670244216921, 'val_loss:', 0.60643852233886719)
('epoch', 202, 'train_loss:', 2.4261990928649904, 'val_loss:', 0.60594051122665404)
('epoch', 203, 'train_loss:', 2.4187455153465272, 'val_loss:', 0.60550171375274653)
('epoch', 204, 'train_loss:', 2.4157533764839174, 'val_loss:', 0.60668888330459592)
('epoch', 205, 'train_loss:', 2.4209818053245544, 'val_loss:', 0.60584969758987428)
('epoch', 206, 'train_loss:', 2.4195053648948668, 'val_loss:', 0.60459655761718745)
('epoch', 207, 'train_loss:', 2.4214201188087463, 'val_loss:', 0.60514447689056394)
('epoch', 208, 'train_loss:', 2.4129378366470338, 'val_loss:', 0.60394042968749995)
('epoch', 209, 'train_loss:', 2.4147256064414977, 'val_loss:', 0.60551034450531005)
('epoch', 210, 'train_loss:', 2.4137257766723632, 'val_loss:', 0.60400238037109377)
('epoch', 211, 'train_loss:', 2.4108228278160095, 'val_loss:', 0.6023166155815125)
('epoch', 212, 'train_loss:', 2.4085056805610656, 'val_loss:', 0.60464041233062749)
('epoch', 213, 'train_loss:', 2.4109446692466734, 'val_loss:', 0.60416563034057613)
('epoch', 214, 'train_loss:', 2.4062591719627382, 'val_loss:', 0.6008746957778931)
('epoch', 215, 'train_loss:', 2.4039811682701111, 'val_loss:', 0.60154112339019772)
('epoch', 216, 'train_loss:', 2.4028355693817138, 'val_loss:', 0.60047767639160154)
('epoch', 217, 'train_loss:', 2.4010962510108946, 'val_loss:', 0.60039415597915646)
('epoch', 218, 'train_loss:', 2.397194256782532, 'val_loss:', 0.60140767335891721)
('epoch', 219, 'train_loss:', 2.3953262376785278, 'val_loss:', 0.59891178131103517)
('epoch', 220, 'train_loss:', 2.3946904850006105, 'val_loss:', 0.59885017633438109)
('epoch', 221, 'train_loss:', 2.3947188305854796, 'val_loss:', 0.59932383060455319)
('epoch', 222, 'train_loss:', 2.3936246395111085, 'val_loss:', 0.59848294496536258)
('epoch', 223, 'train_loss:', 2.3915842056274412, 'val_loss:', 0.59673615694046023)
('epoch', 224, 'train_loss:', 2.3882187986373902, 'val_loss:', 0.59879509925842289)
('epoch', 225, 'train_loss:', 2.3913487601280212, 'val_loss:', 0.59650448560714719)
('epoch', 226, 'train_loss:', 2.3888788962364198, 'val_loss:', 0.59897339582443232)
('epoch', 227, 'train_loss:', 2.3897362041473387, 'val_loss:', 0.59696606397628782)
('epoch', 228, 'train_loss:', 2.3851263332366943, 'val_loss:', 0.5962291288375855)
('epoch', 229, 'train_loss:', 2.381481213569641, 'val_loss:', 0.59489254474639897)
('epoch', 230, 'train_loss:', 2.3804264569282534, 'val_loss:', 0.59484911441802979)
('epoch', 231, 'train_loss:', 2.3800629353523255, 'val_loss:', 0.59602835655212405)
('epoch', 232, 'train_loss:', 2.3774113976955413, 'val_loss:', 0.59489849328994748)
('epoch', 233, 'train_loss:', 2.3767087495326997, 'val_loss:', 0.59607188940048217)
('epoch', 234, 'train_loss:', 2.3733219826221466, 'val_loss:', 0.59430305719375609)
('epoch', 235, 'train_loss:', 2.3713943028450011, 'val_loss:', 0.59469204664230346)
('epoch', 236, 'train_loss:', 2.3689479851722717, 'val_loss:', 0.5942683219909668)
('epoch', 237, 'train_loss:', 2.370810649394989, 'val_loss:', 0.59375083208084112)
('epoch', 238, 'train_loss:', 2.369331614971161, 'val_loss:', 0.59298271179199213)
('epoch', 239, 'train_loss:', 2.3668045306205752, 'val_loss:', 0.59414484739303586)
('epoch', 240, 'train_loss:', 2.3644057369232176, 'val_loss:', 0.59189863443374635)
('epoch', 241, 'train_loss:', 2.3639022755622863, 'val_loss:', 0.59154191732406614)
('epoch', 242, 'train_loss:', 2.3613656079769134, 'val_loss:', 0.5923916411399841)
('epoch', 243, 'train_loss:', 2.3621595263481141, 'val_loss:', 0.59385052680969241)
('epoch', 244, 'train_loss:', 2.3568885171413423, 'val_loss:', 0.59106899023056025)
('epoch', 245, 'train_loss:', 2.3587223684787748, 'val_loss:', 0.59083843708038331)
('epoch', 246, 'train_loss:', 2.3564681363105775, 'val_loss:', 0.59103675365447994)
('epoch', 247, 'train_loss:', 2.3552616035938261, 'val_loss:', 0.58845071554183959)
('epoch', 248, 'train_loss:', 2.3585694968700408, 'val_loss:', 0.58677307844161986)
('epoch', 249, 'train_loss:', 2.3533032214641572, 'val_loss:', 0.58792447209358212)
('epoch', 250, 'train_loss:', 2.3521349632740023, 'val_loss:', 0.58973017454147336)
('epoch', 251, 'train_loss:', 2.3501531875133512, 'val_loss:', 0.58776162505149843)
('epoch', 252, 'train_loss:', 2.3483790409564973, 'val_loss:', 0.58759212255477911)
('epoch', 253, 'train_loss:', 2.3469358122348787, 'val_loss:', 0.58702631235122682)
('epoch', 254, 'train_loss:', 2.3444387388229369, 'val_loss:', 0.58824473381042486)
('epoch', 255, 'train_loss:', 2.3423725581169128, 'val_loss:', 0.58616756439208983)
('epoch', 256, 'train_loss:', 2.3474666821956633, 'val_loss:', 0.58723709583282468)
('epoch', 257, 'train_loss:', 2.3450226032733918, 'val_loss:', 0.58545431256294256)
('epoch', 258, 'train_loss:', 2.3412075412273405, 'val_loss:', 0.58734900236129761)
('epoch', 259, 'train_loss:', 2.340916689634323, 'val_loss:', 0.5869651818275452)
('epoch', 260, 'train_loss:', 2.3412287783622743, 'val_loss:', 0.58405557632446292)
('epoch', 261, 'train_loss:', 2.3351610124111177, 'val_loss:', 0.58522008895874023)
('epoch', 262, 'train_loss:', 2.3356990265846251, 'val_loss:', 0.58558106064796445)
('epoch', 263, 'train_loss:', 2.3348919558525085, 'val_loss:', 0.58288586854934687)
('epoch', 264, 'train_loss:', 2.3335973203182219, 'val_loss:', 0.58513338446617125)
('epoch', 265, 'train_loss:', 2.3275208413600921, 'val_loss:', 0.58456244230270382)
('epoch', 266, 'train_loss:', 2.3299967038631437, 'val_loss:', 0.58354817152023319)
('epoch', 267, 'train_loss:', 2.3304694402217865, 'val_loss:', 0.583780061006546)
('epoch', 268, 'train_loss:', 2.327173743247986, 'val_loss:', 0.58246096968650818)
('epoch', 269, 'train_loss:', 2.3254223716259004, 'val_loss:', 0.5823192799091339)
('epoch', 270, 'train_loss:', 2.3272947394847869, 'val_loss:', 0.58259277820587163)
('epoch', 271, 'train_loss:', 2.3231001150608064, 'val_loss:', 0.58090421080589294)
('epoch', 272, 'train_loss:', 2.3199260759353639, 'val_loss:', 0.58115753412246707)
('epoch', 273, 'train_loss:', 2.321430493593216, 'val_loss:', 0.58156873583793645)
('epoch', 274, 'train_loss:', 2.3206425106525419, 'val_loss:', 0.58060706496238712)
('epoch', 275, 'train_loss:', 2.3180962514877321, 'val_loss:', 0.58064765453338618)
('epoch', 276, 'train_loss:', 2.3155889403820038, 'val_loss:', 0.57854798078536984)
('epoch', 277, 'train_loss:', 2.316156768798828, 'val_loss:', 0.57960388541221619)
('epoch', 278, 'train_loss:', 2.314959260225296, 'val_loss:', 0.57852024078369135)
('epoch', 279, 'train_loss:', 2.3121327292919158, 'val_loss:', 0.57962838053703303)
('epoch', 280, 'train_loss:', 2.3073443353176115, 'val_loss:', 0.57907613754272458)
('epoch', 281, 'train_loss:', 2.3083520245552065, 'val_loss:', 0.57924080014228818)
('epoch', 282, 'train_loss:', 2.3041863930225372, 'val_loss:', 0.57780871868133543)
('epoch', 283, 'train_loss:', 2.3067509889602662, 'val_loss:', 0.57773535251617436)
('epoch', 284, 'train_loss:', 2.3058071720600126, 'val_loss:', 0.5780796921253204)
('epoch', 285, 'train_loss:', 2.3061448287963868, 'val_loss:', 0.57604471087455744)
('epoch', 286, 'train_loss:', 2.299547986984253, 'val_loss:', 0.57659816145896914)
('epoch', 287, 'train_loss:', 2.2988994669914247, 'val_loss:', 0.57696219921112057)
('epoch', 288, 'train_loss:', 2.29816934466362, 'val_loss:', 0.57691350936889652)
('epoch', 289, 'train_loss:', 2.2965270888805391, 'val_loss:', 0.57522398352622983)
('epoch', 290, 'train_loss:', 2.2937156093120574, 'val_loss:', 0.57643749713897707)
('epoch', 291, 'train_loss:', 2.2944712948799135, 'val_loss:', 0.57507049202919003)
('epoch', 292, 'train_loss:', 2.2940887475013731, 'val_loss:', 0.5743934834003448)
('epoch', 293, 'train_loss:', 2.2943047976493833, 'val_loss:', 0.57573362350463864)
('epoch', 294, 'train_loss:', 2.2899763536453248, 'val_loss:', 0.57255907773971559)
('epoch', 295, 'train_loss:', 2.2878388261795042, 'val_loss:', 0.57421803832054141)
('epoch', 296, 'train_loss:', 2.2867809915542603, 'val_loss:', 0.57283176541328429)
('epoch', 297, 'train_loss:', 2.2872004759311677, 'val_loss:', 0.57382310152053828)
('epoch', 298, 'train_loss:', 2.2842491364479063, 'val_loss:', 0.5726549315452576)
('epoch', 299, 'train_loss:', 2.2828600502014158, 'val_loss:', 0.57359543323516848)
('epoch', 300, 'train_loss:', 2.2816633939743043, 'val_loss:', 0.57324691414833073)
('epoch', 301, 'train_loss:', 2.2813409769535067, 'val_loss:', 0.57088953733444214)
('epoch', 302, 'train_loss:', 2.277341629266739, 'val_loss:', 0.57160774350166321)
('epoch', 303, 'train_loss:', 2.2782837712764739, 'val_loss:', 0.57275407075881957)
('epoch', 304, 'train_loss:', 2.2813600969314574, 'val_loss:', 0.57089841723442081)
('epoch', 305, 'train_loss:', 2.2780075120925902, 'val_loss:', 0.57185943841934206)
('epoch', 306, 'train_loss:', 2.2760469877719878, 'val_loss:', 0.56957148194313045)
('epoch', 307, 'train_loss:', 2.2741279292106626, 'val_loss:', 0.57014053344726567)
('epoch', 308, 'train_loss:', 2.2777912545204164, 'val_loss:', 0.57035470128059385)
('epoch', 309, 'train_loss:', 2.2718395507335662, 'val_loss:', 0.56930596709251402)
('epoch', 310, 'train_loss:', 2.2680210244655608, 'val_loss:', 0.5702105331420898)
('epoch', 311, 'train_loss:', 2.2645944106578826, 'val_loss:', 0.571147449016571)
('epoch', 312, 'train_loss:', 2.2669557917118071, 'val_loss:', 0.56913883686065669)
('epoch', 313, 'train_loss:', 2.2651392042636873, 'val_loss:', 0.56881138682365417)
('epoch', 314, 'train_loss:', 2.2629497897624971, 'val_loss:', 0.56736449837684633)
('epoch', 315, 'train_loss:', 2.2627532398700714, 'val_loss:', 0.56798086762428279)
('epoch', 316, 'train_loss:', 2.2632680487632753, 'val_loss:', 0.5682076406478882)
('epoch', 317, 'train_loss:', 2.2611782395839692, 'val_loss:', 0.56600821375846866)
('epoch', 318, 'train_loss:', 2.2603343582153319, 'val_loss:', 0.56677506685256962)
('epoch', 319, 'train_loss:', 2.2575531840324401, 'val_loss:', 0.5650835299491882)
('epoch', 320, 'train_loss:', 2.2533842194080353, 'val_loss:', 0.56512247204780575)
('epoch', 321, 'train_loss:', 2.2589826595783236, 'val_loss:', 0.56470384120941164)
('epoch', 322, 'train_loss:', 2.2580279016494753, 'val_loss:', 0.56723639726638797)
('epoch', 323, 'train_loss:', 2.257283365726471, 'val_loss:', 0.56443297028541561)
('epoch', 324, 'train_loss:', 2.2503006947040558, 'val_loss:', 0.56527562141418453)
('epoch', 325, 'train_loss:', 2.2534855675697325, 'val_loss:', 0.5652776432037353)
('epoch', 326, 'train_loss:', 2.2534057664871217, 'val_loss:', 0.56526511549949643)
('epoch', 327, 'train_loss:', 2.2495870566368104, 'val_loss:', 0.56504887700080875)
('epoch', 328, 'train_loss:', 2.2466716814041137, 'val_loss:', 0.56164477705955507)
('epoch', 329, 'train_loss:', 2.2460461390018462, 'val_loss:', 0.5641671323776245)
('epoch', 330, 'train_loss:', 2.2445828723907471, 'val_loss:', 0.56097214341163637)
('epoch', 331, 'train_loss:', 2.2462378334999085, 'val_loss:', 0.56395521163940432)
('epoch', 332, 'train_loss:', 2.2454698693752291, 'val_loss:', 0.56417320609092714)
('epoch', 333, 'train_loss:', 2.2370429408550261, 'val_loss:', 0.56203018546104433)
('epoch', 334, 'train_loss:', 2.2389741253852846, 'val_loss:', 0.56265009522438048)
('epoch', 335, 'train_loss:', 2.2404023146629335, 'val_loss:', 0.56184873700141902)
('epoch', 336, 'train_loss:', 2.2390996384620667, 'val_loss:', 0.56263257026672364)
('epoch', 337, 'train_loss:', 2.2364078235626219, 'val_loss:', 0.56062273025512699)
('epoch', 338, 'train_loss:', 2.2346098530292511, 'val_loss:', 0.56090589046478267)
('epoch', 339, 'train_loss:', 2.2348273682594297, 'val_loss:', 0.56051326632499698)
('epoch', 340, 'train_loss:', 2.2352498531341554, 'val_loss:', 0.5611009442806244)
('epoch', 341, 'train_loss:', 2.2318160474300384, 'val_loss:', 0.55853568911552431)
('epoch', 342, 'train_loss:', 2.234231721162796, 'val_loss:', 0.55952741622924806)
('epoch', 343, 'train_loss:', 2.2322244346141815, 'val_loss:', 0.5586857378482819)
('epoch', 344, 'train_loss:', 2.2264731633663177, 'val_loss:', 0.55969091534614568)
('epoch', 345, 'train_loss:', 2.2297406923770904, 'val_loss:', 0.56051186919212337)
('epoch', 346, 'train_loss:', 2.2259120523929594, 'val_loss:', 0.55928681612014775)
('epoch', 347, 'train_loss:', 2.2281660842895508, 'val_loss:', 0.55873036146163946)
('epoch', 348, 'train_loss:', 2.2229527032375334, 'val_loss:', 0.55869790792465213)
('epoch', 349, 'train_loss:', 2.2218880414962769, 'val_loss:', 0.55879770040512089)
('epoch', 350, 'train_loss:', 2.2222615206241607, 'val_loss:', 0.5579696583747864)
('epoch', 351, 'train_loss:', 2.2194949448108674, 'val_loss:', 0.55683969736099248)
('epoch', 352, 'train_loss:', 2.2186273252964019, 'val_loss:', 0.555740875005722)
('epoch', 353, 'train_loss:', 2.2179142236709595, 'val_loss:', 0.55757049918174739)
('epoch', 354, 'train_loss:', 2.216710693836212, 'val_loss:', 0.55593218922615051)
('epoch', 355, 'train_loss:', 2.2145252323150633, 'val_loss:', 0.55769288659095761)
('epoch', 356, 'train_loss:', 2.2158251857757567, 'val_loss:', 0.55666605830192561)
('epoch', 357, 'train_loss:', 2.2130207443237304, 'val_loss:', 0.55336950421333309)
('epoch', 358, 'train_loss:', 2.2098990607261659, 'val_loss:', 0.55568998575210571)
('epoch', 359, 'train_loss:', 2.2125806939601897, 'val_loss:', 0.55595605969429018)
('epoch', 360, 'train_loss:', 2.2078597950935364, 'val_loss:', 0.55451029062271118)
('epoch', 361, 'train_loss:', 2.2116167879104616, 'val_loss:', 0.55438610553741452)
('epoch', 362, 'train_loss:', 2.2063215768337248, 'val_loss:', 0.55387004017829899)
('epoch', 363, 'train_loss:', 2.2049637806415556, 'val_loss:', 0.55328460574150085)
('epoch', 364, 'train_loss:', 2.2048124384880068, 'val_loss:', 0.55111460685729985)
('epoch', 365, 'train_loss:', 2.203635697364807, 'val_loss:', 0.5543537271022797)
('epoch', 366, 'train_loss:', 2.2043570566177366, 'val_loss:', 0.55293805599212642)
('epoch', 367, 'train_loss:', 2.2006323099136353, 'val_loss:', 0.55199249386787419)
('epoch', 368, 'train_loss:', 2.2016743111610411, 'val_loss:', 0.5526139152050018)
('epoch', 369, 'train_loss:', 2.199587938785553, 'val_loss:', 0.55201677560806273)
('epoch', 370, 'train_loss:', 2.1980930244922638, 'val_loss:', 0.5530160284042358)
('epoch', 371, 'train_loss:', 2.201098996400833, 'val_loss:', 0.55068568825721742)
('epoch', 372, 'train_loss:', 2.1976449167728425, 'val_loss:', 0.55285728812217716)
('epoch', 373, 'train_loss:', 2.1934840202331545, 'val_loss:', 0.55017741203308101)
('epoch', 374, 'train_loss:', 2.1950374364852907, 'val_loss:', 0.55328051567077641)
('epoch', 375, 'train_loss:', 2.1973848867416383, 'val_loss:', 0.54948463201522824)
('epoch', 376, 'train_loss:', 2.1952676367759705, 'val_loss:', 0.55014119148254392)
('epoch', 377, 'train_loss:', 2.1891699016094206, 'val_loss:', 0.55061157226562496)
('epoch', 378, 'train_loss:', 2.1893494796752928, 'val_loss:', 0.54983025074005132)
('epoch', 379, 'train_loss:', 2.1906426477432253, 'val_loss:', 0.54880131125450138)
('epoch', 380, 'train_loss:', 2.1871269011497496, 'val_loss:', 0.54854795455932615)
('epoch', 381, 'train_loss:', 2.187722302675247, 'val_loss:', 0.54908709049224858)
('epoch', 382, 'train_loss:', 2.1860809934139254, 'val_loss:', 0.54858597636222839)
('epoch', 383, 'train_loss:', 2.1837734746932984, 'val_loss:', 0.54928057312965395)
('epoch', 384, 'train_loss:', 2.1860469484329226, 'val_loss:', 0.54947830200195313)
('epoch', 385, 'train_loss:', 2.1841636979579926, 'val_loss:', 0.54899616122245787)
('epoch', 386, 'train_loss:', 2.183090101480484, 'val_loss:', 0.5472759544849396)
('epoch', 387, 'train_loss:', 2.1786372220516204, 'val_loss:', 0.54697208881378179)
('epoch', 388, 'train_loss:', 2.1776233351230623, 'val_loss:', 0.54712762832641604)
('epoch', 389, 'train_loss:', 2.1771704113483428, 'val_loss:', 0.54698145270347598)
('epoch', 390, 'train_loss:', 2.1757102143764495, 'val_loss:', 0.54641172051429754)
('epoch', 391, 'train_loss:', 2.1752776670455933, 'val_loss:', 0.54498710751533508)
('epoch', 392, 'train_loss:', 2.1769332873821257, 'val_loss:', 0.54560197591781612)
('epoch', 393, 'train_loss:', 2.174624491930008, 'val_loss:', 0.54692857265472417)
('epoch', 394, 'train_loss:', 2.1732570469379424, 'val_loss:', 0.5467606866359711)
('epoch', 395, 'train_loss:', 2.17415181517601, 'val_loss:', 0.5465749537944794)
('epoch', 396, 'train_loss:', 2.1753555738925936, 'val_loss:', 0.546523529291153)
('epoch', 397, 'train_loss:', 2.1694243097305299, 'val_loss:', 0.54703076362609859)
('epoch', 398, 'train_loss:', 2.1713538467884064, 'val_loss:', 0.5453671205043793)
('epoch', 399, 'train_loss:', 2.1685884380340577, 'val_loss:', 0.54401689529418951)
('epoch', 400, 'train_loss:', 2.1683132362365725, 'val_loss:', 0.5446432292461395)
('epoch', 401, 'train_loss:', 2.1676888263225553, 'val_loss:', 0.54313240766525273)
('epoch', 402, 'train_loss:', 2.1655533051490785, 'val_loss:', 0.54180514216423037)
('epoch', 403, 'train_loss:', 2.1629575991630556, 'val_loss:', 0.54502568602561952)
('epoch', 404, 'train_loss:', 2.1637425208091736, 'val_loss:', 0.54444851875305178)
('epoch', 405, 'train_loss:', 2.160611706972122, 'val_loss:', 0.54288478016853337)
('epoch', 406, 'train_loss:', 2.160850511789322, 'val_loss:', 0.54199698925018314)
('epoch', 407, 'train_loss:', 2.1596165025234222, 'val_loss:', 0.54346333146095271)
('epoch', 408, 'train_loss:', 2.1609805691242219, 'val_loss:', 0.54325051546096803)
('epoch', 409, 'train_loss:', 2.1572870647907259, 'val_loss:', 0.54174586772918698)
('epoch', 410, 'train_loss:', 2.1604281997680665, 'val_loss:', 0.54174480795860291)
('epoch', 411, 'train_loss:', 2.1589021933078767, 'val_loss:', 0.5422310435771942)
('epoch', 412, 'train_loss:', 2.1584650492668152, 'val_loss:', 0.54196142673492431)
('epoch', 413, 'train_loss:', 2.155676383972168, 'val_loss:', 0.54109766840934759)
('epoch', 414, 'train_loss:', 2.1521542549133299, 'val_loss:', 0.54159474849700928)
('epoch', 415, 'train_loss:', 2.1550987601280212, 'val_loss:', 0.5426533448696137)
('epoch', 416, 'train_loss:', 2.1535177552700042, 'val_loss:', 0.54104068517684933)
('epoch', 417, 'train_loss:', 2.1503846991062163, 'val_loss:', 0.54114110946655269)
('epoch', 418, 'train_loss:', 2.1485418772697447, 'val_loss:', 0.54009918928146361)
('epoch', 419, 'train_loss:', 2.1472059798240664, 'val_loss:', 0.54124135375022886)
('epoch', 420, 'train_loss:', 2.1444316542148592, 'val_loss:', 0.54105721950531005)
('epoch', 421, 'train_loss:', 2.1471076178550721, 'val_loss:', 0.53783256649971012)
('epoch', 422, 'train_loss:', 2.149747134447098, 'val_loss:', 0.54071450829505918)
('epoch', 423, 'train_loss:', 2.1416846179962157, 'val_loss:', 0.53897040963172915)
('epoch', 424, 'train_loss:', 2.1456556165218355, 'val_loss:', 0.53763021588325499)
('epoch', 425, 'train_loss:', 2.1411899030208588, 'val_loss:', 0.53863575935363772)
('epoch', 426, 'train_loss:', 2.1421694445610044, 'val_loss:', 0.53909218668937686)
('epoch', 427, 'train_loss:', 2.1421064972877502, 'val_loss:', 0.53706749677658083)
('epoch', 428, 'train_loss:', 2.1403173816204073, 'val_loss:', 0.53681663870811458)
('epoch', 429, 'train_loss:', 2.1423510837554933, 'val_loss:', 0.53941409468650814)
('epoch', 430, 'train_loss:', 2.1423470854759215, 'val_loss:', 0.5378388011455536)
('epoch', 431, 'train_loss:', 2.1380639600753786, 'val_loss:', 0.53752066969871526)
('epoch', 432, 'train_loss:', 2.1390662074089049, 'val_loss:', 0.53747653365135195)
('epoch', 433, 'train_loss:', 2.1372222685813904, 'val_loss:', 0.53806630849838255)
('epoch', 434, 'train_loss:', 2.1347912895679473, 'val_loss:', 0.5366155624389648)
('epoch', 435, 'train_loss:', 2.1315324246883391, 'val_loss:', 0.53660266876220708)
('epoch', 436, 'train_loss:', 2.1315552854537962, 'val_loss:', 0.53588265299797055)
('epoch', 437, 'train_loss:', 2.1341611504554749, 'val_loss:', 0.53568565249443056)
('epoch', 438, 'train_loss:', 2.1291789233684542, 'val_loss:', 0.53732581257820133)
('epoch', 439, 'train_loss:', 2.130119470357895, 'val_loss:', 0.53621998667716975)
('epoch', 440, 'train_loss:', 2.128895593881607, 'val_loss:', 0.53498883724212642)
('epoch', 441, 'train_loss:', 2.1268776094913484, 'val_loss:', 0.53463134050369265)
('epoch', 442, 'train_loss:', 2.1275632619857787, 'val_loss:', 0.53384129166603089)
('epoch', 443, 'train_loss:', 2.125740729570389, 'val_loss:', 0.53517490267753598)
('epoch', 444, 'train_loss:', 2.1244368648529051, 'val_loss:', 0.53481613397598271)
('epoch', 445, 'train_loss:', 2.1222131240367887, 'val_loss:', 0.53346692800521855)
('epoch', 446, 'train_loss:', 2.1241769921779632, 'val_loss:', 0.53320343375205992)
('epoch', 447, 'train_loss:', 2.1213489079475405, 'val_loss:', 0.53480802655220028)
('epoch', 448, 'train_loss:', 2.1197150266170501, 'val_loss:', 0.53436715006828306)
('epoch', 449, 'train_loss:', 2.1213706207275389, 'val_loss:', 0.5331200110912323)
('epoch', 450, 'train_loss:', 2.1198002851009368, 'val_loss:', 0.53373573541641239)
('epoch', 451, 'train_loss:', 2.1154572820663451, 'val_loss:', 0.53349916696548461)
('epoch', 452, 'train_loss:', 2.1141296029090881, 'val_loss:', 0.53271155357360844)
('epoch', 453, 'train_loss:', 2.1201150083541869, 'val_loss:', 0.5309471380710602)
('epoch', 454, 'train_loss:', 2.1162921428680419, 'val_loss:', 0.5316496741771698)
('epoch', 455, 'train_loss:', 2.1154718554019927, 'val_loss:', 0.5319671380519867)
('epoch', 456, 'train_loss:', 2.1152853500843047, 'val_loss:', 0.53249669313430781)
('epoch', 457, 'train_loss:', 2.1142235887050629, 'val_loss:', 0.53051559567451478)
('epoch', 458, 'train_loss:', 2.1151841688156128, 'val_loss:', 0.53248548269271856)
('epoch', 459, 'train_loss:', 2.106322829723358, 'val_loss:', 0.5302035200595856)
('epoch', 460, 'train_loss:', 2.1123855268955229, 'val_loss:', 0.53090375900268549)
('epoch', 461, 'train_loss:', 2.1074476182460784, 'val_loss:', 0.5318468832969665)
('epoch', 462, 'train_loss:', 2.1086458778381347, 'val_loss:', 0.52933262467384334)
('epoch', 463, 'train_loss:', 2.1074267220497132, 'val_loss:', 0.5304209458827972)
('epoch', 464, 'train_loss:', 2.1079314255714419, 'val_loss:', 0.5302117204666138)
('epoch', 465, 'train_loss:', 2.1011315727233888, 'val_loss:', 0.53116072654724122)
('epoch', 466, 'train_loss:', 2.103684673309326, 'val_loss:', 0.52984436154365544)
('epoch', 467, 'train_loss:', 2.1048580813407898, 'val_loss:', 0.52998092770576477)
('epoch', 468, 'train_loss:', 2.1043754708766937, 'val_loss:', 0.52780606746673586)
('epoch', 469, 'train_loss:', 2.1033954036235811, 'val_loss:', 0.5295852816104889)
('epoch', 470, 'train_loss:', 2.1014564704895018, 'val_loss:', 0.52978210806846615)
('epoch', 471, 'train_loss:', 2.0983820748329163, 'val_loss:', 0.52854272723197937)
('epoch', 472, 'train_loss:', 2.100900092124939, 'val_loss:', 0.52871651053428648)
('epoch', 473, 'train_loss:', 2.0973714971542359, 'val_loss:', 0.52808174014091491)
('epoch', 474, 'train_loss:', 2.0965456843376158, 'val_loss:', 0.52775857210159305)
('epoch', 475, 'train_loss:', 2.0955691778659822, 'val_loss:', 0.5276545655727386)
('epoch', 476, 'train_loss:', 2.094866863489151, 'val_loss:', 0.52778533220291135)
('epoch', 477, 'train_loss:', 2.0941995120048524, 'val_loss:', 0.52734501719474791)
('epoch', 478, 'train_loss:', 2.0967302870750428, 'val_loss:', 0.52673889279365538)
('epoch', 479, 'train_loss:', 2.0929091668128965, 'val_loss:', 0.5263604342937469)
('epoch', 480, 'train_loss:', 2.0921047949790954, 'val_loss:', 0.52566958308219913)
('epoch', 481, 'train_loss:', 2.0884891211986543, 'val_loss:', 0.52707686424255373)
('epoch', 482, 'train_loss:', 2.0909037113189699, 'val_loss:', 0.52745744347572332)
('epoch', 483, 'train_loss:', 2.0897924351692199, 'val_loss:', 0.52728554844856257)
('epoch', 484, 'train_loss:', 2.0919432115554808, 'val_loss:', 0.5247188448905945)
('epoch', 485, 'train_loss:', 2.091684707403183, 'val_loss:', 0.52627528667449952)
('epoch', 486, 'train_loss:', 2.0874864614009856, 'val_loss:', 0.52659208536148072)
('epoch', 487, 'train_loss:', 2.0839525187015533, 'val_loss:', 0.52507244706153866)
('epoch', 488, 'train_loss:', 2.0871299862861634, 'val_loss:', 0.52439157247543333)
('epoch', 489, 'train_loss:', 2.0851748311519622, 'val_loss:', 0.52365648865699765)
('epoch', 490, 'train_loss:', 2.0857132256031035, 'val_loss:', 0.52435009121894838)
('epoch', 491, 'train_loss:', 2.0826966750621794, 'val_loss:', 0.52271777033805844)
('epoch', 492, 'train_loss:', 2.0818730723857879, 'val_loss:', 0.52576899647712705)
('epoch', 493, 'train_loss:', 2.0771881902217864, 'val_loss:', 0.52535252094268803)
('epoch', 494, 'train_loss:', 2.0765778875350951, 'val_loss:', 0.52390231847763058)
('epoch', 495, 'train_loss:', 2.0750407016277315, 'val_loss:', 0.52312803268432617)
('epoch', 496, 'train_loss:', 2.0796217525005343, 'val_loss:', 0.52365898013114931)
('epoch', 497, 'train_loss:', 2.0774307918548582, 'val_loss:', 0.52246137857437136)
('epoch', 498, 'train_loss:', 2.0769120442867277, 'val_loss:', 0.52317198634147644)
('epoch', 499, 'train_loss:', 2.0753101980686188, 'val_loss:', 0.5232381439208984)
('epoch', 500, 'train_loss:', 2.0748842668533327, 'val_loss:', 0.52240616083145142)
('epoch', 501, 'train_loss:', 2.0773705577850343, 'val_loss:', 0.5219195079803467)
('epoch', 502, 'train_loss:', 2.0759496581554413, 'val_loss:', 0.52350655317306516)
('epoch', 503, 'train_loss:', 2.0720187568664552, 'val_loss:', 0.52128704190254216)
('epoch', 504, 'train_loss:', 2.0726966166496279, 'val_loss:', 0.52171779036521915)
('epoch', 505, 'train_loss:', 2.0721094882488251, 'val_loss:', 0.51978359222412107)
('epoch', 506, 'train_loss:', 2.0718951237201693, 'val_loss:', 0.52276759982109067)
('epoch', 507, 'train_loss:', 2.070037888288498, 'val_loss:', 0.52057378411293032)
('epoch', 508, 'train_loss:', 2.0696910893917084, 'val_loss:', 0.51959885597229005)
('epoch', 509, 'train_loss:', 2.0662305438518525, 'val_loss:', 0.51941979527473447)
('epoch', 510, 'train_loss:', 2.0678570318222045, 'val_loss:', 0.52219427108764649)
('epoch', 511, 'train_loss:', 2.0683113586902619, 'val_loss:', 0.51854612708091741)
('epoch', 512, 'train_loss:', 2.0669628989696505, 'val_loss:', 0.51962885141372683)
('epoch', 513, 'train_loss:', 2.0659461677074433, 'val_loss:', 0.51816701531410214)
('epoch', 514, 'train_loss:', 2.0624488973617554, 'val_loss:', 0.51897583723068241)
('epoch', 515, 'train_loss:', 2.0642019748687743, 'val_loss:', 0.51907225847244265)
('epoch', 516, 'train_loss:', 2.0607000243663789, 'val_loss:', 0.51874071240425113)
('epoch', 517, 'train_loss:', 2.060235904455185, 'val_loss:', 0.5196562242507935)
('epoch', 518, 'train_loss:', 2.0581242847442627, 'val_loss:', 0.51833018660545349)
('epoch', 519, 'train_loss:', 2.0576896142959593, 'val_loss:', 0.51908944368362431)
('epoch', 520, 'train_loss:', 2.0591892671585081, 'val_loss:', 0.51966395616531369)
('epoch', 521, 'train_loss:', 2.0593945109844207, 'val_loss:', 0.5173538494110107)
('epoch', 522, 'train_loss:', 2.0583548879623415, 'val_loss:', 0.51605976581573487)
('epoch', 523, 'train_loss:', 2.0581657886505127, 'val_loss:', 0.51628902792930598)
('epoch', 524, 'train_loss:', 2.0573162102699278, 'val_loss:', 0.51653091549873353)
('epoch', 525, 'train_loss:', 2.0530614984035491, 'val_loss:', 0.51823373198509215)
('epoch', 526, 'train_loss:', 2.0514687621593475, 'val_loss:', 0.51759435296058653)
('epoch', 527, 'train_loss:', 2.0555628287792205, 'val_loss:', 0.51761669039726255)
('epoch', 528, 'train_loss:', 2.0532922184467317, 'val_loss:', 0.51732386708259581)
('epoch', 529, 'train_loss:', 2.0502540838718413, 'val_loss:', 0.5160381925106049)
('epoch', 530, 'train_loss:', 2.0524619579315186, 'val_loss:', 0.51678969860076907)
('epoch', 531, 'train_loss:', 2.0513197839260102, 'val_loss:', 0.51604361176490787)
('epoch', 532, 'train_loss:', 2.0503245258331297, 'val_loss:', 0.51641415119171141)
('epoch', 533, 'train_loss:', 2.0514292120933533, 'val_loss:', 0.51589562177658077)
('epoch', 534, 'train_loss:', 2.0460098195075989, 'val_loss:', 0.51524767160415652)
('epoch', 535, 'train_loss:', 2.0472545480728148, 'val_loss:', 0.51778282046318058)
('epoch', 536, 'train_loss:', 2.0491030418872835, 'val_loss:', 0.51521488070487975)
('epoch', 537, 'train_loss:', 2.0458085167407991, 'val_loss:', 0.51615931630134582)
('epoch', 538, 'train_loss:', 2.0444520461559295, 'val_loss:', 0.51743378758430481)
('epoch', 539, 'train_loss:', 2.0399316680431365, 'val_loss:', 0.51636284351348882)
('epoch', 540, 'train_loss:', 2.0425451517105104, 'val_loss:', 0.51375533938407902)
('epoch', 541, 'train_loss:', 2.0436543321609495, 'val_loss:', 0.51522725939750669)
('epoch', 542, 'train_loss:', 2.0407773184776308, 'val_loss:', 0.51312322616577144)
('epoch', 543, 'train_loss:', 2.0383195233345033, 'val_loss:', 0.51524969816207888)
('epoch', 544, 'train_loss:', 2.0378487575054169, 'val_loss:', 0.51402415871620177)
('epoch', 545, 'train_loss:', 2.0391443872451784, 'val_loss:', 0.51294977784156803)
('epoch', 546, 'train_loss:', 2.0381950151920321, 'val_loss:', 0.51330655097961431)
('epoch', 547, 'train_loss:', 2.0373206353187561, 'val_loss:', 0.51493722200393677)
('epoch', 548, 'train_loss:', 2.0369480931758881, 'val_loss:', 0.51438744783401491)
('epoch', 549, 'train_loss:', 2.0318195641040804, 'val_loss:', 0.51236180663108821)
('epoch', 550, 'train_loss:', 2.031574959754944, 'val_loss:', 0.51269826889038084)
('epoch', 551, 'train_loss:', 2.0355102825164795, 'val_loss:', 0.51293041706085207)
('epoch', 552, 'train_loss:', 2.0309644472599029, 'val_loss:', 0.5125143575668335)
('epoch', 553, 'train_loss:', 2.0315071785449983, 'val_loss:', 0.51203661322593685)
('epoch', 554, 'train_loss:', 2.0321804881095886, 'val_loss:', 0.51194726228713994)
('epoch', 555, 'train_loss:', 2.0318034005165102, 'val_loss:', 0.51272536396980284)
('epoch', 556, 'train_loss:', 2.0297104048728944, 'val_loss:', 0.51300404787063603)
('epoch', 557, 'train_loss:', 2.0287900209426879, 'val_loss:', 0.5121381556987763)
('epoch', 558, 'train_loss:', 2.031492066383362, 'val_loss:', 0.51130776047706605)
('epoch', 559, 'train_loss:', 2.0269550192356109, 'val_loss:', 0.51027919650077824)
('epoch', 560, 'train_loss:', 2.0268963575363159, 'val_loss:', 0.51335000991821289)
('epoch', 561, 'train_loss:', 2.027723573446274, 'val_loss:', 0.5099732732772827)
('epoch', 562, 'train_loss:', 2.0225860631465911, 'val_loss:', 0.51110284447669985)
('epoch', 563, 'train_loss:', 2.0245830845832824, 'val_loss:', 0.51226210713386533)
('epoch', 564, 'train_loss:', 2.0229013085365297, 'val_loss:', 0.51090619564056394)
('epoch', 565, 'train_loss:', 2.0214890944957733, 'val_loss:', 0.51142764210700986)
('epoch', 566, 'train_loss:', 2.0293793284893038, 'val_loss:', 0.50998486757278438)
('epoch', 567, 'train_loss:', 2.0212191629409788, 'val_loss:', 0.5097225677967071)
('epoch', 568, 'train_loss:', 2.0258205687999724, 'val_loss:', 0.50941824078559872)
('epoch', 569, 'train_loss:', 2.0188127779960632, 'val_loss:', 0.50897441864013671)
('epoch', 570, 'train_loss:', 2.0209314334392547, 'val_loss:', 0.50854994654655461)
('epoch', 571, 'train_loss:', 2.016840672492981, 'val_loss:', 0.51009051442146303)
('epoch', 572, 'train_loss:', 2.0168476033210756, 'val_loss:', 0.51084295392036438)
('epoch', 573, 'train_loss:', 2.0184561157226564, 'val_loss:', 0.50991326570510864)
('epoch', 574, 'train_loss:', 2.0196972358226777, 'val_loss:', 0.50769557118415831)
('epoch', 575, 'train_loss:', 2.0163980007171629, 'val_loss:', 0.5073547863960266)
('epoch', 576, 'train_loss:', 2.0139258980751036, 'val_loss:', 0.51006872296333317)
('epoch', 577, 'train_loss:', 2.0132282257080076, 'val_loss:', 0.50903741240501399)
('epoch', 578, 'train_loss:', 2.0141672003269195, 'val_loss:', 0.50708057284355168)
('epoch', 579, 'train_loss:', 2.0139900100231172, 'val_loss:', 0.50794427394866948)
('epoch', 580, 'train_loss:', 2.01268563747406, 'val_loss:', 0.50773461699485778)
('epoch', 581, 'train_loss:', 2.0113068878650666, 'val_loss:', 0.50767581701278686)
('epoch', 582, 'train_loss:', 2.0118853044509888, 'val_loss:', 0.50692679762840276)
('epoch', 583, 'train_loss:', 2.012971911430359, 'val_loss:', 0.509073988199234)
('epoch', 584, 'train_loss:', 2.008089336156845, 'val_loss:', 0.50718669414520268)
('epoch', 585, 'train_loss:', 2.0115814077854157, 'val_loss:', 0.50843905091285702)
('epoch', 586, 'train_loss:', 2.005267940759659, 'val_loss:', 0.50678784251213072)
('epoch', 587, 'train_loss:', 2.0045532619953157, 'val_loss:', 0.50469307184219359)
('epoch', 588, 'train_loss:', 2.0027341961860659, 'val_loss:', 0.50641568064689635)
('epoch', 589, 'train_loss:', 2.0071980941295622, 'val_loss:', 0.50747338891029359)
('epoch', 590, 'train_loss:', 2.0023951649665834, 'val_loss:', 0.50485486626625065)
('epoch', 591, 'train_loss:', 2.0011635935306549, 'val_loss:', 0.50775303721427922)
('epoch', 592, 'train_loss:', 2.0023981201648713, 'val_loss:', 0.50554768562316899)
('epoch', 593, 'train_loss:', 2.0013619983196258, 'val_loss:', 0.505503933429718)
('epoch', 594, 'train_loss:', 2.003346004486084, 'val_loss:', 0.50504760980606078)
('epoch', 595, 'train_loss:', 2.0020488011837005, 'val_loss:', 0.50607470273971555)
('epoch', 596, 'train_loss:', 2.0004774355888366, 'val_loss:', 0.50479920744895934)
('epoch', 597, 'train_loss:', 2.000043170452118, 'val_loss:', 0.50567707538604734)
('epoch', 598, 'train_loss:', 1.9973396313190461, 'val_loss:', 0.50487491607666013)
('epoch', 599, 'train_loss:', 1.9954849064350129, 'val_loss:', 0.50453776717185972)
