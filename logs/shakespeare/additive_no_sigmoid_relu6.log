(keras)skoppula@sls-sm-8:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=1 python lstm_all_variants.py -t -d shakespeare -v additive_no_sigmoid_relu6
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'shakespeare', 'train': True, 'variant': 'additive_no_sigmoid_relu6', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (29743, 30), (29743, 30), (7436, 30), (7436, 30))
('num steps in trn and val epochs', 116, 29)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2680 get requests, put_count=2481 evicted_count=1000 eviction_rate=0.403063 and unsatisfied allocation rate=0.484701
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2722 get requests, put_count=2829 evicted_count=1000 eviction_rate=0.353482 and unsatisfied allocation rate=0.336517
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24354 get requests, put_count=24355 evicted_count=1000 eviction_rate=0.0410593 and unsatisfied allocation rate=0.0434426
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 0, 'train_loss:', 4.4128535199165349, 'val_loss:', 1.040169324874878)
('epoch', 1, 'train_loss:', 4.0556982731819149, 'val_loss:', 0.99480120182037357)
('epoch', 2, 'train_loss:', 3.9387200498580932, 'val_loss:', 0.97556018829345703)
('epoch', 3, 'train_loss:', 3.8809462332725526, 'val_loss:', 0.9653217768669129)
('epoch', 4, 'train_loss:', 3.8307639360427856, 'val_loss:', 0.95379939794540403)
('epoch', 5, 'train_loss:', 3.7766029930114744, 'val_loss:', 0.93665549755096433)
('epoch', 6, 'train_loss:', 3.7026052093505859, 'val_loss:', 0.91463827848434454)
('epoch', 7, 'train_loss:', 3.6122050094604492, 'val_loss:', 0.89317133665084836)
('epoch', 8, 'train_loss:', 3.5176258754730223, 'val_loss:', 0.865319082736969)
('epoch', 9, 'train_loss:', 3.4262941527366637, 'val_loss:', 0.84506821870803828)
('epoch', 10, 'train_loss:', 3.3506609535217287, 'val_loss:', 0.83377504348754883)
('epoch', 11, 'train_loss:', 3.2943542027473449, 'val_loss:', 0.8175007700920105)
('epoch', 12, 'train_loss:', 3.2416394996643065, 'val_loss:', 0.80493714332580568)
('epoch', 13, 'train_loss:', 3.1884347534179689, 'val_loss:', 0.79366131067276002)
('epoch', 14, 'train_loss:', 3.1488908290863038, 'val_loss:', 0.77960706949234004)
('epoch', 15, 'train_loss:', 3.1004421377182005, 'val_loss:', 0.77308145284652707)
('epoch', 16, 'train_loss:', 3.0617034411430359, 'val_loss:', 0.7634829664230347)
('epoch', 17, 'train_loss:', 3.02446617603302, 'val_loss:', 0.75356671333312986)
('epoch', 18, 'train_loss:', 2.9953845405578612, 'val_loss:', 0.7470647644996643)
('epoch', 19, 'train_loss:', 2.9699564695358278, 'val_loss:', 0.74164083957672122)
('epoch', 20, 'train_loss:', 2.9446656203269956, 'val_loss:', 0.73459623813629149)
('epoch', 21, 'train_loss:', 2.925668969154358, 'val_loss:', 0.72678285837173462)
('epoch', 22, 'train_loss:', 2.908449149131775, 'val_loss:', 0.72489486932754521)
('epoch', 23, 'train_loss:', 2.8878701019287107, 'val_loss:', 0.72090976953506469)
('epoch', 24, 'train_loss:', 2.872765688896179, 'val_loss:', 0.71823459386825561)
('epoch', 25, 'train_loss:', 2.8600245547294616, 'val_loss:', 0.71509922742843623)
('epoch', 26, 'train_loss:', 2.8448393011093138, 'val_loss:', 0.71011645555496217)
('epoch', 27, 'train_loss:', 2.8306345534324646, 'val_loss:', 0.70712868213653568)
('epoch', 28, 'train_loss:', 2.8200770068168639, 'val_loss:', 0.70469772338867187)
('epoch', 29, 'train_loss:', 2.8094728279113768, 'val_loss:', 0.70263373613357549)
('epoch', 30, 'train_loss:', 2.7977585077285765, 'val_loss:', 0.69985897779464723)
('epoch', 31, 'train_loss:', 2.7875064015388489, 'val_loss:', 0.69666633605957029)
('epoch', 32, 'train_loss:', 2.7753326892852783, 'val_loss:', 0.69491199254989622)
('epoch', 33, 'train_loss:', 2.7674822211265564, 'val_loss:', 0.69244443655014043)
('epoch', 34, 'train_loss:', 2.7565634632110596, 'val_loss:', 0.68882041692733764)
('epoch', 35, 'train_loss:', 2.748316698074341, 'val_loss:', 0.68693545818328861)
('epoch', 36, 'train_loss:', 2.7365256333351136, 'val_loss:', 0.68522153854370116)
('epoch', 37, 'train_loss:', 2.7270763564109801, 'val_loss:', 0.68400713682174685)
('epoch', 38, 'train_loss:', 2.7192709779739381, 'val_loss:', 0.68001291990280155)
('epoch', 39, 'train_loss:', 2.7076805901527403, 'val_loss:', 0.67867312192916873)
('epoch', 40, 'train_loss:', 2.6953077030181887, 'val_loss:', 0.67541903257369995)
('epoch', 41, 'train_loss:', 2.695551960468292, 'val_loss:', 0.67277996301651)
('epoch', 42, 'train_loss:', 2.6854136443138121, 'val_loss:', 0.67101094007492068)
('epoch', 43, 'train_loss:', 2.6790226459503175, 'val_loss:', 0.66812199115753179)
('epoch', 44, 'train_loss:', 2.6690558314323427, 'val_loss:', 0.66790267229080202)
('epoch', 45, 'train_loss:', 2.6614354991912843, 'val_loss:', 0.66493409395217895)
('epoch', 46, 'train_loss:', 2.6562928652763365, 'val_loss:', 0.66297829151153564)
('epoch', 47, 'train_loss:', 2.6457104659080506, 'val_loss:', 0.66151181936264036)
('epoch', 48, 'train_loss:', 2.6381865358352661, 'val_loss:', 0.6591653490066528)
('epoch', 49, 'train_loss:', 2.6325125741958617, 'val_loss:', 0.65882763862609861)
('epoch', 50, 'train_loss:', 2.6232881951332092, 'val_loss:', 0.65514972448348996)
('epoch', 51, 'train_loss:', 2.6159942007064818, 'val_loss:', 0.65410102605819698)
('epoch', 52, 'train_loss:', 2.6078315949440003, 'val_loss:', 0.65281540155410767)
('epoch', 53, 'train_loss:', 2.6034828996658326, 'val_loss:', 0.65034327268600467)
('epoch', 54, 'train_loss:', 2.5956463360786439, 'val_loss:', 0.64924825191497804)
('epoch', 55, 'train_loss:', 2.5896963715553283, 'val_loss:', 0.64923503160476681)
('epoch', 56, 'train_loss:', 2.5843753337860109, 'val_loss:', 0.64553648948669429)
('epoch', 57, 'train_loss:', 2.5771091318130495, 'val_loss:', 0.64474993944168091)
('epoch', 58, 'train_loss:', 2.5742103886604308, 'val_loss:', 0.64245562076568608)
('epoch', 59, 'train_loss:', 2.5653169178962707, 'val_loss:', 0.64026738166809083)
('epoch', 60, 'train_loss:', 2.559469470977783, 'val_loss:', 0.63983051776885991)
('epoch', 61, 'train_loss:', 2.5546773695945739, 'val_loss:', 0.63768308162689213)
('epoch', 62, 'train_loss:', 2.548369917869568, 'val_loss:', 0.63680157899856571)
('epoch', 63, 'train_loss:', 2.5392313599586487, 'val_loss:', 0.6363134026527405)
('epoch', 64, 'train_loss:', 2.5358596658706665, 'val_loss:', 0.63529377460479741)
('epoch', 65, 'train_loss:', 2.5335774135589602, 'val_loss:', 0.63485820531845094)
('epoch', 66, 'train_loss:', 2.5280430698394776, 'val_loss:', 0.63297594547271729)
('epoch', 67, 'train_loss:', 2.5242107129096985, 'val_loss:', 0.63008975505828857)
('epoch', 68, 'train_loss:', 2.5181353116035461, 'val_loss:', 0.62940334796905517)
('epoch', 69, 'train_loss:', 2.5116764855384828, 'val_loss:', 0.62832596302032473)
('epoch', 70, 'train_loss:', 2.507051944732666, 'val_loss:', 0.62587465524673458)
('epoch', 71, 'train_loss:', 2.5059445643424989, 'val_loss:', 0.62697970628738409)
('epoch', 72, 'train_loss:', 2.498534243106842, 'val_loss:', 0.62475366115570063)
('epoch', 73, 'train_loss:', 2.4934212756156922, 'val_loss:', 0.62489804983139041)
('epoch', 74, 'train_loss:', 2.4882738375663758, 'val_loss:', 0.62120713233947755)
('epoch', 75, 'train_loss:', 2.4854598617553711, 'val_loss:', 0.62142060041427616)
('epoch', 76, 'train_loss:', 2.4812268066406249, 'val_loss:', 0.6198604321479797)
('epoch', 77, 'train_loss:', 2.4775838327407835, 'val_loss:', 0.61911054134368892)
('epoch', 78, 'train_loss:', 2.4708620190620421, 'val_loss:', 0.61960038185119626)
('epoch', 79, 'train_loss:', 2.4706271529197692, 'val_loss:', 0.61595542192459107)
('epoch', 80, 'train_loss:', 2.4660869789123536, 'val_loss:', 0.61629674434661863)
('epoch', 81, 'train_loss:', 2.4601667308807373, 'val_loss:', 0.62005663633346553)
('epoch', 82, 'train_loss:', 2.4542655181884765, 'val_loss:', 0.61517981290817259)
('epoch', 83, 'train_loss:', 2.4494330096244812, 'val_loss:', 0.61394066810607906)
('epoch', 84, 'train_loss:', 2.4480946469306946, 'val_loss:', 0.61390864133834844)
('epoch', 85, 'train_loss:', 2.4430911302566529, 'val_loss:', 0.61221035718917849)
('epoch', 86, 'train_loss:', 2.4400072669982911, 'val_loss:', 0.61188032150268556)
('epoch', 87, 'train_loss:', 2.4391939878463744, 'val_loss:', 0.60896496772766118)
('epoch', 88, 'train_loss:', 2.4304369854927064, 'val_loss:', 0.60822406530380246)
('epoch', 89, 'train_loss:', 2.4307676792144775, 'val_loss:', 0.60693634986877443)
('epoch', 90, 'train_loss:', 2.4261045622825623, 'val_loss:', 0.60747842788696294)
('epoch', 91, 'train_loss:', 2.4209293079376222, 'val_loss:', 0.60443539619445796)
('epoch', 92, 'train_loss:', 2.4201767969131471, 'val_loss:', 0.60549236297607423)
('epoch', 93, 'train_loss:', 2.4124132680892942, 'val_loss:', 0.60985108375549313)
('epoch', 94, 'train_loss:', 2.4107241702079771, 'val_loss:', 0.60491893529891971)
('epoch', 95, 'train_loss:', 2.4087331295013428, 'val_loss:', 0.604503161907196)
('epoch', 96, 'train_loss:', 2.3987016105651855, 'val_loss:', 0.60021976709365843)
('epoch', 97, 'train_loss:', 2.3981547880172731, 'val_loss:', 0.60024368524551397)
('epoch', 98, 'train_loss:', 2.3960940980911256, 'val_loss:', 0.60141056776046753)
('epoch', 99, 'train_loss:', 2.3943945288658144, 'val_loss:', 0.60114014625549317)
('epoch', 100, 'train_loss:', 2.388340985774994, 'val_loss:', 0.59760295867919921)
('epoch', 101, 'train_loss:', 2.3898150038719179, 'val_loss:', 0.59819805145263671)
('epoch', 102, 'train_loss:', 2.381672646999359, 'val_loss:', 0.6003991794586182)
('epoch', 103, 'train_loss:', 2.3814672827720642, 'val_loss:', 0.59598600625991827)
('epoch', 104, 'train_loss:', 2.376981568336487, 'val_loss:', 0.59377873897552491)
('epoch', 105, 'train_loss:', 2.3731989765167238, 'val_loss:', 0.59266585350036616)
('epoch', 106, 'train_loss:', 2.3704878401756289, 'val_loss:', 0.59286714315414424)
('epoch', 107, 'train_loss:', 2.3651671242713928, 'val_loss:', 0.59138287782669063)
('epoch', 108, 'train_loss:', 2.3621355164051057, 'val_loss:', 0.59175166606903074)
('epoch', 109, 'train_loss:', 2.3570594918727874, 'val_loss:', 0.59235920667648312)
('epoch', 110, 'train_loss:', 2.3611934876441953, 'val_loss:', 0.58947998523712153)
('epoch', 111, 'train_loss:', 2.3556780469417573, 'val_loss:', 0.58963188171386716)
('epoch', 112, 'train_loss:', 2.3511353886127471, 'val_loss:', 0.58871797084808353)
('epoch', 113, 'train_loss:', 2.3491432368755341, 'val_loss:', 0.58762858629226689)
('epoch', 114, 'train_loss:', 2.3416471481323242, 'val_loss:', 0.58898354887962345)
('epoch', 115, 'train_loss:', 2.3423588395118715, 'val_loss:', 0.58517265439033506)
('epoch', 116, 'train_loss:', 2.3400578379631041, 'val_loss:', 0.58516214370727537)
('epoch', 117, 'train_loss:', 2.3342591226100922, 'val_loss:', 0.58595879197120671)
('epoch', 118, 'train_loss:', 2.3314303958415987, 'val_loss:', 0.58386695384979248)
('epoch', 119, 'train_loss:', 2.3291953063011168, 'val_loss:', 0.58228443980216982)
('epoch', 120, 'train_loss:', 2.3241406047344206, 'val_loss:', 0.58159777164459225)
('epoch', 121, 'train_loss:', 2.3252670574188232, 'val_loss:', 0.58103183746337894)
('epoch', 122, 'train_loss:', 2.3218994402885436, 'val_loss:', 0.58054454326629634)
('epoch', 123, 'train_loss:', 2.3160769367218017, 'val_loss:', 0.57991135358810419)
('epoch', 124, 'train_loss:', 2.3136444473266602, 'val_loss:', 0.5805000114440918)
('epoch', 125, 'train_loss:', 2.3117416918277742, 'val_loss:', 0.57953550100326534)
('epoch', 126, 'train_loss:', 2.3079432940483091, 'val_loss:', 0.57849238634109501)
('epoch', 127, 'train_loss:', 2.3042826664447786, 'val_loss:', 0.57631157517433163)
('epoch', 128, 'train_loss:', 2.3019805777072908, 'val_loss:', 0.57761828541755678)
('epoch', 129, 'train_loss:', 2.3019174218177794, 'val_loss:', 0.57801328420639042)
('epoch', 130, 'train_loss:', 2.2951271855831146, 'val_loss:', 0.57433541178703307)
('epoch', 131, 'train_loss:', 2.2952027094364165, 'val_loss:', 0.57568192124366757)
('epoch', 132, 'train_loss:', 2.2918578493595123, 'val_loss:', 0.57728528857231143)
('epoch', 133, 'train_loss:', 2.290489983558655, 'val_loss:', 0.57463392496109011)
('epoch', 134, 'train_loss:', 2.283783118724823, 'val_loss:', 0.57096325635910039)
('epoch', 135, 'train_loss:', 2.2827537047863005, 'val_loss:', 0.57142973423004151)
('epoch', 136, 'train_loss:', 2.2813626658916473, 'val_loss:', 0.57180682539939875)
('epoch', 137, 'train_loss:', 2.2743278884887697, 'val_loss:', 0.5726028454303741)
('epoch', 138, 'train_loss:', 2.27549467086792, 'val_loss:', 0.57024761915206912)
('epoch', 139, 'train_loss:', 2.2714501702785492, 'val_loss:', 0.57008238673210143)
('epoch', 140, 'train_loss:', 2.2676016175746918, 'val_loss:', 0.569935142993927)
('epoch', 141, 'train_loss:', 2.2665526258945463, 'val_loss:', 0.56885334014892575)
('epoch', 142, 'train_loss:', 2.2653086090087893, 'val_loss:', 0.57074065089225767)
('epoch', 143, 'train_loss:', 2.2614672315120696, 'val_loss:', 0.56578728675842282)
('epoch', 144, 'train_loss:', 2.2597486591339111, 'val_loss:', 0.5663026058673859)
('epoch', 145, 'train_loss:', 2.2578296327590941, 'val_loss:', 0.56561497688293461)
('epoch', 146, 'train_loss:', 2.2522245407104493, 'val_loss:', 0.56637162923812867)
('epoch', 147, 'train_loss:', 2.2528852903842926, 'val_loss:', 0.56618919253349309)
('epoch', 148, 'train_loss:', 2.2504144883155823, 'val_loss:', 0.56403847694396969)
('epoch', 149, 'train_loss:', 2.2463630390167237, 'val_loss:', 0.56241884231567385)
('epoch', 150, 'train_loss:', 2.2422373950481416, 'val_loss:', 0.56267234086990359)
('epoch', 151, 'train_loss:', 2.2466737878322602, 'val_loss:', 0.56446239352226257)
('epoch', 152, 'train_loss:', 2.2385676217079165, 'val_loss:', 0.56148673772811886)
('epoch', 153, 'train_loss:', 2.2360707795619965, 'val_loss:', 0.55972749948501588)
('epoch', 154, 'train_loss:', 2.2324625599384307, 'val_loss:', 0.56025399923324581)
('epoch', 155, 'train_loss:', 2.2331504511833189, 'val_loss:', 0.55993765830993647)
('epoch', 156, 'train_loss:', 2.2290104329586029, 'val_loss:', 0.55743145823478701)
('epoch', 157, 'train_loss:', 2.2262879097461701, 'val_loss:', 0.55846569299697879)
('epoch', 158, 'train_loss:', 2.2244562482833863, 'val_loss:', 0.55672336578369142)
('epoch', 159, 'train_loss:', 2.2245711374282835, 'val_loss:', 0.55934600591659545)
('epoch', 160, 'train_loss:', 2.2192293930053713, 'val_loss:', 0.55445051670074463)
('epoch', 161, 'train_loss:', 2.2152833569049837, 'val_loss:', 0.5557113492488861)
('epoch', 162, 'train_loss:', 2.2160638606548311, 'val_loss:', 0.55390267491340639)
('epoch', 163, 'train_loss:', 2.2123562860488892, 'val_loss:', 0.55584994077682492)
('epoch', 164, 'train_loss:', 2.2118759036064146, 'val_loss:', 0.55398987650871279)
('epoch', 165, 'train_loss:', 2.2081472396850588, 'val_loss:', 0.55317111611366276)
('epoch', 166, 'train_loss:', 2.2050259029865265, 'val_loss:', 0.5611522686481476)
('epoch', 167, 'train_loss:', 2.2041197419166565, 'val_loss:', 0.55342836260795591)
('epoch', 168, 'train_loss:', 2.2009053313732148, 'val_loss:', 0.55255131483078002)
('epoch', 169, 'train_loss:', 2.1997006046772003, 'val_loss:', 0.55075175404548649)
('epoch', 170, 'train_loss:', 2.1958339953422548, 'val_loss:', 0.55265908479690551)
('epoch', 171, 'train_loss:', 2.1950367569923399, 'val_loss:', 0.55195981979370112)
('epoch', 172, 'train_loss:', 2.1960577118396758, 'val_loss:', 0.55153434753417974)
('epoch', 173, 'train_loss:', 2.1927742576599121, 'val_loss:', 0.54943013429641718)
('epoch', 174, 'train_loss:', 2.1888618516921996, 'val_loss:', 0.54839590072631839)
('epoch', 175, 'train_loss:', 2.1849058449268339, 'val_loss:', 0.54899749159812927)
('epoch', 176, 'train_loss:', 2.1850822591781616, 'val_loss:', 0.54837487459182743)
('epoch', 177, 'train_loss:', 2.1796442091464998, 'val_loss:', 0.54895782947540284)
('epoch', 178, 'train_loss:', 2.1796532130241393, 'val_loss:', 0.54810827851295474)
('epoch', 179, 'train_loss:', 2.1782225751876831, 'val_loss:', 0.54557760715484616)
('epoch', 180, 'train_loss:', 2.1760304832458495, 'val_loss:', 0.54683085680007937)
('epoch', 181, 'train_loss:', 2.1724080562591555, 'val_loss:', 0.54465932011604312)
('epoch', 182, 'train_loss:', 2.1694863820075989, 'val_loss:', 0.54702498078346251)
('epoch', 183, 'train_loss:', 2.1711946177482604, 'val_loss:', 0.54401662111282345)
('epoch', 184, 'train_loss:', 2.1714331758022309, 'val_loss:', 0.54607018947601316)
('epoch', 185, 'train_loss:', 2.1657048010826112, 'val_loss:', 0.54386126518249511)
('epoch', 186, 'train_loss:', 2.1623748195171357, 'val_loss:', 0.54247056245803837)
('epoch', 187, 'train_loss:', 2.1588064932823183, 'val_loss:', 0.54181627273559574)
('epoch', 188, 'train_loss:', 2.1593900072574614, 'val_loss:', 0.54174091219902043)
('epoch', 189, 'train_loss:', 2.1603773760795595, 'val_loss:', 0.54101303577423099)
('epoch', 190, 'train_loss:', 2.1541515100002289, 'val_loss:', 0.54255046129226681)
('epoch', 191, 'train_loss:', 2.1567601585388183, 'val_loss:', 0.54164386034011835)
('epoch', 192, 'train_loss:', 2.1537956857681273, 'val_loss:', 0.5375059652328491)
('epoch', 193, 'train_loss:', 2.1506661581993103, 'val_loss:', 0.53841453790664673)
('epoch', 194, 'train_loss:', 2.1507095265388489, 'val_loss:', 0.54140303134918211)
('epoch', 195, 'train_loss:', 2.1446236824989318, 'val_loss:', 0.53924954891204835)
('epoch', 196, 'train_loss:', 2.1430974352359771, 'val_loss:', 0.53860768079757693)
('epoch', 197, 'train_loss:', 2.1417002117633821, 'val_loss:', 0.53815430045127866)
('epoch', 198, 'train_loss:', 2.1413511228561402, 'val_loss:', 0.5388842141628265)
('epoch', 199, 'train_loss:', 2.1378792035579681, 'val_loss:', 0.53722581863403318)
('epoch', 200, 'train_loss:', 2.1391420888900758, 'val_loss:', 0.5359958851337433)
('epoch', 201, 'train_loss:', 2.1321527445316315, 'val_loss:', 0.53593951106071469)
('epoch', 202, 'train_loss:', 2.1329661905765533, 'val_loss:', 0.53687325358390803)
('epoch', 203, 'train_loss:', 2.1283835160732267, 'val_loss:', 0.53587212085723879)
('epoch', 204, 'train_loss:', 2.1264328634738923, 'val_loss:', 0.53401611328124998)
('epoch', 205, 'train_loss:', 2.1262085533142088, 'val_loss:', 0.53440130352973936)
('epoch', 206, 'train_loss:', 2.1211485576629641, 'val_loss:', 0.53477795481681822)
('epoch', 207, 'train_loss:', 2.1187768375873564, 'val_loss:', 0.53139787077903744)
('epoch', 208, 'train_loss:', 2.1204870939254761, 'val_loss:', 0.5332906401157379)
('epoch', 209, 'train_loss:', 2.1166440641880033, 'val_loss:', 0.53204579830169674)
('epoch', 210, 'train_loss:', 2.1170651483535767, 'val_loss:', 0.53280627846717832)
('epoch', 211, 'train_loss:', 2.1169705998897554, 'val_loss:', 0.53153994321823117)
('epoch', 212, 'train_loss:', 2.1126543986797333, 'val_loss:', 0.53226220607757568)
('epoch', 213, 'train_loss:', 2.1125496006011963, 'val_loss:', 0.53326645255088811)
('epoch', 214, 'train_loss:', 2.1119821810722352, 'val_loss:', 0.531212512254715)
('epoch', 215, 'train_loss:', 2.1099649405479433, 'val_loss:', 0.52857853889465334)
('epoch', 216, 'train_loss:', 2.1094621360301971, 'val_loss:', 0.53353916645050048)
('epoch', 217, 'train_loss:', 2.1047544825077056, 'val_loss:', 0.52808070540428165)
('epoch', 218, 'train_loss:', 2.1014061725139617, 'val_loss:', 0.53023319482803344)
('epoch', 219, 'train_loss:', 2.1011765921115875, 'val_loss:', 0.52845192670822139)
('epoch', 220, 'train_loss:', 2.0987808871269227, 'val_loss:', 0.52891139507293705)
('epoch', 221, 'train_loss:', 2.0990679931640623, 'val_loss:', 0.52813968062400818)
('epoch', 222, 'train_loss:', 2.0978255546092988, 'val_loss:', 0.52635394573211669)
('epoch', 223, 'train_loss:', 2.0962313008308411, 'val_loss:', 0.52797724127769474)
('epoch', 224, 'train_loss:', 2.0934186029434203, 'val_loss:', 0.52735213518142698)
('epoch', 225, 'train_loss:', 2.0934767115116117, 'val_loss:', 0.52616831064224245)
('epoch', 226, 'train_loss:', 2.0851760947704316, 'val_loss:', 0.52834710597991941)
('epoch', 227, 'train_loss:', 2.0901523637771606, 'val_loss:', 0.52426768660545353)
('epoch', 228, 'train_loss:', 2.0891095221042635, 'val_loss:', 0.52459773063659665)
('epoch', 229, 'train_loss:', 2.0811552262306212, 'val_loss:', 0.52291249394416806)
('epoch', 230, 'train_loss:', 2.0843890202045441, 'val_loss:', 0.52459029674530033)
('epoch', 231, 'train_loss:', 2.0769477939605712, 'val_loss:', 0.52314882516860961)
('epoch', 232, 'train_loss:', 2.0827626049518586, 'val_loss:', 0.52579222321510311)
('epoch', 233, 'train_loss:', 2.0805425560474395, 'val_loss:', 0.52201800346374516)
('epoch', 234, 'train_loss:', 2.0744164097309112, 'val_loss:', 0.52449294090270993)
('epoch', 235, 'train_loss:', 2.0744681978225707, 'val_loss:', 0.51978860378265379)
('epoch', 236, 'train_loss:', 2.0702568972110749, 'val_loss:', 0.52043889403343202)
('epoch', 237, 'train_loss:', 2.0706996190547944, 'val_loss:', 0.52353012323379522)
('epoch', 238, 'train_loss:', 2.0681549620628359, 'val_loss:', 0.52103863835334774)
('epoch', 239, 'train_loss:', 2.0653293442726137, 'val_loss:', 0.52087977647781369)
('epoch', 240, 'train_loss:', 2.0712318325042727, 'val_loss:', 0.51984515070915227)
('epoch', 241, 'train_loss:', 2.06669456243515, 'val_loss:', 0.52025139451026914)
('epoch', 242, 'train_loss:', 2.0629640340805055, 'val_loss:', 0.52085765957832342)
('epoch', 243, 'train_loss:', 2.0657427334785461, 'val_loss:', 0.52060540795326238)
('epoch', 244, 'train_loss:', 2.0612221813201903, 'val_loss:', 0.51956179380416867)
('epoch', 245, 'train_loss:', 2.0593457543849945, 'val_loss:', 0.51920201778411867)
('epoch', 246, 'train_loss:', 2.0550998055934908, 'val_loss:', 0.51805805563926699)
('epoch', 247, 'train_loss:', 2.0555018937587737, 'val_loss:', 0.51899404525756831)
('epoch', 248, 'train_loss:', 2.0544886136054994, 'val_loss:', 0.51860314488410952)
('epoch', 249, 'train_loss:', 2.0495558059215546, 'val_loss:', 0.51691341757774356)
('epoch', 250, 'train_loss:', 2.0509944748878479, 'val_loss:', 0.51820053815841671)
('epoch', 251, 'train_loss:', 2.0528040027618406, 'val_loss:', 0.51675296545028682)
('epoch', 252, 'train_loss:', 2.0502714443206789, 'val_loss:', 0.51498058795928958)
('epoch', 253, 'train_loss:', 2.0446995377540587, 'val_loss:', 0.5155963695049286)
('epoch', 254, 'train_loss:', 2.0421728181838987, 'val_loss:', 0.51538006901741029)
('epoch', 255, 'train_loss:', 2.0453403151035308, 'val_loss:', 0.51532685995101923)
('epoch', 256, 'train_loss:', 2.0398679268360138, 'val_loss:', 0.51629118800163265)
('epoch', 257, 'train_loss:', 2.0391482663154603, 'val_loss:', 0.51349333167076106)
('epoch', 258, 'train_loss:', 2.0410027384757994, 'val_loss:', 0.51376312851905825)
('epoch', 259, 'train_loss:', 2.0334300553798674, 'val_loss:', 0.51251790881156922)
('epoch', 260, 'train_loss:', 2.0339372348785401, 'val_loss:', 0.51404988527297979)
('epoch', 261, 'train_loss:', 2.0341956520080569, 'val_loss:', 0.51259284019470219)
('epoch', 262, 'train_loss:', 2.0299162590503692, 'val_loss:', 0.51338648438453671)
('epoch', 263, 'train_loss:', 2.0294014954566957, 'val_loss:', 0.51096615672111512)
('epoch', 264, 'train_loss:', 2.028126360177994, 'val_loss:', 0.51234957575798035)
('epoch', 265, 'train_loss:', 2.0280584573745726, 'val_loss:', 0.51118782043457034)
('epoch', 266, 'train_loss:', 2.0308788645267488, 'val_loss:', 0.51070252537727356)
('epoch', 267, 'train_loss:', 2.026069189310074, 'val_loss:', 0.51163883328437809)
('epoch', 268, 'train_loss:', 2.0249804735183714, 'val_loss:', 0.51240378975868228)
('epoch', 269, 'train_loss:', 2.0219850492477418, 'val_loss:', 0.5132825815677643)
('epoch', 270, 'train_loss:', 2.0218217873573305, 'val_loss:', 0.51003674149513245)
('epoch', 271, 'train_loss:', 2.0208649325370787, 'val_loss:', 0.50843502640724181)
('epoch', 272, 'train_loss:', 2.0152522742748262, 'val_loss:', 0.5107699227333069)
('epoch', 273, 'train_loss:', 2.0171440148353579, 'val_loss:', 0.50953456044197087)
('epoch', 274, 'train_loss:', 2.0134425044059752, 'val_loss:', 0.5087366712093353)
('epoch', 275, 'train_loss:', 2.0146996009349825, 'val_loss:', 0.50615431308746339)
('epoch', 276, 'train_loss:', 2.0127178657054903, 'val_loss:', 0.50871222138404848)
('epoch', 277, 'train_loss:', 2.0112998378276825, 'val_loss:', 0.50697240114212039)
('epoch', 278, 'train_loss:', 2.0095704984664917, 'val_loss:', 0.5078825390338898)
('epoch', 279, 'train_loss:', 2.0042805707454683, 'val_loss:', 0.50674596667289729)
('epoch', 280, 'train_loss:', 2.0087430965900421, 'val_loss:', 0.50896139264106754)
('epoch', 281, 'train_loss:', 2.0090743267536162, 'val_loss:', 0.50759079575538635)
('epoch', 282, 'train_loss:', 2.0058077847957612, 'val_loss:', 0.50649475812911993)
('epoch', 283, 'train_loss:', 2.0019630181789396, 'val_loss:', 0.50617559909820553)
('epoch', 284, 'train_loss:', 2.0004982113838197, 'val_loss:', 0.50498583555221555)
('epoch', 285, 'train_loss:', 2.0001741147041319, 'val_loss:', 0.5050936949253082)
('epoch', 286, 'train_loss:', 1.9972724032402038, 'val_loss:', 0.50396642208099363)
('epoch', 287, 'train_loss:', 1.9971680212020875, 'val_loss:', 0.50586616277694707)
('epoch', 288, 'train_loss:', 1.9985291612148286, 'val_loss:', 0.50509571790695196)
('epoch', 289, 'train_loss:', 1.9966708445549011, 'val_loss:', 0.50397389292716976)
('epoch', 290, 'train_loss:', 1.9955528616905212, 'val_loss:', 0.5024278962612152)
('epoch', 291, 'train_loss:', 1.9938371419906615, 'val_loss:', 0.50260688662528996)
('epoch', 292, 'train_loss:', 1.9903696882724762, 'val_loss:', 0.5025729155540466)
('epoch', 293, 'train_loss:', 1.9905054044723511, 'val_loss:', 0.50448042869567866)
('epoch', 294, 'train_loss:', 1.9845054507255555, 'val_loss:', 0.50235885381698608)
('epoch', 295, 'train_loss:', 1.9886283981800079, 'val_loss:', 0.50331058025360109)
('epoch', 296, 'train_loss:', 1.986562466621399, 'val_loss:', 0.50277385234832761)
('epoch', 297, 'train_loss:', 1.987766455411911, 'val_loss:', 0.50207145690917965)
('epoch', 298, 'train_loss:', 1.9852256619930266, 'val_loss:', 0.5009477937221527)
('epoch', 299, 'train_loss:', 1.985015445947647, 'val_loss:', 0.50129909396171568)
('epoch', 300, 'train_loss:', 1.9833044159412383, 'val_loss:', 0.50056932568550105)
('epoch', 301, 'train_loss:', 1.9781414926052094, 'val_loss:', 0.4994399058818817)
('epoch', 302, 'train_loss:', 1.9770431733131408, 'val_loss:', 0.50090581774711607)
('epoch', 303, 'train_loss:', 1.9761984694004058, 'val_loss:', 0.501366651058197)
('epoch', 304, 'train_loss:', 1.976563310623169, 'val_loss:', 0.49975839018821716)
('epoch', 305, 'train_loss:', 1.9758955907821656, 'val_loss:', 0.50039423823356632)
('epoch', 306, 'train_loss:', 1.97601837515831, 'val_loss:', 0.49926894307136538)
('epoch', 307, 'train_loss:', 1.9725159811973572, 'val_loss:', 0.49898166179656983)
('epoch', 308, 'train_loss:', 1.9698522245883943, 'val_loss:', 0.49849379420280454)
('epoch', 309, 'train_loss:', 1.9730807292461394, 'val_loss:', 0.49863320350646972)
('epoch', 310, 'train_loss:', 1.9698757719993591, 'val_loss:', 0.49884713888168336)
('epoch', 311, 'train_loss:', 1.9680095410346985, 'val_loss:', 0.4980184078216553)
('epoch', 312, 'train_loss:', 1.9678444147109986, 'val_loss:', 0.49593251585960391)
('epoch', 313, 'train_loss:', 1.9623279869556427, 'val_loss:', 0.4944427299499512)
('epoch', 314, 'train_loss:', 1.9602438199520111, 'val_loss:', 0.49948970437049867)
('epoch', 315, 'train_loss:', 1.9675440859794617, 'val_loss:', 0.49706113338470459)
('epoch', 316, 'train_loss:', 1.9661113941669464, 'val_loss:', 0.49798809409141542)
('epoch', 317, 'train_loss:', 1.9605299615859986, 'val_loss:', 0.49690276384353638)
('epoch', 318, 'train_loss:', 1.9580440390110017, 'val_loss:', 0.49542112350463868)
('epoch', 319, 'train_loss:', 1.960639226436615, 'val_loss:', 0.49578737139701845)
('epoch', 320, 'train_loss:', 1.9577984726428985, 'val_loss:', 0.49577079296112059)
('epoch', 321, 'train_loss:', 1.9587206602096559, 'val_loss:', 0.49389065504074098)
('epoch', 322, 'train_loss:', 1.954864546060562, 'val_loss:', 0.49529523968696593)
('epoch', 323, 'train_loss:', 1.9560725355148316, 'val_loss:', 0.49365802526474001)
('epoch', 324, 'train_loss:', 1.9546483469009399, 'val_loss:', 0.49347338557243348)
('epoch', 325, 'train_loss:', 1.9477768850326538, 'val_loss:', 0.49306375384330747)
('epoch', 326, 'train_loss:', 1.9533562886714935, 'val_loss:', 0.4955479061603546)
('epoch', 327, 'train_loss:', 1.9484992218017578, 'val_loss:', 0.49209907531738284)
('epoch', 328, 'train_loss:', 1.9485600757598878, 'val_loss:', 0.49527663111686704)
('epoch', 329, 'train_loss:', 1.9436404037475585, 'val_loss:', 0.49343680500984194)
('epoch', 330, 'train_loss:', 1.9503959214687348, 'val_loss:', 0.49243403792381285)
('epoch', 331, 'train_loss:', 1.9453266191482543, 'val_loss:', 0.49445901870727538)
('epoch', 332, 'train_loss:', 1.9450809800624846, 'val_loss:', 0.49350054264068605)
('epoch', 333, 'train_loss:', 1.9461863589286805, 'val_loss:', 0.49303643107414247)
('epoch', 334, 'train_loss:', 1.9436068689823152, 'val_loss:', 0.49063193917274472)
('epoch', 335, 'train_loss:', 1.9422598397731781, 'val_loss:', 0.49190105199813844)
('epoch', 336, 'train_loss:', 1.9394268572330475, 'val_loss:', 0.49025275468826296)
('epoch', 337, 'train_loss:', 1.939500436782837, 'val_loss:', 0.49057950377464293)
('epoch', 338, 'train_loss:', 1.9381854760646819, 'val_loss:', 0.48968237519264224)
('epoch', 339, 'train_loss:', 1.9373713099956513, 'val_loss:', 0.48909346818923949)
('epoch', 340, 'train_loss:', 1.9349925386905671, 'val_loss:', 0.48923330307006835)
('epoch', 341, 'train_loss:', 1.9359486210346222, 'val_loss:', 0.49056868672370912)
('epoch', 342, 'train_loss:', 1.9350653898715973, 'val_loss:', 0.48960856676101683)
('epoch', 343, 'train_loss:', 1.9355606675148009, 'val_loss:', 0.49007728815078733)
('epoch', 344, 'train_loss:', 1.9293546724319457, 'val_loss:', 0.49050250172615051)
('epoch', 345, 'train_loss:', 1.9289896738529206, 'val_loss:', 0.48877266168594358)
('epoch', 346, 'train_loss:', 1.9311224377155305, 'val_loss:', 0.48874947071075442)
('epoch', 347, 'train_loss:', 1.9285997986793517, 'val_loss:', 0.48832255482673648)
('epoch', 348, 'train_loss:', 1.9293452703952789, 'val_loss:', 0.48879273653030397)
('epoch', 349, 'train_loss:', 1.9292211329936981, 'val_loss:', 0.48803222298622129)
('epoch', 350, 'train_loss:', 1.9222314703464507, 'val_loss:', 0.48958203196525574)
('epoch', 351, 'train_loss:', 1.9211793923377991, 'val_loss:', 0.48759323954582212)
('epoch', 352, 'train_loss:', 1.9253691780567168, 'val_loss:', 0.48810804128646851)
('epoch', 353, 'train_loss:', 1.922061287164688, 'val_loss:', 0.48767079114913942)
('epoch', 354, 'train_loss:', 1.9168342161178589, 'val_loss:', 0.48674446702003477)
('epoch', 355, 'train_loss:', 1.9228013145923615, 'val_loss:', 0.48659669160842894)
('epoch', 356, 'train_loss:', 1.9165886890888215, 'val_loss:', 0.48619243621826169)
('epoch', 357, 'train_loss:', 1.9154897558689117, 'val_loss:', 0.48501281857490541)
('epoch', 358, 'train_loss:', 1.9184796977043153, 'val_loss:', 0.4872685468196869)
('epoch', 359, 'train_loss:', 1.917826714515686, 'val_loss:', 0.48516517758369448)
('epoch', 360, 'train_loss:', 1.9117504894733428, 'val_loss:', 0.48548056244850157)
('epoch', 361, 'train_loss:', 1.9140395367145537, 'val_loss:', 0.48665700197219847)
('epoch', 362, 'train_loss:', 1.9120246350765229, 'val_loss:', 0.48507814288139345)
('epoch', 363, 'train_loss:', 1.910336161851883, 'val_loss:', 0.4849623692035675)
('epoch', 364, 'train_loss:', 1.9113594734668731, 'val_loss:', 0.4850998258590698)
('epoch', 365, 'train_loss:', 1.9071262729167939, 'val_loss:', 0.48354080438613889)
('epoch', 366, 'train_loss:', 1.9090931987762452, 'val_loss:', 0.4829506838321686)
('epoch', 367, 'train_loss:', 1.9099559199810028, 'val_loss:', 0.48479162812232973)
('epoch', 368, 'train_loss:', 1.9110582876205444, 'val_loss:', 0.48479657649993896)
('epoch', 369, 'train_loss:', 1.9060522794723511, 'val_loss:', 0.48684530496597289)
('epoch', 370, 'train_loss:', 1.908479903936386, 'val_loss:', 0.48341736316680906)
('epoch', 371, 'train_loss:', 1.9064946830272675, 'val_loss:', 0.4830558240413666)
('epoch', 372, 'train_loss:', 1.9030181181430816, 'val_loss:', 0.48303702712059021)
('epoch', 373, 'train_loss:', 1.8997663593292236, 'val_loss:', 0.48566181898117067)
('epoch', 374, 'train_loss:', 1.9005806064605713, 'val_loss:', 0.48355594396591184)
('epoch', 375, 'train_loss:', 1.9022665393352509, 'val_loss:', 0.48046357274055479)
('epoch', 376, 'train_loss:', 1.900200058221817, 'val_loss:', 0.48080124258995055)
('epoch', 377, 'train_loss:', 1.89931680560112, 'val_loss:', 0.48183379173278806)
('epoch', 378, 'train_loss:', 1.8960015797615051, 'val_loss:', 0.48123201012611388)
('epoch', 379, 'train_loss:', 1.8977625954151154, 'val_loss:', 0.48036100387573244)
('epoch', 380, 'train_loss:', 1.8960253965854645, 'val_loss:', 0.47964417695999145)
('epoch', 381, 'train_loss:', 1.8971437895298005, 'val_loss:', 0.48151555895805359)
('epoch', 382, 'train_loss:', 1.8939112365245818, 'val_loss:', 0.4826443552970886)
('epoch', 383, 'train_loss:', 1.8967725145816803, 'val_loss:', 0.48045525670051575)
('epoch', 384, 'train_loss:', 1.88978471159935, 'val_loss:', 0.48256720066070558)
('epoch', 385, 'train_loss:', 1.8910995745658874, 'val_loss:', 0.47948186397552489)
('epoch', 386, 'train_loss:', 1.8876304495334626, 'val_loss:', 0.48139046072959901)
('epoch', 387, 'train_loss:', 1.8887397742271423, 'val_loss:', 0.4803371298313141)
('epoch', 388, 'train_loss:', 1.8876272368431091, 'val_loss:', 0.48058905720710754)
('epoch', 389, 'train_loss:', 1.8866580379009248, 'val_loss:', 0.47845358848571778)
('epoch', 390, 'train_loss:', 1.8845919132232667, 'val_loss:', 0.47848540663719175)
('epoch', 391, 'train_loss:', 1.8872005724906922, 'val_loss:', 0.47889364123344419)
('epoch', 392, 'train_loss:', 1.8849009156227112, 'val_loss:', 0.47924610137939455)
('epoch', 393, 'train_loss:', 1.88519562125206, 'val_loss:', 0.4781603217124939)
('epoch', 394, 'train_loss:', 1.8847154402732849, 'val_loss:', 0.47906433224678041)
('epoch', 395, 'train_loss:', 1.8838343966007232, 'val_loss:', 0.47673816323280332)
('epoch', 396, 'train_loss:', 1.8839312398433685, 'val_loss:', 0.47576503872871401)
('epoch', 397, 'train_loss:', 1.8855235123634337, 'val_loss:', 0.47708764076232912)
('epoch', 398, 'train_loss:', 1.8848460066318511, 'val_loss:', 0.47699928402900693)
('epoch', 399, 'train_loss:', 1.8807939314842224, 'val_loss:', 0.47805092453956605)
('epoch', 400, 'train_loss:', 1.8809001076221465, 'val_loss:', 0.47681570529937745)
('epoch', 401, 'train_loss:', 1.8777293694019317, 'val_loss:', 0.4768876874446869)
('epoch', 402, 'train_loss:', 1.8778991329669952, 'val_loss:', 0.47698106765747073)
('epoch', 403, 'train_loss:', 1.8752430510520934, 'val_loss:', 0.47664874792098999)
('epoch', 404, 'train_loss:', 1.8752477061748505, 'val_loss:', 0.47498400807380675)
('epoch', 405, 'train_loss:', 1.8745350074768066, 'val_loss:', 0.47726048827171325)
('epoch', 406, 'train_loss:', 1.8755923235416412, 'val_loss:', 0.4774527335166931)
('epoch', 407, 'train_loss:', 1.8712457156181335, 'val_loss:', 0.47421481132507326)
('epoch', 408, 'train_loss:', 1.8746332132816315, 'val_loss:', 0.4763527262210846)
('epoch', 409, 'train_loss:', 1.870908774137497, 'val_loss:', 0.47646999359130859)
('epoch', 410, 'train_loss:', 1.8719184851646424, 'val_loss:', 0.47303373098373414)
('epoch', 411, 'train_loss:', 1.8719665753841399, 'val_loss:', 0.47370195031166079)
('epoch', 412, 'train_loss:', 1.8724748933315276, 'val_loss:', 0.47346063017845152)
('epoch', 413, 'train_loss:', 1.8679012596607207, 'val_loss:', 0.4739004147052765)
('epoch', 414, 'train_loss:', 1.8669459021091461, 'val_loss:', 0.47667131781578065)
('epoch', 415, 'train_loss:', 1.866704226732254, 'val_loss:', 0.47249838113784792)
('epoch', 416, 'train_loss:', 1.8628820943832398, 'val_loss:', 0.47426313400268555)
('epoch', 417, 'train_loss:', 1.8641105091571808, 'val_loss:', 0.47272214889526365)
('epoch', 418, 'train_loss:', 1.8676736855506897, 'val_loss:', 0.47349443078041076)
('epoch', 419, 'train_loss:', 1.8651810348033906, 'val_loss:', 0.47427093505859375)
('epoch', 420, 'train_loss:', 1.8644593203067779, 'val_loss:', 0.47311869502067566)
('epoch', 421, 'train_loss:', 1.8611608183383941, 'val_loss:', 0.47398121476173399)
('epoch', 422, 'train_loss:', 1.8647271120548248, 'val_loss:', 0.47301668524742124)
('epoch', 423, 'train_loss:', 1.8596693205833434, 'val_loss:', 0.47398579716682432)
('epoch', 424, 'train_loss:', 1.8622665536403655, 'val_loss:', 0.47143899679183959)
('epoch', 425, 'train_loss:', 1.8615210020542146, 'val_loss:', 0.47214226961135863)
('epoch', 426, 'train_loss:', 1.8558083415031432, 'val_loss:', 0.46957757592201232)
('epoch', 427, 'train_loss:', 1.8529481065273286, 'val_loss:', 0.47133158802986147)
('epoch', 428, 'train_loss:', 1.8552593874931336, 'val_loss:', 0.46984278082847597)
('epoch', 429, 'train_loss:', 1.8560898578166962, 'val_loss:', 0.47101854562759399)
('epoch', 430, 'train_loss:', 1.8535761368274688, 'val_loss:', 0.47203343868255615)
('epoch', 431, 'train_loss:', 1.8513962829113007, 'val_loss:', 0.46958983421325684)
('epoch', 432, 'train_loss:', 1.8533760654926299, 'val_loss:', 0.47143487095832826)
('epoch', 433, 'train_loss:', 1.8514805507659913, 'val_loss:', 0.4707253122329712)
('epoch', 434, 'train_loss:', 1.8519955444335938, 'val_loss:', 0.47096644401550292)
('epoch', 435, 'train_loss:', 1.8505261254310608, 'val_loss:', 0.47040361404418946)
('epoch', 436, 'train_loss:', 1.8521049010753632, 'val_loss:', 0.4705199086666107)
('epoch', 437, 'train_loss:', 1.8482695341110229, 'val_loss:', 0.47004561185836791)
('epoch', 438, 'train_loss:', 1.8513752627372742, 'val_loss:', 0.4703554594516754)
('epoch', 439, 'train_loss:', 1.8465405678749085, 'val_loss:', 0.47029595613479613)
('epoch', 440, 'train_loss:', 1.8474585056304931, 'val_loss:', 0.47089304924011233)
('epoch', 441, 'train_loss:', 1.8429110407829286, 'val_loss:', 0.47184452295303347)
('epoch', 442, 'train_loss:', 1.8482355380058288, 'val_loss:', 0.4706524407863617)
('epoch', 443, 'train_loss:', 1.8456337106227876, 'val_loss:', 0.46947995662689207)
('epoch', 444, 'train_loss:', 1.8445798647403717, 'val_loss:', 0.46914668560028078)
('epoch', 445, 'train_loss:', 1.8428977453708648, 'val_loss:', 0.47003779292106629)
('epoch', 446, 'train_loss:', 1.8444308054447174, 'val_loss:', 0.46835443735122678)
('epoch', 447, 'train_loss:', 1.845949386358261, 'val_loss:', 0.47011565923690796)
('epoch', 448, 'train_loss:', 1.84123997092247, 'val_loss:', 0.4709114158153534)
('epoch', 449, 'train_loss:', 1.840795270204544, 'val_loss:', 0.46710383176803588)
('epoch', 450, 'train_loss:', 1.8401465594768525, 'val_loss:', 0.46841616749763487)
('epoch', 451, 'train_loss:', 1.83915323138237, 'val_loss:', 0.46847697496414187)
('epoch', 452, 'train_loss:', 1.8403793752193451, 'val_loss:', 0.46898072361946108)
('epoch', 453, 'train_loss:', 1.8397901010513307, 'val_loss:', 0.46807319641113282)
('epoch', 454, 'train_loss:', 1.8369419741630555, 'val_loss:', 0.46812788844108583)
('epoch', 455, 'train_loss:', 1.8344114398956299, 'val_loss:', 0.46914268016815186)
('epoch', 456, 'train_loss:', 1.8352536559104919, 'val_loss:', 0.46727302789688108)
('epoch', 457, 'train_loss:', 1.8355349910259247, 'val_loss:', 0.46898640632629396)
('epoch', 458, 'train_loss:', 1.8331814146041869, 'val_loss:', 0.46890076994895935)
('epoch', 459, 'train_loss:', 1.8330923211574555, 'val_loss:', 0.46585160613059995)
('epoch', 460, 'train_loss:', 1.8360669088363648, 'val_loss:', 0.46811222076416015)
('epoch', 461, 'train_loss:', 1.8342740285396575, 'val_loss:', 0.46890150785446166)
('epoch', 462, 'train_loss:', 1.8314219725131988, 'val_loss:', 0.46640036821365355)
('epoch', 463, 'train_loss:', 1.8348032462596893, 'val_loss:', 0.46669380426406859)
('epoch', 464, 'train_loss:', 1.8319041895866395, 'val_loss:', 0.46788835287094116)
('epoch', 465, 'train_loss:', 1.8277787554264069, 'val_loss:', 0.46757553100585936)
('epoch', 466, 'train_loss:', 1.8312920701503754, 'val_loss:', 0.46645342946052554)
('epoch', 467, 'train_loss:', 1.8323619532585145, 'val_loss:', 0.4653693163394928)
('epoch', 468, 'train_loss:', 1.8239694654941558, 'val_loss:', 0.46596328616142274)
('epoch', 469, 'train_loss:', 1.8300966799259186, 'val_loss:', 0.46394899010658264)
('epoch', 470, 'train_loss:', 1.8244741177558899, 'val_loss:', 0.46484220147132871)
('epoch', 471, 'train_loss:', 1.8282269716262818, 'val_loss:', 0.46685089588165285)
('epoch', 472, 'train_loss:', 1.8250413346290588, 'val_loss:', 0.46135065793991087)
('epoch', 473, 'train_loss:', 1.8225915801525117, 'val_loss:', 0.46297411203384398)
('epoch', 474, 'train_loss:', 1.8271299684047699, 'val_loss:', 0.46573756456375121)
('epoch', 475, 'train_loss:', 1.8267932176589965, 'val_loss:', 0.46469871640205385)
('epoch', 476, 'train_loss:', 1.8232238948345185, 'val_loss:', 0.46590826869010926)
('epoch', 477, 'train_loss:', 1.8237691390514374, 'val_loss:', 0.46516777276992799)
('epoch', 478, 'train_loss:', 1.8217305052280426, 'val_loss:', 0.46331927299499509)
('epoch', 479, 'train_loss:', 1.8177349293231964, 'val_loss:', 0.4672183620929718)
('epoch', 480, 'train_loss:', 1.8221290659904481, 'val_loss:', 0.46393571376800535)
('epoch', 481, 'train_loss:', 1.8210770642757417, 'val_loss:', 0.46422898530960083)
('epoch', 482, 'train_loss:', 1.8188057315349579, 'val_loss:', 0.46407754540443419)
('epoch', 483, 'train_loss:', 1.8160544228553772, 'val_loss:', 0.46351142764091491)
('epoch', 484, 'train_loss:', 1.8174824178218842, 'val_loss:', 0.46488762259483335)
('epoch', 485, 'train_loss:', 1.8162179350852967, 'val_loss:', 0.46419702172279359)
('epoch', 486, 'train_loss:', 1.820329988002777, 'val_loss:', 0.462664395570755)
('epoch', 487, 'train_loss:', 1.814793232679367, 'val_loss:', 0.46228943347930906)
('epoch', 488, 'train_loss:', 1.8166868424415588, 'val_loss:', 0.46305313587188723)
('epoch', 489, 'train_loss:', 1.814997045993805, 'val_loss:', 0.46315510511398317)
('epoch', 490, 'train_loss:', 1.8110002768039704, 'val_loss:', 0.46302869915962219)
('epoch', 491, 'train_loss:', 1.8142360591888427, 'val_loss:', 0.46261132359504697)
('epoch', 492, 'train_loss:', 1.8155549013614654, 'val_loss:', 0.46361259818077089)
('epoch', 493, 'train_loss:', 1.8148456370830537, 'val_loss:', 0.46350440502166745)
('epoch', 494, 'train_loss:', 1.8133616948127746, 'val_loss:', 0.46070737957954405)
('epoch', 495, 'train_loss:', 1.8091213512420654, 'val_loss:', 0.46248151659965514)
('epoch', 496, 'train_loss:', 1.8084282302856445, 'val_loss:', 0.46004230380058286)
('epoch', 497, 'train_loss:', 1.8078488278388978, 'val_loss:', 0.46110733985900881)
('epoch', 498, 'train_loss:', 1.8132574999332427, 'val_loss:', 0.46006887078285219)
('epoch', 499, 'train_loss:', 1.8053106319904328, 'val_loss:', 0.46226336956024172)
('epoch', 500, 'train_loss:', 1.8120097529888153, 'val_loss:', 0.46070675492286683)
('epoch', 501, 'train_loss:', 1.8074224245548249, 'val_loss:', 0.46227387309074403)
('epoch', 502, 'train_loss:', 1.8046958291530608, 'val_loss:', 0.46199005484580996)
('epoch', 503, 'train_loss:', 1.8121107208728791, 'val_loss:', 0.46114548206329348)
('epoch', 504, 'train_loss:', 1.8062218356132507, 'val_loss:', 0.46088497877120971)
('epoch', 505, 'train_loss:', 1.8055838239192963, 'val_loss:', 0.46035176277160644)
('epoch', 506, 'train_loss:', 1.8046430253982544, 'val_loss:', 0.4604083478450775)
('epoch', 507, 'train_loss:', 1.8046970999240874, 'val_loss:', 0.46137904763221743)
('epoch', 508, 'train_loss:', 1.8018431520462037, 'val_loss:', 0.4595323610305786)
('epoch', 509, 'train_loss:', 1.8048394811153412, 'val_loss:', 0.46168911337852481)
('epoch', 510, 'train_loss:', 1.8022541344165801, 'val_loss:', 0.46060631275177)
('epoch', 511, 'train_loss:', 1.8010627436637878, 'val_loss:', 0.45726503014564512)
('epoch', 512, 'train_loss:', 1.8000244164466859, 'val_loss:', 0.45997392654418945)
('epoch', 513, 'train_loss:', 1.799222776889801, 'val_loss:', 0.45927716851234435)
('epoch', 514, 'train_loss:', 1.8002786314487458, 'val_loss:', 0.4594176983833313)
('epoch', 515, 'train_loss:', 1.8009163188934325, 'val_loss:', 0.45919219613075257)
('epoch', 516, 'train_loss:', 1.8003404474258422, 'val_loss:', 0.46137819647789002)
('epoch', 517, 'train_loss:', 1.7974041998386383, 'val_loss:', 0.45787917494773867)
('epoch', 518, 'train_loss:', 1.7960935270786285, 'val_loss:', 0.45928959012031556)
('epoch', 519, 'train_loss:', 1.7969875621795655, 'val_loss:', 0.46051206111907961)
('epoch', 520, 'train_loss:', 1.7964332830905914, 'val_loss:', 0.45930349230766299)
('epoch', 521, 'train_loss:', 1.7935083544254302, 'val_loss:', 0.45869833469390869)
('epoch', 522, 'train_loss:', 1.7969398784637451, 'val_loss:', 0.45897979259490967)
('epoch', 523, 'train_loss:', 1.7934130859374999, 'val_loss:', 0.46084669828414915)
('epoch', 524, 'train_loss:', 1.7941363382339477, 'val_loss:', 0.45733190655708311)
('epoch', 525, 'train_loss:', 1.7956658363342286, 'val_loss:', 0.45838229656219481)
('epoch', 526, 'train_loss:', 1.792035048007965, 'val_loss:', 0.45872154355049133)
('epoch', 527, 'train_loss:', 1.7908153223991394, 'val_loss:', 0.45864241003990175)
('epoch', 528, 'train_loss:', 1.7892955100536347, 'val_loss:', 0.45705157160758975)
('epoch', 529, 'train_loss:', 1.7907810401916504, 'val_loss:', 0.45802396178245547)
('epoch', 530, 'train_loss:', 1.7927661430835724, 'val_loss:', 0.45669516086578371)
('epoch', 531, 'train_loss:', 1.7910658776760102, 'val_loss:', 0.45874490022659303)
('epoch', 532, 'train_loss:', 1.7893515861034393, 'val_loss:', 0.45695399880409243)
('epoch', 533, 'train_loss:', 1.7895315408706665, 'val_loss:', 0.45622130513191222)
('epoch', 534, 'train_loss:', 1.7885917508602143, 'val_loss:', 0.4563568699359894)
('epoch', 535, 'train_loss:', 1.7859526765346527, 'val_loss:', 0.45673323988914488)
('epoch', 536, 'train_loss:', 1.7887596094608307, 'val_loss:', 0.45760954141616822)
('epoch', 537, 'train_loss:', 1.7912661492824555, 'val_loss:', 0.45764847755432131)
('epoch', 538, 'train_loss:', 1.7871095263957977, 'val_loss:', 0.45687153220176696)
('epoch', 539, 'train_loss:', 1.7848412156105042, 'val_loss:', 0.45959333181381223)
('epoch', 540, 'train_loss:', 1.7887423300743104, 'val_loss:', 0.45531828641891481)
('epoch', 541, 'train_loss:', 1.7858645451068877, 'val_loss:', 0.45711521983146669)
('epoch', 542, 'train_loss:', 1.7861800038814544, 'val_loss:', 0.45677653312683103)
('epoch', 543, 'train_loss:', 1.782154290676117, 'val_loss:', 0.45648544430732729)
('epoch', 544, 'train_loss:', 1.7835740280151366, 'val_loss:', 0.45898612856864929)
('epoch', 545, 'train_loss:', 1.7839152479171754, 'val_loss:', 0.45577874660491946)
('epoch', 546, 'train_loss:', 1.7824224257469177, 'val_loss:', 0.45658820748329165)
('epoch', 547, 'train_loss:', 1.7805141830444335, 'val_loss:', 0.45682480335235598)
('epoch', 548, 'train_loss:', 1.778990205526352, 'val_loss:', 0.45633132576942442)
('epoch', 549, 'train_loss:', 1.7815733444690705, 'val_loss:', 0.45677373170852659)
('epoch', 550, 'train_loss:', 1.7803555488586427, 'val_loss:', 0.45507668495178222)
('epoch', 551, 'train_loss:', 1.7820651257038116, 'val_loss:', 0.45500990271568298)
('epoch', 552, 'train_loss:', 1.7761929142475128, 'val_loss:', 0.4549669098854065)
('epoch', 553, 'train_loss:', 1.7785377669334412, 'val_loss:', 0.45718075752258303)
('epoch', 554, 'train_loss:', 1.775782675743103, 'val_loss:', 0.45586197257041933)
('epoch', 555, 'train_loss:', 1.778258661031723, 'val_loss:', 0.45536676406860349)
('epoch', 556, 'train_loss:', 1.7799698650836944, 'val_loss:', 0.45623521447181703)
('epoch', 557, 'train_loss:', 1.7760010957717896, 'val_loss:', 0.45460204362869261)
('epoch', 558, 'train_loss:', 1.7750448286533356, 'val_loss:', 0.45478323578834534)
('epoch', 559, 'train_loss:', 1.7760460567474365, 'val_loss:', 0.45459480047225953)
('epoch', 560, 'train_loss:', 1.7734098386764527, 'val_loss:', 0.45204174995422364)
('epoch', 561, 'train_loss:', 1.7789224803447723, 'val_loss:', 0.4556582748889923)
('epoch', 562, 'train_loss:', 1.7756744456291198, 'val_loss:', 0.45419766068458556)
('epoch', 563, 'train_loss:', 1.7736141526699065, 'val_loss:', 0.45307345747947692)
('epoch', 564, 'train_loss:', 1.7737516760826111, 'val_loss:', 0.45339675188064577)
('epoch', 565, 'train_loss:', 1.7745207023620606, 'val_loss:', 0.4544864797592163)
('epoch', 566, 'train_loss:', 1.7686003696918489, 'val_loss:', 0.45311708450317384)
('epoch', 567, 'train_loss:', 1.771661581993103, 'val_loss:', 0.45366852045059203)
('epoch', 568, 'train_loss:', 1.7714254534244538, 'val_loss:', 0.45548110485076904)
('epoch', 569, 'train_loss:', 1.7696975922584535, 'val_loss:', 0.45394953131675719)
('epoch', 570, 'train_loss:', 1.770209445953369, 'val_loss:', 0.45568476915359496)
('epoch', 571, 'train_loss:', 1.7702735614776612, 'val_loss:', 0.45574636936187746)
('epoch', 572, 'train_loss:', 1.7694028437137603, 'val_loss:', 0.45405127286911012)
('epoch', 573, 'train_loss:', 1.7675040662288666, 'val_loss:', 0.45476186037063598)
('epoch', 574, 'train_loss:', 1.766157068014145, 'val_loss:', 0.45582566499710081)
('epoch', 575, 'train_loss:', 1.7652042245864867, 'val_loss:', 0.45378072261810304)
('epoch', 576, 'train_loss:', 1.7693266761302948, 'val_loss:', 0.4538588571548462)
('epoch', 577, 'train_loss:', 1.7690575671195985, 'val_loss:', 0.4533458423614502)
('epoch', 578, 'train_loss:', 1.7631013333797454, 'val_loss:', 0.4548411536216736)
('epoch', 579, 'train_loss:', 1.7663039886951446, 'val_loss:', 0.45332839012145998)
('epoch', 580, 'train_loss:', 1.7662115144729613, 'val_loss:', 0.4503074586391449)
('epoch', 581, 'train_loss:', 1.7646614360809325, 'val_loss:', 0.45174092054367065)
('epoch', 582, 'train_loss:', 1.7650883328914642, 'val_loss:', 0.4543087947368622)
('epoch', 583, 'train_loss:', 1.7636211931705474, 'val_loss:', 0.45321063399314881)
('epoch', 584, 'train_loss:', 1.7638061130046845, 'val_loss:', 0.4503327441215515)
('epoch', 585, 'train_loss:', 1.7655310285091401, 'val_loss:', 0.45458636283874509)
('epoch', 586, 'train_loss:', 1.7648224258422851, 'val_loss:', 0.44997881650924682)
('epoch', 587, 'train_loss:', 1.7605253446102143, 'val_loss:', 0.45425832748413086)
('epoch', 588, 'train_loss:', 1.7622032845020295, 'val_loss:', 0.45445259451866149)
('epoch', 589, 'train_loss:', 1.7582584416866303, 'val_loss:', 0.44995349526405337)
('epoch', 590, 'train_loss:', 1.7614656627178191, 'val_loss:', 0.45168556928634646)
('epoch', 591, 'train_loss:', 1.7581941819190978, 'val_loss:', 0.45039324045181273)
('epoch', 592, 'train_loss:', 1.7582334697246551, 'val_loss:', 0.45128474235534666)
('epoch', 593, 'train_loss:', 1.7594912362098694, 'val_loss:', 0.45073593139648438)
('epoch', 594, 'train_loss:', 1.7573744165897369, 'val_loss:', 0.45039819359779359)
('epoch', 595, 'train_loss:', 1.7594408321380615, 'val_loss:', 0.45107141137123108)
('epoch', 596, 'train_loss:', 1.7541062486171723, 'val_loss:', 0.45088284254074096)
('epoch', 597, 'train_loss:', 1.7581992554664612, 'val_loss:', 0.45311930418014529)
('epoch', 598, 'train_loss:', 1.7582859230041503, 'val_loss:', 0.45072251558303833)
('epoch', 599, 'train_loss:', 1.7586072528362273, 'val_loss:', 0.45045691967010498)
