(keras)skoppula@sls-sm-7:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=1 python lstm_all_variants.py -t -d sms -v additive_no_sigmoid_relu6
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'sms', 'train': True, 'variant': 'additive_no_sigmoid_relu6', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (9292, 30), (9292, 30), (2323, 30), (2323, 30))
('num steps in trn and val epochs', 36, 9)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2680 get requests, put_count=2462 evicted_count=1000 eviction_rate=0.406174 and unsatisfied allocation rate=0.491791
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2798 get requests, put_count=2906 evicted_count=1000 eviction_rate=0.344116 and unsatisfied allocation rate=0.327019
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
('epoch', 0, 'train_loss:', 1.7668463277816773, 'val_loss:', 0.43287610530853271)
('epoch', 1, 'train_loss:', 1.7065628290176391, 'val_loss:', 0.42013626098632811)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 24474 get requests, put_count=24538 evicted_count=1000 eviction_rate=0.0407531 and unsatisfied allocation rate=0.0406554
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 2, 'train_loss:', 1.6613829088211061, 'val_loss:', 0.41016383647918703)
('epoch', 3, 'train_loss:', 1.6269659137725829, 'val_loss:', 0.4023052453994751)
('epoch', 4, 'train_loss:', 1.5981432247161864, 'val_loss:', 0.3957317781448364)
('epoch', 5, 'train_loss:', 1.5761837863922119, 'val_loss:', 0.39150074005126956)
('epoch', 6, 'train_loss:', 1.5560775899887085, 'val_loss:', 0.38653761863708497)
('epoch', 7, 'train_loss:', 1.5417642259597779, 'val_loss:', 0.38292902946472168)
('epoch', 8, 'train_loss:', 1.5293677711486817, 'val_loss:', 0.38011134147644043)
('epoch', 9, 'train_loss:', 1.5199473714828491, 'val_loss:', 0.37868270397186277)
('epoch', 10, 'train_loss:', 1.5119676494598389, 'val_loss:', 0.37615308761596677)
('epoch', 11, 'train_loss:', 1.5041478347778321, 'val_loss:', 0.37527806282043458)
('epoch', 12, 'train_loss:', 1.4985320329666139, 'val_loss:', 0.37370785713195803)
('epoch', 13, 'train_loss:', 1.4936094617843627, 'val_loss:', 0.37140936374664307)
('epoch', 14, 'train_loss:', 1.4886540365219116, 'val_loss:', 0.37108540534973145)
('epoch', 15, 'train_loss:', 1.486085238456726, 'val_loss:', 0.37112739086151125)
('epoch', 16, 'train_loss:', 1.4795438480377197, 'val_loss:', 0.36869880199432375)
('epoch', 17, 'train_loss:', 1.4771702861785889, 'val_loss:', 0.3680419921875)
('epoch', 18, 'train_loss:', 1.4738439702987671, 'val_loss:', 0.36720510959625247)
('epoch', 19, 'train_loss:', 1.4699079895019531, 'val_loss:', 0.36757328033447267)
('epoch', 20, 'train_loss:', 1.4683534193038941, 'val_loss:', 0.36641110420227052)
('epoch', 21, 'train_loss:', 1.4677411556243896, 'val_loss:', 0.36627322196960449)
('epoch', 22, 'train_loss:', 1.4632817506790161, 'val_loss:', 0.36496972560882568)
('epoch', 23, 'train_loss:', 1.4638594770431519, 'val_loss:', 0.3656899881362915)
('epoch', 24, 'train_loss:', 1.4599072027206421, 'val_loss:', 0.36479948520660399)
('epoch', 25, 'train_loss:', 1.4565153336524963, 'val_loss:', 0.3634550714492798)
('epoch', 26, 'train_loss:', 1.455901346206665, 'val_loss:', 0.36315576553344725)
('epoch', 27, 'train_loss:', 1.453783164024353, 'val_loss:', 0.36157305955886843)
('epoch', 28, 'train_loss:', 1.4513445043563842, 'val_loss:', 0.36261285305023194)
('epoch', 29, 'train_loss:', 1.4477588033676148, 'val_loss:', 0.36143115997314451)
('epoch', 30, 'train_loss:', 1.445348446369171, 'val_loss:', 0.36027602672576903)
('epoch', 31, 'train_loss:', 1.4401763844490052, 'val_loss:', 0.35891846656799314)
('epoch', 32, 'train_loss:', 1.43900541305542, 'val_loss:', 0.35927022695541383)
('epoch', 33, 'train_loss:', 1.4380906295776368, 'val_loss:', 0.35780287027359009)
('epoch', 34, 'train_loss:', 1.4309645247459413, 'val_loss:', 0.35708280086517336)
('epoch', 35, 'train_loss:', 1.4271766471862792, 'val_loss:', 0.35516879081726072)
('epoch', 36, 'train_loss:', 1.4209963035583497, 'val_loss:', 0.35305251121520997)
('epoch', 37, 'train_loss:', 1.4140370917320251, 'val_loss:', 0.35291439056396484)
('epoch', 38, 'train_loss:', 1.4039087891578674, 'val_loss:', 0.35071547508239748)
('epoch', 39, 'train_loss:', 1.3962280368804931, 'val_loss:', 0.34786265611648559)
('epoch', 40, 'train_loss:', 1.3890614104270935, 'val_loss:', 0.34563849925994872)
('epoch', 41, 'train_loss:', 1.3795424199104309, 'val_loss:', 0.34391624689102174)
('epoch', 42, 'train_loss:', 1.3704132795333863, 'val_loss:', 0.34061432123184204)
('epoch', 43, 'train_loss:', 1.3576456522941589, 'val_loss:', 0.33765602350234986)
('epoch', 44, 'train_loss:', 1.3517413759231567, 'val_loss:', 0.33602812528610232)
('epoch', 45, 'train_loss:', 1.3377828097343445, 'val_loss:', 0.33266018867492675)
('epoch', 46, 'train_loss:', 1.3299043297767639, 'val_loss:', 0.32952912330627443)
('epoch', 47, 'train_loss:', 1.3172308444976806, 'val_loss:', 0.32930836439132688)
('epoch', 48, 'train_loss:', 1.3086932516098022, 'val_loss:', 0.32433804273605349)
('epoch', 49, 'train_loss:', 1.2985352659225464, 'val_loss:', 0.32274181604385377)
('epoch', 50, 'train_loss:', 1.2881802773475648, 'val_loss:', 0.32310843944549561)
('epoch', 51, 'train_loss:', 1.2816509795188904, 'val_loss:', 0.32023315429687499)
('epoch', 52, 'train_loss:', 1.2721968007087707, 'val_loss:', 0.31686730384826661)
('epoch', 53, 'train_loss:', 1.2655562305450438, 'val_loss:', 0.31553625822067261)
('epoch', 54, 'train_loss:', 1.2623498702049256, 'val_loss:', 0.31223855495452879)
('epoch', 55, 'train_loss:', 1.250073094367981, 'val_loss:', 0.31132443666458132)
('epoch', 56, 'train_loss:', 1.2460211420059204, 'val_loss:', 0.31007957696914673)
('epoch', 57, 'train_loss:', 1.23534503698349, 'val_loss:', 0.30859151363372805)
('epoch', 58, 'train_loss:', 1.2310820126533508, 'val_loss:', 0.30699275255203246)
('epoch', 59, 'train_loss:', 1.2224573493003845, 'val_loss:', 0.30591507434844972)
('epoch', 60, 'train_loss:', 1.2158044004440307, 'val_loss:', 0.30456780433654784)
('epoch', 61, 'train_loss:', 1.2144307518005371, 'val_loss:', 0.3023654341697693)
('epoch', 62, 'train_loss:', 1.2092298722267152, 'val_loss:', 0.30133765935897827)
('epoch', 63, 'train_loss:', 1.2020660018920899, 'val_loss:', 0.29878228664398193)
('epoch', 64, 'train_loss:', 1.2000966739654542, 'val_loss:', 0.29685885190963746)
('epoch', 65, 'train_loss:', 1.1955515980720519, 'val_loss:', 0.29644674062728882)
('epoch', 66, 'train_loss:', 1.1908034038543702, 'val_loss:', 0.29439271450042725)
('epoch', 67, 'train_loss:', 1.1833703374862672, 'val_loss:', 0.2963428044319153)
('epoch', 68, 'train_loss:', 1.1779762172698975, 'val_loss:', 0.29428659915924071)
('epoch', 69, 'train_loss:', 1.1709877610206605, 'val_loss:', 0.29141177892684939)
('epoch', 70, 'train_loss:', 1.1709485077857971, 'val_loss:', 0.29238054275512693)
('epoch', 71, 'train_loss:', 1.1658511900901793, 'val_loss:', 0.29130614519119263)
('epoch', 72, 'train_loss:', 1.161910035610199, 'val_loss:', 0.28949823379516604)
('epoch', 73, 'train_loss:', 1.1613928651809693, 'val_loss:', 0.29061893701553343)
('epoch', 74, 'train_loss:', 1.1537935495376588, 'val_loss:', 0.28757445335388182)
('epoch', 75, 'train_loss:', 1.1510488486289978, 'val_loss:', 0.28677504539489745)
('epoch', 76, 'train_loss:', 1.1482069945335389, 'val_loss:', 0.28508176326751711)
('epoch', 77, 'train_loss:', 1.1463934636116029, 'val_loss:', 0.28595249414443968)
('epoch', 78, 'train_loss:', 1.1440205025672912, 'val_loss:', 0.28415167331695557)
('epoch', 79, 'train_loss:', 1.1420147609710694, 'val_loss:', 0.28353579044342042)
('epoch', 80, 'train_loss:', 1.133872730731964, 'val_loss:', 0.28406015396118162)
('epoch', 81, 'train_loss:', 1.1302089357376099, 'val_loss:', 0.28440468072891234)
('epoch', 82, 'train_loss:', 1.1310347151756286, 'val_loss:', 0.28304337739944457)
('epoch', 83, 'train_loss:', 1.1260112595558167, 'val_loss:', 0.27966527462005614)
('epoch', 84, 'train_loss:', 1.1268916606903077, 'val_loss:', 0.28056362867355344)
('epoch', 85, 'train_loss:', 1.1220721149444579, 'val_loss:', 0.28109337091445924)
('epoch', 86, 'train_loss:', 1.1190750670433045, 'val_loss:', 0.27953295946121215)
('epoch', 87, 'train_loss:', 1.1140175986289977, 'val_loss:', 0.27780385971069338)
('epoch', 88, 'train_loss:', 1.117814462184906, 'val_loss:', 0.27721803426742553)
('epoch', 89, 'train_loss:', 1.1102822947502136, 'val_loss:', 0.27543676137924195)
('epoch', 90, 'train_loss:', 1.10677095413208, 'val_loss:', 0.27583117961883546)
('epoch', 91, 'train_loss:', 1.1067232513427734, 'val_loss:', 0.27369774818420411)
('epoch', 92, 'train_loss:', 1.1019744014739989, 'val_loss:', 0.27702682256698608)
('epoch', 93, 'train_loss:', 1.0981584119796752, 'val_loss:', 0.27576692581176759)
('epoch', 94, 'train_loss:', 1.0943538427352906, 'val_loss:', 0.27395057439804077)
('epoch', 95, 'train_loss:', 1.093414306640625, 'val_loss:', 0.27169870376586913)
('epoch', 96, 'train_loss:', 1.0892583799362183, 'val_loss:', 0.27386216402053831)
('epoch', 97, 'train_loss:', 1.0893546152114868, 'val_loss:', 0.27364997386932371)
('epoch', 98, 'train_loss:', 1.0845868301391601, 'val_loss:', 0.27142938137054445)
('epoch', 99, 'train_loss:', 1.07889666557312, 'val_loss:', 0.26978204965591429)
('epoch', 100, 'train_loss:', 1.0782877612113952, 'val_loss:', 0.27047724962234498)
('epoch', 101, 'train_loss:', 1.0793163657188416, 'val_loss:', 0.26731615543365478)
('epoch', 102, 'train_loss:', 1.0774080014228822, 'val_loss:', 0.26890616178512572)
('epoch', 103, 'train_loss:', 1.0700290203094482, 'val_loss:', 0.26743477821350098)
('epoch', 104, 'train_loss:', 1.0665187263488769, 'val_loss:', 0.26666357755661013)
('epoch', 105, 'train_loss:', 1.0662112712860108, 'val_loss:', 0.26708329200744629)
('epoch', 106, 'train_loss:', 1.0668214130401612, 'val_loss:', 0.26555518150329588)
('epoch', 107, 'train_loss:', 1.0651961660385132, 'val_loss:', 0.26441792249679563)
('epoch', 108, 'train_loss:', 1.0563413405418396, 'val_loss:', 0.2633328723907471)
('epoch', 109, 'train_loss:', 1.0543442869186401, 'val_loss:', 0.26273123025894163)
('epoch', 110, 'train_loss:', 1.0553933668136597, 'val_loss:', 0.26387790441513059)
('epoch', 111, 'train_loss:', 1.047686767578125, 'val_loss:', 0.26195951938629153)
('epoch', 112, 'train_loss:', 1.0508791732788085, 'val_loss:', 0.26264699935913088)
('epoch', 113, 'train_loss:', 1.0445925354957581, 'val_loss:', 0.26218485832214355)
('epoch', 114, 'train_loss:', 1.0430985379219055, 'val_loss:', 0.26066201210021972)
('epoch', 115, 'train_loss:', 1.0414434719085692, 'val_loss:', 0.26067751884460449)
('epoch', 116, 'train_loss:', 1.0422170114517213, 'val_loss:', 0.26067083597183227)
('epoch', 117, 'train_loss:', 1.0371521997451782, 'val_loss:', 0.25955284357070924)
('epoch', 118, 'train_loss:', 1.0363646578788757, 'val_loss:', 0.25748788833618164)
('epoch', 119, 'train_loss:', 1.0348001599311829, 'val_loss:', 0.25844290733337405)
('epoch', 120, 'train_loss:', 1.0331007099151612, 'val_loss:', 0.25755672931671142)
('epoch', 121, 'train_loss:', 1.0277813172340393, 'val_loss:', 0.25594150304794311)
('epoch', 122, 'train_loss:', 1.0280314087867737, 'val_loss:', 0.25574032783508299)
('epoch', 123, 'train_loss:', 1.0229588055610657, 'val_loss:', 0.25718483209609988)
('epoch', 124, 'train_loss:', 1.0237017273902893, 'val_loss:', 0.25561081886291503)
('epoch', 125, 'train_loss:', 1.0211840057373047, 'val_loss:', 0.25648045301437378)
('epoch', 126, 'train_loss:', 1.0237961101531983, 'val_loss:', 0.25592715740203859)
('epoch', 127, 'train_loss:', 1.0166493606567384, 'val_loss:', 0.25497745513916015)
('epoch', 128, 'train_loss:', 1.01692284822464, 'val_loss:', 0.25341111183166504)
('epoch', 129, 'train_loss:', 1.0200679993629456, 'val_loss:', 0.2534813070297241)
('epoch', 130, 'train_loss:', 1.0134506678581239, 'val_loss:', 0.25334187746047976)
('epoch', 131, 'train_loss:', 1.0139701294898986, 'val_loss:', 0.25127735137939455)
('epoch', 132, 'train_loss:', 1.0081091475486756, 'val_loss:', 0.25271354913711547)
('epoch', 133, 'train_loss:', 1.006641902923584, 'val_loss:', 0.25163916587829588)
('epoch', 134, 'train_loss:', 1.0039367079734802, 'val_loss:', 0.25269466400146484)
('epoch', 135, 'train_loss:', 1.0025206947326659, 'val_loss:', 0.25172284603118894)
('epoch', 136, 'train_loss:', 1.0051345396041871, 'val_loss:', 0.25103751659393309)
('epoch', 137, 'train_loss:', 1.0011743211746216, 'val_loss:', 0.25116730213165284)
('epoch', 138, 'train_loss:', 1.0009630203247071, 'val_loss:', 0.24943314790725707)
('epoch', 139, 'train_loss:', 0.99342379331588748, 'val_loss:', 0.25073782444000242)
('epoch', 140, 'train_loss:', 0.98843046665191647, 'val_loss:', 0.24802036285400392)
('epoch', 141, 'train_loss:', 0.99059251308441165, 'val_loss:', 0.24822150468826293)
('epoch', 142, 'train_loss:', 0.99186557054519653, 'val_loss:', 0.24872439622879028)
('epoch', 143, 'train_loss:', 0.98707407236099243, 'val_loss:', 0.24741300344467163)
('epoch', 144, 'train_loss:', 0.98604272127151493, 'val_loss:', 0.24503265857696532)
('epoch', 145, 'train_loss:', 0.98851470470428471, 'val_loss:', 0.24648378372192384)
('epoch', 146, 'train_loss:', 0.98328624248504637, 'val_loss:', 0.24474531173706054)
('epoch', 147, 'train_loss:', 0.98371205568313602, 'val_loss:', 0.24795231103897095)
('epoch', 148, 'train_loss:', 0.98071278810501095, 'val_loss:', 0.24662922143936158)
('epoch', 149, 'train_loss:', 0.97869236946105953, 'val_loss:', 0.24382809638977052)
('epoch', 150, 'train_loss:', 0.98038218498229979, 'val_loss:', 0.24413668394088744)
('epoch', 151, 'train_loss:', 0.97235511541366582, 'val_loss:', 0.24414392948150634)
('epoch', 152, 'train_loss:', 0.97586827039718627, 'val_loss:', 0.24376307725906371)
('epoch', 153, 'train_loss:', 0.96949545383453373, 'val_loss:', 0.24260270357131958)
('epoch', 154, 'train_loss:', 0.97336283445358274, 'val_loss:', 0.24308673381805421)
('epoch', 155, 'train_loss:', 0.96820835828781127, 'val_loss:', 0.24136598587036132)
('epoch', 156, 'train_loss:', 0.9661785697937012, 'val_loss:', 0.24443612337112428)
('epoch', 157, 'train_loss:', 0.9645316290855408, 'val_loss:', 0.24152538299560547)
('epoch', 158, 'train_loss:', 0.96265470504760742, 'val_loss:', 0.24056265592575074)
('epoch', 159, 'train_loss:', 0.96351774692535397, 'val_loss:', 0.24108543395996093)
('epoch', 160, 'train_loss:', 0.96121124267578129, 'val_loss:', 0.24070457935333253)
('epoch', 161, 'train_loss:', 0.95642962694168088, 'val_loss:', 0.24054693460464477)
('epoch', 162, 'train_loss:', 0.95890969276428217, 'val_loss:', 0.24056464195251465)
('epoch', 163, 'train_loss:', 0.95775315046310427, 'val_loss:', 0.23968358755111693)
('epoch', 164, 'train_loss:', 0.95540930032730098, 'val_loss:', 0.23942136287689209)
('epoch', 165, 'train_loss:', 0.95328363656997683, 'val_loss:', 0.23819456815719606)
('epoch', 166, 'train_loss:', 0.94986232757568356, 'val_loss:', 0.23845121622085572)
('epoch', 167, 'train_loss:', 0.94845287561416625, 'val_loss:', 0.23861429452896118)
('epoch', 168, 'train_loss:', 0.9499507784843445, 'val_loss:', 0.23593727588653565)
('epoch', 169, 'train_loss:', 0.95041308164596561, 'val_loss:', 0.23487812280654907)
('epoch', 170, 'train_loss:', 0.94521527290344243, 'val_loss:', 0.23554139614105224)
('epoch', 171, 'train_loss:', 0.94464741468429569, 'val_loss:', 0.23655258417129515)
('epoch', 172, 'train_loss:', 0.94333371877670291, 'val_loss:', 0.23649307012557982)
('epoch', 173, 'train_loss:', 0.94332497358322143, 'val_loss:', 0.2342788314819336)
('epoch', 174, 'train_loss:', 0.94144336700439457, 'val_loss:', 0.23648253917694093)
('epoch', 175, 'train_loss:', 0.93599590539932254, 'val_loss:', 0.23500471353530883)
('epoch', 176, 'train_loss:', 0.9397832036018372, 'val_loss:', 0.23629170656204224)
('epoch', 177, 'train_loss:', 0.93492118120193479, 'val_loss:', 0.23301507949829101)
('epoch', 178, 'train_loss:', 0.93302175998687742, 'val_loss:', 0.23385545015335082)
('epoch', 179, 'train_loss:', 0.93236036062240601, 'val_loss:', 0.23340869426727295)
('epoch', 180, 'train_loss:', 0.93431334972381597, 'val_loss:', 0.23532438516616822)
('epoch', 181, 'train_loss:', 0.93382781744003296, 'val_loss:', 0.23126523971557617)
('epoch', 182, 'train_loss:', 0.92913702726364134, 'val_loss:', 0.2315336585044861)
('epoch', 183, 'train_loss:', 0.92630475521087652, 'val_loss:', 0.23085678339004517)
('epoch', 184, 'train_loss:', 0.92873114824295044, 'val_loss:', 0.22980266809463501)
('epoch', 185, 'train_loss:', 0.92573288440704349, 'val_loss:', 0.2317358946800232)
('epoch', 186, 'train_loss:', 0.92284429550170899, 'val_loss:', 0.23191695451736449)
('epoch', 187, 'train_loss:', 0.91838054418563841, 'val_loss:', 0.22982681751251222)
('epoch', 188, 'train_loss:', 0.91880005598068237, 'val_loss:', 0.23247877359390259)
('epoch', 189, 'train_loss:', 0.92050542116165157, 'val_loss:', 0.22971980333328246)
('epoch', 190, 'train_loss:', 0.91767222166061402, 'val_loss:', 0.22899939060211183)
('epoch', 191, 'train_loss:', 0.91667994260787966, 'val_loss:', 0.23088964939117432)
('epoch', 192, 'train_loss:', 0.91630897760391239, 'val_loss:', 0.22697740316390991)
('epoch', 193, 'train_loss:', 0.91270714282989507, 'val_loss:', 0.22875601053237915)
('epoch', 194, 'train_loss:', 0.91345246553421022, 'val_loss:', 0.22819965362548827)
('epoch', 195, 'train_loss:', 0.9090798926353455, 'val_loss:', 0.22751307725906372)
('epoch', 196, 'train_loss:', 0.91026527166366578, 'val_loss:', 0.22723661422729491)
('epoch', 197, 'train_loss:', 0.90876397609710691, 'val_loss:', 0.22891109466552734)
('epoch', 198, 'train_loss:', 0.91078612089157107, 'val_loss:', 0.22598712921142577)
('epoch', 199, 'train_loss:', 0.9031582856178284, 'val_loss:', 0.22607870578765868)
('epoch', 200, 'train_loss:', 0.90454628467559817, 'val_loss:', 0.22613268136978149)
('epoch', 201, 'train_loss:', 0.90386810541152951, 'val_loss:', 0.22641971588134766)
('epoch', 202, 'train_loss:', 0.90184122085571294, 'val_loss:', 0.22477924108505248)
('epoch', 203, 'train_loss:', 0.90458028078079222, 'val_loss:', 0.22636935710906983)
('epoch', 204, 'train_loss:', 0.8974039816856384, 'val_loss:', 0.22551075696945191)
('epoch', 205, 'train_loss:', 0.89599802970886233, 'val_loss:', 0.22593924999237061)
('epoch', 206, 'train_loss:', 0.89783186674118043, 'val_loss:', 0.2245338249206543)
('epoch', 207, 'train_loss:', 0.89713342189788814, 'val_loss:', 0.22230205774307252)
('epoch', 208, 'train_loss:', 0.89668199300765994, 'val_loss:', 0.22269655227661134)
('epoch', 209, 'train_loss:', 0.89253445863723757, 'val_loss:', 0.22387630701065064)
('epoch', 210, 'train_loss:', 0.89391053676605225, 'val_loss:', 0.22401800394058227)
('epoch', 211, 'train_loss:', 0.89094264507293697, 'val_loss:', 0.22363099098205566)
('epoch', 212, 'train_loss:', 0.88985487937927243, 'val_loss:', 0.22377195596694946)
('epoch', 213, 'train_loss:', 0.88992154121398925, 'val_loss:', 0.22049713134765625)
('epoch', 214, 'train_loss:', 0.88919399499893192, 'val_loss:', 0.22199664115905762)
('epoch', 215, 'train_loss:', 0.8890802764892578, 'val_loss:', 0.22258618831634522)
('epoch', 216, 'train_loss:', 0.88778620719909673, 'val_loss:', 0.22140135526657104)
('epoch', 217, 'train_loss:', 0.88791149377822876, 'val_loss:', 0.22171642541885375)
('epoch', 218, 'train_loss:', 0.88522792100906367, 'val_loss:', 0.22281936883926393)
('epoch', 219, 'train_loss:', 0.88320317745208743, 'val_loss:', 0.22081343173980714)
('epoch', 220, 'train_loss:', 0.88266728878021239, 'val_loss:', 0.22106213331222535)
('epoch', 221, 'train_loss:', 0.88152814626693721, 'val_loss:', 0.22120107412338258)
('epoch', 222, 'train_loss:', 0.87751955270767212, 'val_loss:', 0.21805932521820068)
('epoch', 223, 'train_loss:', 0.88147665023803712, 'val_loss:', 0.22015901088714598)
('epoch', 224, 'train_loss:', 0.87518515110015871, 'val_loss:', 0.22005010128021241)
('epoch', 225, 'train_loss:', 0.87884387016296384, 'val_loss:', 0.21907610177993775)
('epoch', 226, 'train_loss:', 0.87735722064971922, 'val_loss:', 0.21913241863250732)
('epoch', 227, 'train_loss:', 0.8751487135887146, 'val_loss:', 0.21896317481994629)
('epoch', 228, 'train_loss:', 0.87370590448379515, 'val_loss:', 0.2182352375984192)
('epoch', 229, 'train_loss:', 0.8734754180908203, 'val_loss:', 0.2187701916694641)
('epoch', 230, 'train_loss:', 0.87201702117919921, 'val_loss:', 0.21733572244644164)
('epoch', 231, 'train_loss:', 0.8697914814949036, 'val_loss:', 0.21623480081558227)
('epoch', 232, 'train_loss:', 0.86884131908416751, 'val_loss:', 0.21765291213989257)
('epoch', 233, 'train_loss:', 0.86381977796554565, 'val_loss:', 0.21797362565994263)
('epoch', 234, 'train_loss:', 0.869845073223114, 'val_loss:', 0.21812408924102783)
('epoch', 235, 'train_loss:', 0.86428030729293825, 'val_loss:', 0.21652963399887085)
('epoch', 236, 'train_loss:', 0.86728594541549686, 'val_loss:', 0.21520051717758179)
('epoch', 237, 'train_loss:', 0.86646904230117794, 'val_loss:', 0.21567063808441161)
('epoch', 238, 'train_loss:', 0.85949164152145385, 'val_loss:', 0.21792967557907106)
('epoch', 239, 'train_loss:', 0.86281737804412839, 'val_loss:', 0.21505686521530151)
('epoch', 240, 'train_loss:', 0.86549931049346929, 'val_loss:', 0.21469897985458375)
('epoch', 241, 'train_loss:', 0.86187633991241452, 'val_loss:', 0.21485923767089843)
('epoch', 242, 'train_loss:', 0.85694491147995, 'val_loss:', 0.2138378405570984)
('epoch', 243, 'train_loss:', 0.85721574306488035, 'val_loss:', 0.21571485280990602)
('epoch', 244, 'train_loss:', 0.8574914121627808, 'val_loss:', 0.21645657539367677)
('epoch', 245, 'train_loss:', 0.85711746931076049, 'val_loss:', 0.21372669696807861)
('epoch', 246, 'train_loss:', 0.85636119365692143, 'val_loss:', 0.2163946008682251)
('epoch', 247, 'train_loss:', 0.85232949495315546, 'val_loss:', 0.2144042992591858)
('epoch', 248, 'train_loss:', 0.85497432470321655, 'val_loss:', 0.21456822872161865)
('epoch', 249, 'train_loss:', 0.85638135910034174, 'val_loss:', 0.21200411558151244)
('epoch', 250, 'train_loss:', 0.85069665193557742, 'val_loss:', 0.21163792371749879)
('epoch', 251, 'train_loss:', 0.84969253301620484, 'val_loss:', 0.21141743421554565)
('epoch', 252, 'train_loss:', 0.85151984214782717, 'val_loss:', 0.21238895177841186)
('epoch', 253, 'train_loss:', 0.84995009422302248, 'val_loss:', 0.21188820600509645)
('epoch', 254, 'train_loss:', 0.84453600645065308, 'val_loss:', 0.21217289209365844)
('epoch', 255, 'train_loss:', 0.84694352865219114, 'val_loss:', 0.21135260105133058)
('epoch', 256, 'train_loss:', 0.84380957126617429, 'val_loss:', 0.21214293956756591)
('epoch', 257, 'train_loss:', 0.84470876693725583, 'val_loss:', 0.21239511966705321)
('epoch', 258, 'train_loss:', 0.84152813196182252, 'val_loss:', 0.20996130943298341)
('epoch', 259, 'train_loss:', 0.84173780679702759, 'val_loss:', 0.210718834400177)
('epoch', 260, 'train_loss:', 0.83885597944259649, 'val_loss:', 0.20911773681640625)
('epoch', 261, 'train_loss:', 0.84111950159072879, 'val_loss:', 0.21087543725967406)
('epoch', 262, 'train_loss:', 0.8382702326774597, 'val_loss:', 0.20927920818328857)
('epoch', 263, 'train_loss:', 0.83965180158615116, 'val_loss:', 0.20876537322998046)
('epoch', 264, 'train_loss:', 0.83850752592086797, 'val_loss:', 0.20935377359390259)
('epoch', 265, 'train_loss:', 0.83915116310119631, 'val_loss:', 0.21168601274490356)
('epoch', 266, 'train_loss:', 0.83834511756896968, 'val_loss:', 0.20987080097198485)
('epoch', 267, 'train_loss:', 0.83669603347778321, 'val_loss:', 0.20928366661071776)
('epoch', 268, 'train_loss:', 0.83540605783462529, 'val_loss:', 0.20752370595932007)
('epoch', 269, 'train_loss:', 0.83171792507171627, 'val_loss:', 0.20819592237472534)
('epoch', 270, 'train_loss:', 0.83636790752410883, 'val_loss:', 0.21070802927017213)
('epoch', 271, 'train_loss:', 0.83233867645263671, 'val_loss:', 0.20958287000656128)
('epoch', 272, 'train_loss:', 0.83314666748046873, 'val_loss:', 0.20915848016738892)
('epoch', 273, 'train_loss:', 0.82815720558166506, 'val_loss:', 0.20726740598678589)
('epoch', 274, 'train_loss:', 0.82889089822769169, 'val_loss:', 0.20804942131042481)
('epoch', 275, 'train_loss:', 0.8282745671272278, 'val_loss:', 0.20650826692581176)
('epoch', 276, 'train_loss:', 0.82851226568222047, 'val_loss:', 0.21041429042816162)
('epoch', 277, 'train_loss:', 0.82674193859100342, 'val_loss:', 0.2076592493057251)
('epoch', 278, 'train_loss:', 0.82369441032409663, 'val_loss:', 0.20401153326034546)
('epoch', 279, 'train_loss:', 0.82550759315490718, 'val_loss:', 0.20573831796646119)
('epoch', 280, 'train_loss:', 0.82377059221267701, 'val_loss:', 0.20582853078842164)
('epoch', 281, 'train_loss:', 0.82726935148239134, 'val_loss:', 0.20535381078720094)
('epoch', 282, 'train_loss:', 0.82263464450836177, 'val_loss:', 0.2074584197998047)
('epoch', 283, 'train_loss:', 0.81787330389022828, 'val_loss:', 0.20524578809738159)
('epoch', 284, 'train_loss:', 0.82078464508056637, 'val_loss:', 0.20549538850784302)
('epoch', 285, 'train_loss:', 0.81753642082214351, 'val_loss:', 0.20543706178665161)
('epoch', 286, 'train_loss:', 0.81644580602645878, 'val_loss:', 0.20459665536880492)
('epoch', 287, 'train_loss:', 0.81805502891540527, 'val_loss:', 0.203792085647583)
('epoch', 288, 'train_loss:', 0.81351667165756225, 'val_loss:', 0.20679499149322511)
('epoch', 289, 'train_loss:', 0.81477998495101933, 'val_loss:', 0.20486583948135376)
('epoch', 290, 'train_loss:', 0.81320451974868779, 'val_loss:', 0.20611167430877686)
('epoch', 291, 'train_loss:', 0.81797740221023563, 'val_loss:', 0.20417241334915162)
('epoch', 292, 'train_loss:', 0.81400400400161743, 'val_loss:', 0.20515164613723755)
('epoch', 293, 'train_loss:', 0.81496945381164554, 'val_loss:', 0.20256001472473145)
('epoch', 294, 'train_loss:', 0.81023169279098506, 'val_loss:', 0.20408417224884035)
('epoch', 295, 'train_loss:', 0.81397050619125366, 'val_loss:', 0.20175933361053466)
('epoch', 296, 'train_loss:', 0.80868076086044316, 'val_loss:', 0.20129877328872681)
('epoch', 297, 'train_loss:', 0.81013436794281002, 'val_loss:', 0.20343768835067749)
('epoch', 298, 'train_loss:', 0.80468833446502686, 'val_loss:', 0.20384812116622925)
('epoch', 299, 'train_loss:', 0.80725207567214963, 'val_loss:', 0.20386879682540893)
('epoch', 300, 'train_loss:', 0.81125003099441528, 'val_loss:', 0.20499732971191406)
('epoch', 301, 'train_loss:', 0.80731653451919561, 'val_loss:', 0.20248681545257569)
('epoch', 302, 'train_loss:', 0.80286045312881471, 'val_loss:', 0.20270215988159179)
('epoch', 303, 'train_loss:', 0.80325091361999512, 'val_loss:', 0.20074287652969361)
('epoch', 304, 'train_loss:', 0.80136048555374151, 'val_loss:', 0.20082960605621339)
('epoch', 305, 'train_loss:', 0.80380147933959956, 'val_loss:', 0.2011653971672058)
('epoch', 306, 'train_loss:', 0.80042793989181515, 'val_loss:', 0.19982549667358399)
('epoch', 307, 'train_loss:', 0.80313330650329595, 'val_loss:', 0.20046388387680053)
('epoch', 308, 'train_loss:', 0.79666779994964598, 'val_loss:', 0.20123524427413941)
('epoch', 309, 'train_loss:', 0.8008808398246765, 'val_loss:', 0.19923512220382691)
('epoch', 310, 'train_loss:', 0.7966397166252136, 'val_loss:', 0.19955608129501343)
('epoch', 311, 'train_loss:', 0.7964128470420837, 'val_loss:', 0.19974702358245849)
('epoch', 312, 'train_loss:', 0.79263829469680791, 'val_loss:', 0.19883068799972534)
('epoch', 313, 'train_loss:', 0.79567383289337157, 'val_loss:', 0.19955764770507811)
('epoch', 314, 'train_loss:', 0.79252134084701542, 'val_loss:', 0.19884465456008912)
('epoch', 315, 'train_loss:', 0.79409861326217657, 'val_loss:', 0.19611480474472046)
('epoch', 316, 'train_loss:', 0.79154552221298213, 'val_loss:', 0.19799081325531007)
('epoch', 317, 'train_loss:', 0.79143843412399295, 'val_loss:', 0.19920079231262208)
('epoch', 318, 'train_loss:', 0.78983310699462894, 'val_loss:', 0.19799275875091552)
('epoch', 319, 'train_loss:', 0.78750226497650144, 'val_loss:', 0.19796541929244996)
('epoch', 320, 'train_loss:', 0.78841615438461299, 'val_loss:', 0.19663905143737792)
('epoch', 321, 'train_loss:', 0.78534647703170779, 'val_loss:', 0.1986897611618042)
('epoch', 322, 'train_loss:', 0.78582648277282718, 'val_loss:', 0.19753790140151978)
('epoch', 323, 'train_loss:', 0.78477001428604121, 'val_loss:', 0.19624487876892091)
('epoch', 324, 'train_loss:', 0.78566189289093014, 'val_loss:', 0.19792147636413573)
('epoch', 325, 'train_loss:', 0.78606721401214597, 'val_loss:', 0.1983916926383972)
('epoch', 326, 'train_loss:', 0.78458439350128173, 'val_loss:', 0.19665281057357789)
('epoch', 327, 'train_loss:', 0.78850373029708865, 'val_loss:', 0.1964714503288269)
('epoch', 328, 'train_loss:', 0.78210231304168698, 'val_loss:', 0.19656935214996338)
('epoch', 329, 'train_loss:', 0.7831326675415039, 'val_loss:', 0.19494013786315917)
('epoch', 330, 'train_loss:', 0.78232960939407348, 'val_loss:', 0.19565066337585449)
('epoch', 331, 'train_loss:', 0.77712638616561891, 'val_loss:', 0.19565514564514161)
('epoch', 332, 'train_loss:', 0.77866982698440557, 'val_loss:', 0.19548394918441772)
('epoch', 333, 'train_loss:', 0.77922355651855468, 'val_loss:', 0.19738199472427367)
('epoch', 334, 'train_loss:', 0.77394516706466676, 'val_loss:', 0.19256280422210692)
('epoch', 335, 'train_loss:', 0.77715914964675903, 'val_loss:', 0.19427379131317138)
('epoch', 336, 'train_loss:', 0.77584703445434566, 'val_loss:', 0.19539092540740965)
('epoch', 337, 'train_loss:', 0.78062314033508295, 'val_loss:', 0.19232788085937499)
('epoch', 338, 'train_loss:', 0.76880243778228763, 'val_loss:', 0.19457852840423584)
('epoch', 339, 'train_loss:', 0.76950585603713995, 'val_loss:', 0.19351515531539917)
('epoch', 340, 'train_loss:', 0.7756366944313049, 'val_loss:', 0.19205806493759156)
('epoch', 341, 'train_loss:', 0.76925062179565429, 'val_loss:', 0.1920364761352539)
('epoch', 342, 'train_loss:', 0.77276085615158085, 'val_loss:', 0.19607073783874512)
('epoch', 343, 'train_loss:', 0.76707879543304447, 'val_loss:', 0.19281713247299195)
('epoch', 344, 'train_loss:', 0.76644930839538572, 'val_loss:', 0.19330093145370483)
('epoch', 345, 'train_loss:', 0.76429574370384212, 'val_loss:', 0.18908980131149292)
('epoch', 346, 'train_loss:', 0.76344632863998418, 'val_loss:', 0.19287455797195435)
('epoch', 347, 'train_loss:', 0.76921544075012205, 'val_loss:', 0.19209403276443482)
('epoch', 348, 'train_loss:', 0.76679715633392331, 'val_loss:', 0.19154840230941772)
('epoch', 349, 'train_loss:', 0.76489699125289912, 'val_loss:', 0.19336519479751588)
('epoch', 350, 'train_loss:', 0.76645975112915044, 'val_loss:', 0.19050225019454955)
('epoch', 351, 'train_loss:', 0.76482764959335325, 'val_loss:', 0.19412878274917603)
('epoch', 352, 'train_loss:', 0.75974529504776001, 'val_loss:', 0.19117437362670897)
('epoch', 353, 'train_loss:', 0.76027256488800043, 'val_loss:', 0.19202873945236207)
('epoch', 354, 'train_loss:', 0.76176327466964722, 'val_loss:', 0.19146982908248902)
('epoch', 355, 'train_loss:', 0.75781146287918089, 'val_loss:', 0.19059051513671876)
('epoch', 356, 'train_loss:', 0.76368931055068967, 'val_loss:', 0.19127251625061034)
('epoch', 357, 'train_loss:', 0.75971098423004146, 'val_loss:', 0.19148499011993408)
('epoch', 358, 'train_loss:', 0.7559043979644775, 'val_loss:', 0.19110654830932616)
('epoch', 359, 'train_loss:', 0.75938558340072637, 'val_loss:', 0.19041329383850097)
('epoch', 360, 'train_loss:', 0.7600616550445557, 'val_loss:', 0.18918710231781005)
('epoch', 361, 'train_loss:', 0.75622470855712887, 'val_loss:', 0.18953634262084962)
('epoch', 362, 'train_loss:', 0.75188949108123782, 'val_loss:', 0.18951457738876343)
('epoch', 363, 'train_loss:', 0.75653932094573972, 'val_loss:', 0.1889829444885254)
('epoch', 364, 'train_loss:', 0.75645469665527343, 'val_loss:', 0.18784251928329468)
('epoch', 365, 'train_loss:', 0.74921633481979366, 'val_loss:', 0.1869165825843811)
('epoch', 366, 'train_loss:', 0.7538598585128784, 'val_loss:', 0.18745821952819824)
('epoch', 367, 'train_loss:', 0.75125111460685734, 'val_loss:', 0.1902177119255066)
('epoch', 368, 'train_loss:', 0.74730384469032285, 'val_loss:', 0.18948883771896363)
('epoch', 369, 'train_loss:', 0.7462078332901001, 'val_loss:', 0.18737937211990358)
('epoch', 370, 'train_loss:', 0.74846477508544917, 'val_loss:', 0.18831670045852661)
('epoch', 371, 'train_loss:', 0.746322238445282, 'val_loss:', 0.1861497926712036)
('epoch', 372, 'train_loss:', 0.74938446998596187, 'val_loss:', 0.18950539350509643)
('epoch', 373, 'train_loss:', 0.74994987010955816, 'val_loss:', 0.18788013935089112)
('epoch', 374, 'train_loss:', 0.7494324207305908, 'val_loss:', 0.18642458200454712)
('epoch', 375, 'train_loss:', 0.74746201992034911, 'val_loss:', 0.18692031860351563)
('epoch', 376, 'train_loss:', 0.7426021814346313, 'val_loss:', 0.18449660062789916)
('epoch', 377, 'train_loss:', 0.74224388957023624, 'val_loss:', 0.18814839124679567)
('epoch', 378, 'train_loss:', 0.74564566969871526, 'val_loss:', 0.1873776412010193)
('epoch', 379, 'train_loss:', 0.74229877233505248, 'val_loss:', 0.18484709024429322)
('epoch', 380, 'train_loss:', 0.73850212216377253, 'val_loss:', 0.18485772132873535)
('epoch', 381, 'train_loss:', 0.74761887431144713, 'val_loss:', 0.18667947053909301)
('epoch', 382, 'train_loss:', 0.74057381391525268, 'val_loss:', 0.18684252500534057)
('epoch', 383, 'train_loss:', 0.73955273270606992, 'val_loss:', 0.1851806116104126)
('epoch', 384, 'train_loss:', 0.74132266640663147, 'val_loss:', 0.18418775558471678)
('epoch', 385, 'train_loss:', 0.73917501211166381, 'val_loss:', 0.18622334957122802)
('epoch', 386, 'train_loss:', 0.73340177893638614, 'val_loss:', 0.18423601865768433)
('epoch', 387, 'train_loss:', 0.74445845127105714, 'val_loss:', 0.18667001724243165)
('epoch', 388, 'train_loss:', 0.73574746370315547, 'val_loss:', 0.18425871253013612)
('epoch', 389, 'train_loss:', 0.73515637993812566, 'val_loss:', 0.1833316969871521)
('epoch', 390, 'train_loss:', 0.73727053523063657, 'val_loss:', 0.18601902008056639)
('epoch', 391, 'train_loss:', 0.73537752509117127, 'val_loss:', 0.18722413063049317)
('epoch', 392, 'train_loss:', 0.73167369008064265, 'val_loss:', 0.18625617504119873)
('epoch', 393, 'train_loss:', 0.73054651975631713, 'val_loss:', 0.18577051877975465)
('epoch', 394, 'train_loss:', 0.73364224553108215, 'val_loss:', 0.18708520650863647)
('epoch', 395, 'train_loss:', 0.73330955266952513, 'val_loss:', 0.18237976789474486)
('epoch', 396, 'train_loss:', 0.7290040528774262, 'val_loss:', 0.18450517177581788)
('epoch', 397, 'train_loss:', 0.73071744561195373, 'val_loss:', 0.18290080428123473)
('epoch', 398, 'train_loss:', 0.72725008130073543, 'val_loss:', 0.18227994799613953)
('epoch', 399, 'train_loss:', 0.7322281169891357, 'val_loss:', 0.18355944633483887)
('epoch', 400, 'train_loss:', 0.72955551862716672, 'val_loss:', 0.18121089816093444)
('epoch', 401, 'train_loss:', 0.72328949451446534, 'val_loss:', 0.18284043312072754)
('epoch', 402, 'train_loss:', 0.72995603203773496, 'val_loss:', 0.18144694089889526)
('epoch', 403, 'train_loss:', 0.72599288105964666, 'val_loss:', 0.18378313779830932)
('epoch', 404, 'train_loss:', 0.72111722826957703, 'val_loss:', 0.18161119699478148)
('epoch', 405, 'train_loss:', 0.72793632864952085, 'val_loss:', 0.18334389209747315)
('epoch', 406, 'train_loss:', 0.71665780663490297, 'val_loss:', 0.18230631589889526)
('epoch', 407, 'train_loss:', 0.72765169501304627, 'val_loss:', 0.18186952948570251)
('epoch', 408, 'train_loss:', 0.71826576948165899, 'val_loss:', 0.18129362821578979)
('epoch', 409, 'train_loss:', 0.72098428964614869, 'val_loss:', 0.18087926030158996)
('epoch', 410, 'train_loss:', 0.71722600102424616, 'val_loss:', 0.17939877986907959)
('epoch', 411, 'train_loss:', 0.72248760819435121, 'val_loss:', 0.18357604265213012)
('epoch', 412, 'train_loss:', 0.71634198427200313, 'val_loss:', 0.17956389784812926)
('epoch', 413, 'train_loss:', 0.72073628306388859, 'val_loss:', 0.18007861256599425)
('epoch', 414, 'train_loss:', 0.71788354873657223, 'val_loss:', 0.18156559348106385)
('epoch', 415, 'train_loss:', 0.71672580003738406, 'val_loss:', 0.17981473088264466)
('epoch', 416, 'train_loss:', 0.71724667072296144, 'val_loss:', 0.18113745570182802)
('epoch', 417, 'train_loss:', 0.71410057425498963, 'val_loss:', 0.18233243346214295)
('epoch', 418, 'train_loss:', 0.70980460047721861, 'val_loss:', 0.17880227088928222)
('epoch', 419, 'train_loss:', 0.72181071996688839, 'val_loss:', 0.17934830069541932)
('epoch', 420, 'train_loss:', 0.71258344769477844, 'val_loss:', 0.18041706562042237)
('epoch', 421, 'train_loss:', 0.7155107855796814, 'val_loss:', 0.18075403928756714)
('epoch', 422, 'train_loss:', 0.71223522424697872, 'val_loss:', 0.17801860809326173)
('epoch', 423, 'train_loss:', 0.71186719059944148, 'val_loss:', 0.18048932313919067)
('epoch', 424, 'train_loss:', 0.70832718133926387, 'val_loss:', 0.18201770067214965)
('epoch', 425, 'train_loss:', 0.71516404032707215, 'val_loss:', 0.18180745363235473)
('epoch', 426, 'train_loss:', 0.71126801013946528, 'val_loss:', 0.18127249240875243)
('epoch', 427, 'train_loss:', 0.70754201412200923, 'val_loss:', 0.17812382221221923)
('epoch', 428, 'train_loss:', 0.70741276383399965, 'val_loss:', 0.17816027879714966)
('epoch', 429, 'train_loss:', 0.71642712831497191, 'val_loss:', 0.1787865138053894)
('epoch', 430, 'train_loss:', 0.70495876193046569, 'val_loss:', 0.18020527124404906)
('epoch', 431, 'train_loss:', 0.70755600333213808, 'val_loss:', 0.18081363677978515)
('epoch', 432, 'train_loss:', 0.71022541403770445, 'val_loss:', 0.17951115727424621)
('epoch', 433, 'train_loss:', 0.70803270936012264, 'val_loss:', 0.17804256796836854)
('epoch', 434, 'train_loss:', 0.70852837204933161, 'val_loss:', 0.17872567892074584)
('epoch', 435, 'train_loss:', 0.70264482259750372, 'val_loss:', 0.17872113943099976)
('epoch', 436, 'train_loss:', 0.70317374348640438, 'val_loss:', 0.17961403489112854)
('epoch', 437, 'train_loss:', 0.70979271888732909, 'val_loss:', 0.17804397463798524)
('epoch', 438, 'train_loss:', 0.7008007228374481, 'val_loss:', 0.17534545183181763)
('epoch', 439, 'train_loss:', 0.7009899497032166, 'val_loss:', 0.17674010515213012)
('epoch', 440, 'train_loss:', 0.70226707100868224, 'val_loss:', 0.17654170632362365)
('epoch', 441, 'train_loss:', 0.70230249166488645, 'val_loss:', 0.17659788608551025)
('epoch', 442, 'train_loss:', 0.70233424186706539, 'val_loss:', 0.17475749373435975)
('epoch', 443, 'train_loss:', 0.70372776985168461, 'val_loss:', 0.17587607979774475)
('epoch', 444, 'train_loss:', 0.70195684909820555, 'val_loss:', 0.17743411540985107)
('epoch', 445, 'train_loss:', 0.69885574579238896, 'val_loss:', 0.17519665002822876)
('epoch', 446, 'train_loss:', 0.70012395262718197, 'val_loss:', 0.17760642170906066)
('epoch', 447, 'train_loss:', 0.69748531222343446, 'val_loss:', 0.17678576707839966)
('epoch', 448, 'train_loss:', 0.69648046731948854, 'val_loss:', 0.17802677035331727)
('epoch', 449, 'train_loss:', 0.69320092916488651, 'val_loss:', 0.17511821269989014)
('epoch', 450, 'train_loss:', 0.69571748733520511, 'val_loss:', 0.17416294097900389)
('epoch', 451, 'train_loss:', 0.69649457931518555, 'val_loss:', 0.17556390523910523)
('epoch', 452, 'train_loss:', 0.69709216594696044, 'val_loss:', 0.17591024398803712)
('epoch', 453, 'train_loss:', 0.69248727440834046, 'val_loss:', 0.17492879509925843)
('epoch', 454, 'train_loss:', 0.6954646611213684, 'val_loss:', 0.17655040025711061)
('epoch', 455, 'train_loss:', 0.69338718056678772, 'val_loss:', 0.17510344982147216)
('epoch', 456, 'train_loss:', 0.69897397994995114, 'val_loss:', 0.17594131469726562)
('epoch', 457, 'train_loss:', 0.69577716231346132, 'val_loss:', 0.17509091615676881)
('epoch', 458, 'train_loss:', 0.68817170143127437, 'val_loss:', 0.17378849387168885)
('epoch', 459, 'train_loss:', 0.69141804099082949, 'val_loss:', 0.17249334692955018)
('epoch', 460, 'train_loss:', 0.69093356370925907, 'val_loss:', 0.17540146231651307)
('epoch', 461, 'train_loss:', 0.68880783319473271, 'val_loss:', 0.17706338047981263)
('epoch', 462, 'train_loss:', 0.68936625599861145, 'val_loss:', 0.1742836606502533)
('epoch', 463, 'train_loss:', 0.69193938493728635, 'val_loss:', 0.17207607388496399)
('epoch', 464, 'train_loss:', 0.6826304173469544, 'val_loss:', 0.17467933177947997)
('epoch', 465, 'train_loss:', 0.68384051442146299, 'val_loss:', 0.17335218787193299)
('epoch', 466, 'train_loss:', 0.68904307007789611, 'val_loss:', 0.1749085283279419)
('epoch', 467, 'train_loss:', 0.6849503636360168, 'val_loss:', 0.17316918253898619)
('epoch', 468, 'train_loss:', 0.68281549930572505, 'val_loss:', 0.17273390173912048)
('epoch', 469, 'train_loss:', 0.68751289725303655, 'val_loss:', 0.17492134928703307)
('epoch', 470, 'train_loss:', 0.68158421874046327, 'val_loss:', 0.17534765720367432)
('epoch', 471, 'train_loss:', 0.68406617999076846, 'val_loss:', 0.1732140612602234)
('epoch', 472, 'train_loss:', 0.67917618751525877, 'val_loss:', 0.17105992197990416)
('epoch', 473, 'train_loss:', 0.68180612683296205, 'val_loss:', 0.17222667694091798)
('epoch', 474, 'train_loss:', 0.68665626406669622, 'val_loss:', 0.17334509015083313)
('epoch', 475, 'train_loss:', 0.68172179579734804, 'val_loss:', 0.17371466159820556)
('epoch', 476, 'train_loss:', 0.67691088557243351, 'val_loss:', 0.17392483353614807)
('epoch', 477, 'train_loss:', 0.68113000988960271, 'val_loss:', 0.16938830137252808)
('epoch', 478, 'train_loss:', 0.67749982476234438, 'val_loss:', 0.17070452332496644)
('epoch', 479, 'train_loss:', 0.68140589594841006, 'val_loss:', 0.17166077494621276)
('epoch', 480, 'train_loss:', 0.67819940209388729, 'val_loss:', 0.17185522317886354)
('epoch', 481, 'train_loss:', 0.67459590196609498, 'val_loss:', 0.17022123217582702)
('epoch', 482, 'train_loss:', 0.67451884031295772, 'val_loss:', 0.1706168508529663)
('epoch', 483, 'train_loss:', 0.67617059111595157, 'val_loss:', 0.17004504203796386)
('epoch', 484, 'train_loss:', 0.67797815084457402, 'val_loss:', 0.1703592348098755)
('epoch', 485, 'train_loss:', 0.67461796760559078, 'val_loss:', 0.17108103394508362)
('epoch', 486, 'train_loss:', 0.67864411830902105, 'val_loss:', 0.16855278372764587)
('epoch', 487, 'train_loss:', 0.68110241651535031, 'val_loss:', 0.16965883612632751)
('epoch', 488, 'train_loss:', 0.67608844518661504, 'val_loss:', 0.17074108362197876)
('epoch', 489, 'train_loss:', 0.6701705133914948, 'val_loss:', 0.16964844584465028)
('epoch', 490, 'train_loss:', 0.67754569172859191, 'val_loss:', 0.17125889897346497)
('epoch', 491, 'train_loss:', 0.67689450502395632, 'val_loss:', 0.16937483906745909)
('epoch', 492, 'train_loss:', 0.67106955289840697, 'val_loss:', 0.17136066436767577)
('epoch', 493, 'train_loss:', 0.67026934862136844, 'val_loss:', 0.16944305419921876)
('epoch', 494, 'train_loss:', 0.67216326475143429, 'val_loss:', 0.16925475001335144)
('epoch', 495, 'train_loss:', 0.66716566801071164, 'val_loss:', 0.16914939045906066)
('epoch', 496, 'train_loss:', 0.66635669112205509, 'val_loss:', 0.17025961279869078)
('epoch', 497, 'train_loss:', 0.67019774079322814, 'val_loss:', 0.16846349120140075)
('epoch', 498, 'train_loss:', 0.6675529134273529, 'val_loss:', 0.1701173198223114)
('epoch', 499, 'train_loss:', 0.66858255147933965, 'val_loss:', 0.16650532841682433)
('epoch', 500, 'train_loss:', 0.67038284301757811, 'val_loss:', 0.16829396009445191)
('epoch', 501, 'train_loss:', 0.66790837764739985, 'val_loss:', 0.17213446974754334)
('epoch', 502, 'train_loss:', 0.66579364776611327, 'val_loss:', 0.17078192234039308)
('epoch', 503, 'train_loss:', 0.66111673831939699, 'val_loss:', 0.1670365047454834)
('epoch', 504, 'train_loss:', 0.66274578809738161, 'val_loss:', 0.17023549556732179)
('epoch', 505, 'train_loss:', 0.66540943026542665, 'val_loss:', 0.16807831764221193)
('epoch', 506, 'train_loss:', 0.66368760347366329, 'val_loss:', 0.16879236221313476)
('epoch', 507, 'train_loss:', 0.66902036905288698, 'val_loss:', 0.16786182284355164)
('epoch', 508, 'train_loss:', 0.66114603281021123, 'val_loss:', 0.16822387099266053)
('epoch', 509, 'train_loss:', 0.66383614063262941, 'val_loss:', 0.16775813937187195)
('epoch', 510, 'train_loss:', 0.66486301660537717, 'val_loss:', 0.16855517983436585)
('epoch', 511, 'train_loss:', 0.6623688745498657, 'val_loss:', 0.16435740709304811)
('epoch', 512, 'train_loss:', 0.65984148621559147, 'val_loss:', 0.16838515758514405)
('epoch', 513, 'train_loss:', 0.65818512320518496, 'val_loss:', 0.1680875015258789)
('epoch', 514, 'train_loss:', 0.65785812616348271, 'val_loss:', 0.16766417503356934)
('epoch', 515, 'train_loss:', 0.66041987776756284, 'val_loss:', 0.16641633629798888)
('epoch', 516, 'train_loss:', 0.66372606396675105, 'val_loss:', 0.16668493390083314)
('epoch', 517, 'train_loss:', 0.65922251939773557, 'val_loss:', 0.16767911791801451)
('epoch', 518, 'train_loss:', 0.65245559811592102, 'val_loss:', 0.16581617474555968)
('epoch', 519, 'train_loss:', 0.65833785533905032, 'val_loss:', 0.16554269075393677)
('epoch', 520, 'train_loss:', 0.65704439163208006, 'val_loss:', 0.16545622467994689)
('epoch', 521, 'train_loss:', 0.65087941050529485, 'val_loss:', 0.16507167339324952)
('epoch', 522, 'train_loss:', 0.65498548984527583, 'val_loss:', 0.16908897757530211)
('epoch', 523, 'train_loss:', 0.65391852378845217, 'val_loss:', 0.16554029583930968)
('epoch', 524, 'train_loss:', 0.65317515015602112, 'val_loss:', 0.1665616226196289)
('epoch', 525, 'train_loss:', 0.65400853991508479, 'val_loss:', 0.16557508945465088)
('epoch', 526, 'train_loss:', 0.65476362943649291, 'val_loss:', 0.16631281733512879)
('epoch', 527, 'train_loss:', 0.65141289114952083, 'val_loss:', 0.16410651445388794)
('epoch', 528, 'train_loss:', 0.64994947195053099, 'val_loss:', 0.16533745050430299)
('epoch', 529, 'train_loss:', 0.65366201162338256, 'val_loss:', 0.16403841853141785)
('epoch', 530, 'train_loss:', 0.65051164031028752, 'val_loss:', 0.16654136657714844)
('epoch', 531, 'train_loss:', 0.65159546613693242, 'val_loss:', 0.16495437741279603)
('epoch', 532, 'train_loss:', 0.65183007836341855, 'val_loss:', 0.16321310758590699)
('epoch', 533, 'train_loss:', 0.64993047356605527, 'val_loss:', 0.16155477285385131)
('epoch', 534, 'train_loss:', 0.65343595504760743, 'val_loss:', 0.16341795086860655)
('epoch', 535, 'train_loss:', 0.65073884844779972, 'val_loss:', 0.16798533320426942)
('epoch', 536, 'train_loss:', 0.64981798291206361, 'val_loss:', 0.1635310971736908)
('epoch', 537, 'train_loss:', 0.64814228653907779, 'val_loss:', 0.16499179005622863)
('epoch', 538, 'train_loss:', 0.64311430931091307, 'val_loss:', 0.16703507781028748)
('epoch', 539, 'train_loss:', 0.64492172002792358, 'val_loss:', 0.16344982624053955)
('epoch', 540, 'train_loss:', 0.64303457260131835, 'val_loss:', 0.16091781735420227)
('epoch', 541, 'train_loss:', 0.64569046378135686, 'val_loss:', 0.16559302687644958)
('epoch', 542, 'train_loss:', 0.64396136045455932, 'val_loss:', 0.16423821091651916)
('epoch', 543, 'train_loss:', 0.64945214271545415, 'val_loss:', 0.16392361164093017)
('epoch', 544, 'train_loss:', 0.64241708636283879, 'val_loss:', 0.16466996788978577)
('epoch', 545, 'train_loss:', 0.64106865286827086, 'val_loss:', 0.1652950656414032)
('epoch', 546, 'train_loss:', 0.64282686233520503, 'val_loss:', 0.16359567284584045)
('epoch', 547, 'train_loss:', 0.64152621865272519, 'val_loss:', 0.16189246177673339)
('epoch', 548, 'train_loss:', 0.63418846130371098, 'val_loss:', 0.16238340377807617)
('epoch', 549, 'train_loss:', 0.64014189124107357, 'val_loss:', 0.16299155712127686)
('epoch', 550, 'train_loss:', 0.6402077412605286, 'val_loss:', 0.16243601322174073)
('epoch', 551, 'train_loss:', 0.64060905337333685, 'val_loss:', 0.16269495487213134)
('epoch', 552, 'train_loss:', 0.6393914020061493, 'val_loss:', 0.16391325235366822)
('epoch', 553, 'train_loss:', 0.64445475339889524, 'val_loss:', 0.1630192792415619)
('epoch', 554, 'train_loss:', 0.63897115349769595, 'val_loss:', 0.16014769673347473)
('epoch', 555, 'train_loss:', 0.6358760607242584, 'val_loss:', 0.16190066099166869)
('epoch', 556, 'train_loss:', 0.63538511037826539, 'val_loss:', 0.16371524930000306)
('epoch', 557, 'train_loss:', 0.64029299378395077, 'val_loss:', 0.16277058720588683)
('epoch', 558, 'train_loss:', 0.64177232980728149, 'val_loss:', 0.15914929986000062)
('epoch', 559, 'train_loss:', 0.63580912351608276, 'val_loss:', 0.16281198740005492)
('epoch', 560, 'train_loss:', 0.63991745233535768, 'val_loss:', 0.16186582803726196)
('epoch', 561, 'train_loss:', 0.63300321221351619, 'val_loss:', 0.15969748854637145)
('epoch', 562, 'train_loss:', 0.6344489896297455, 'val_loss:', 0.16310903906822205)
('epoch', 563, 'train_loss:', 0.63468840360641476, 'val_loss:', 0.16085793852806091)
('epoch', 564, 'train_loss:', 0.63373395562171941, 'val_loss:', 0.16203946352005005)
('epoch', 565, 'train_loss:', 0.63603344321250921, 'val_loss:', 0.16341501116752624)
('epoch', 566, 'train_loss:', 0.63542934417724606, 'val_loss:', 0.15942287921905518)
('epoch', 567, 'train_loss:', 0.63158520579338073, 'val_loss:', 0.16167854666709899)
('epoch', 568, 'train_loss:', 0.62874826788902283, 'val_loss:', 0.16086761355400087)
('epoch', 569, 'train_loss:', 0.63453562378883366, 'val_loss:', 0.16233697295188904)
('epoch', 570, 'train_loss:', 0.63657450556755069, 'val_loss:', 0.16006325960159301)
('epoch', 571, 'train_loss:', 0.62962248682975774, 'val_loss:', 0.16066395759582519)
('epoch', 572, 'train_loss:', 0.62716792225837703, 'val_loss:', 0.16086930274963379)
('epoch', 573, 'train_loss:', 0.63060023427009582, 'val_loss:', 0.15919814348220826)
('epoch', 574, 'train_loss:', 0.63046992897987364, 'val_loss:', 0.1630340838432312)
('epoch', 575, 'train_loss:', 0.62979454159736636, 'val_loss:', 0.15913064837455748)
('epoch', 576, 'train_loss:', 0.62341273307800293, 'val_loss:', 0.15940993309020995)
('epoch', 577, 'train_loss:', 0.62910551190376285, 'val_loss:', 0.15891004204750062)
('epoch', 578, 'train_loss:', 0.63110383152961735, 'val_loss:', 0.15822980761528016)
('epoch', 579, 'train_loss:', 0.62960191965103152, 'val_loss:', 0.15965152859687806)
('epoch', 580, 'train_loss:', 0.62984518527984623, 'val_loss:', 0.1579516625404358)
('epoch', 581, 'train_loss:', 0.62416700363159183, 'val_loss:', 0.16063315749168397)
('epoch', 582, 'train_loss:', 0.62236349105834965, 'val_loss:', 0.1587024736404419)
('epoch', 583, 'train_loss:', 0.62768079519271847, 'val_loss:', 0.16021583557128907)
('epoch', 584, 'train_loss:', 0.62221764445304872, 'val_loss:', 0.15802424073219298)
('epoch', 585, 'train_loss:', 0.61897570967674254, 'val_loss:', 0.15860185146331787)
('epoch', 586, 'train_loss:', 0.62209018468856814, 'val_loss:', 0.15706221699714662)
('epoch', 587, 'train_loss:', 0.62141827702522279, 'val_loss:', 0.15720997214317323)
('epoch', 588, 'train_loss:', 0.62899781703948976, 'val_loss:', 0.15949321985244752)
('epoch', 589, 'train_loss:', 0.62442661881446837, 'val_loss:', 0.16016168951988219)
('epoch', 590, 'train_loss:', 0.62098793268203734, 'val_loss:', 0.15899236917495727)
('epoch', 591, 'train_loss:', 0.6197709882259369, 'val_loss:', 0.15872337460517882)
('epoch', 592, 'train_loss:', 0.61892814278602604, 'val_loss:', 0.1572391164302826)
('epoch', 593, 'train_loss:', 0.62206906318664545, 'val_loss:', 0.16102852106094359)
('epoch', 594, 'train_loss:', 0.61709863662719722, 'val_loss:', 0.15722111821174622)
('epoch', 595, 'train_loss:', 0.62197295546531672, 'val_loss:', 0.15861825585365297)
('epoch', 596, 'train_loss:', 0.62095245718955994, 'val_loss:', 0.15792476177215575)
('epoch', 597, 'train_loss:', 0.61505766630172731, 'val_loss:', 0.15560506343841551)
('epoch', 598, 'train_loss:', 0.6168883740901947, 'val_loss:', 0.15730020403862)
('epoch', 599, 'train_loss:', 0.61590078353881839, 'val_loss:', 0.15825856685638429)
