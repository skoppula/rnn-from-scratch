(keras)skoppula@sls-sm-7:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=3 python lstm_all_variants.py -t -d sms -v additive_no_sigmoid_linear
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'sms', 'train': True, 'variant': 'additive_no_sigmoid_linear', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (9292, 30), (9292, 30), (2323, 30), (2323, 30))
('num steps in trn and val epochs', 36, 9)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:82:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2389 get requests, put_count=2250 evicted_count=1000 eviction_rate=0.444444 and unsatisfied allocation rate=0.518627
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2439 get requests, put_count=2475 evicted_count=1000 eviction_rate=0.40404 and unsatisfied allocation rate=0.404674
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4776 get requests, put_count=4677 evicted_count=1000 eviction_rate=0.213812 and unsatisfied allocation rate=0.242462
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
('epoch', 0, 'train_loss:', 1.4909771609306335, 'val_loss:', 0.36039212703704832)
('epoch', 1, 'train_loss:', 1.4482772660255432, 'val_loss:', 0.35964297533035278)
('epoch', 2, 'train_loss:', 1.4435278010368346, 'val_loss:', 0.3600227999687195)
('epoch', 3, 'train_loss:', 1.4417576837539672, 'val_loss:', 0.35985827207565307)
('epoch', 4, 'train_loss:', 1.4383042478561401, 'val_loss:', 0.35914504289627075)
('epoch', 5, 'train_loss:', 1.4353759908676147, 'val_loss:', 0.35777824640274047)
('epoch', 6, 'train_loss:', 1.4341983532905578, 'val_loss:', 0.35705899477005004)
('epoch', 7, 'train_loss:', 1.4323627591133117, 'val_loss:', 0.35793269872665406)
('epoch', 8, 'train_loss:', 1.4347155165672303, 'val_loss:', 0.35820206880569461)
('epoch', 9, 'train_loss:', 1.4323551559448242, 'val_loss:', 0.35706554889678954)
('epoch', 10, 'train_loss:', 1.4332243657112123, 'val_loss:', 0.35749023675918579)
('epoch', 11, 'train_loss:', 1.4318310189247132, 'val_loss:', 0.3574154567718506)
('epoch', 12, 'train_loss:', 1.4328139424324036, 'val_loss:', 0.35700748205184935)
('epoch', 13, 'train_loss:', 1.4313289666175841, 'val_loss:', 0.35697097301483155)
('epoch', 14, 'train_loss:', 1.4309633159637452, 'val_loss:', 0.35620968341827391)
('epoch', 15, 'train_loss:', 1.4246097803115845, 'val_loss:', 0.35703871488571165)
('epoch', 16, 'train_loss:', 1.4303346705436706, 'val_loss:', 0.3556935167312622)
('epoch', 17, 'train_loss:', 1.4284711384773254, 'val_loss:', 0.35580852985382078)
('epoch', 18, 'train_loss:', 1.4268607378005982, 'val_loss:', 0.35626276731491091)
('epoch', 19, 'train_loss:', 1.4200138902664186, 'val_loss:', 0.35543188810348508)
('epoch', 20, 'train_loss:', 1.4229198241233825, 'val_loss:', 0.35344838619232177)
('epoch', 21, 'train_loss:', 1.4182754254341126, 'val_loss:', 0.35457505702972414)
('epoch', 22, 'train_loss:', 1.4183373737335205, 'val_loss:', 0.35483421802520754)
('epoch', 23, 'train_loss:', 1.4161801886558534, 'val_loss:', 0.3536053514480591)
('epoch', 24, 'train_loss:', 1.4125173377990723, 'val_loss:', 0.35172163486480712)
('epoch', 25, 'train_loss:', 1.4085481333732606, 'val_loss:', 0.35164511442184448)
('epoch', 26, 'train_loss:', 1.399430923461914, 'val_loss:', 0.34943333387374875)
('epoch', 27, 'train_loss:', 1.3975333762168884, 'val_loss:', 0.34773201227188111)
('epoch', 28, 'train_loss:', 1.3854049372673034, 'val_loss:', 0.34520276784896853)
('epoch', 29, 'train_loss:', 1.3796760439872742, 'val_loss:', 0.34125094652175902)
('epoch', 30, 'train_loss:', 1.3707253241539001, 'val_loss:', 0.34044176816940308)
('epoch', 31, 'train_loss:', 1.3574345850944518, 'val_loss:', 0.33696481704711911)
('epoch', 32, 'train_loss:', 1.3480035161972046, 'val_loss:', 0.33527406215667727)
('epoch', 33, 'train_loss:', 1.3308172774314881, 'val_loss:', 0.33261128902435305)
('epoch', 34, 'train_loss:', 1.3190476632118224, 'val_loss:', 0.33004239797592161)
('epoch', 35, 'train_loss:', 1.3139971566200257, 'val_loss:', 0.32620243310928343)
('epoch', 36, 'train_loss:', 1.3046874809265137, 'val_loss:', 0.32390567541122439)
('epoch', 37, 'train_loss:', 1.2922123861312866, 'val_loss:', 0.3238749814033508)
('epoch', 38, 'train_loss:', 1.284412648677826, 'val_loss:', 0.3206726360321045)
('epoch', 39, 'train_loss:', 1.2766128706932067, 'val_loss:', 0.31873890399932864)
('epoch', 40, 'train_loss:', 1.2712094378471375, 'val_loss:', 0.31886872529983523)
('epoch', 41, 'train_loss:', 1.2651764822006226, 'val_loss:', 0.31702984094619752)
('epoch', 42, 'train_loss:', 1.2586554932594298, 'val_loss:', 0.31451510667800903)
('epoch', 43, 'train_loss:', 1.2588074755668641, 'val_loss:', 0.31245165348052978)
('epoch', 44, 'train_loss:', 1.2472724246978759, 'val_loss:', 0.31336131095886233)
('epoch', 45, 'train_loss:', 1.2438531589508057, 'val_loss:', 0.30978854179382326)
('epoch', 46, 'train_loss:', 1.2419386124610901, 'val_loss:', 0.30914012908935545)
('epoch', 47, 'train_loss:', 1.2352661991119385, 'val_loss:', 0.30779676914215087)
('epoch', 48, 'train_loss:', 1.2335487508773804, 'val_loss:', 0.30765417814254759)
('epoch', 49, 'train_loss:', 1.2295946240425111, 'val_loss:', 0.30569975852966308)
('epoch', 50, 'train_loss:', 1.2298453235626221, 'val_loss:', 0.30600839376449585)
('epoch', 51, 'train_loss:', 1.2247410011291504, 'val_loss:', 0.30603108644485472)
('epoch', 52, 'train_loss:', 1.217534692287445, 'val_loss:', 0.30439191579818725)
('epoch', 53, 'train_loss:', 1.2144129347801209, 'val_loss:', 0.30371603727340696)
('epoch', 54, 'train_loss:', 1.2130529189109802, 'val_loss:', 0.30204169750213622)
('epoch', 55, 'train_loss:', 1.207996699810028, 'val_loss:', 0.300463342666626)
('epoch', 56, 'train_loss:', 1.2064527487754821, 'val_loss:', 0.29990832805633544)
('epoch', 57, 'train_loss:', 1.2019778680801392, 'val_loss:', 0.29887949466705321)
('epoch', 58, 'train_loss:', 1.1997599792480469, 'val_loss:', 0.29957993507385255)
('epoch', 59, 'train_loss:', 1.1934208512306212, 'val_loss:', 0.29870097637176513)
('epoch', 60, 'train_loss:', 1.1914886760711669, 'val_loss:', 0.29840554237365724)
('epoch', 61, 'train_loss:', 1.1894923996925355, 'val_loss:', 0.29665875196456909)
('epoch', 62, 'train_loss:', 1.1870457935333252, 'val_loss:', 0.29567982673645021)
('epoch', 63, 'train_loss:', 1.1852398824691772, 'val_loss:', 0.29582382440567018)
('epoch', 64, 'train_loss:', 1.1801477789878845, 'val_loss:', 0.29625107288360597)
('epoch', 65, 'train_loss:', 1.1775533580780029, 'val_loss:', 0.29477089166641235)
('epoch', 66, 'train_loss:', 1.1754910445213318, 'val_loss:', 0.29148313522338865)
('epoch', 67, 'train_loss:', 1.1700809121131897, 'val_loss:', 0.29052312135696412)
('epoch', 68, 'train_loss:', 1.1686208486557006, 'val_loss:', 0.29263248205184939)
('epoch', 69, 'train_loss:', 1.1687556910514831, 'val_loss:', 0.29063037633895872)
('epoch', 70, 'train_loss:', 1.1665838718414308, 'val_loss:', 0.29040289878845216)
('epoch', 71, 'train_loss:', 1.1617334079742432, 'val_loss:', 0.28786422729492189)
('epoch', 72, 'train_loss:', 1.1591878890991212, 'val_loss:', 0.28867772817611692)
('epoch', 73, 'train_loss:', 1.1562063527107238, 'val_loss:', 0.28761775732040407)
('epoch', 74, 'train_loss:', 1.1559203267097473, 'val_loss:', 0.28970480918884278)
('epoch', 75, 'train_loss:', 1.1494991970062256, 'val_loss:', 0.28668628215789793)
('epoch', 76, 'train_loss:', 1.1485311222076415, 'val_loss:', 0.28648793697357178)
('epoch', 77, 'train_loss:', 1.1504428100585937, 'val_loss:', 0.28632858514785764)
('epoch', 78, 'train_loss:', 1.1447035431861878, 'val_loss:', 0.28572218894958495)
('epoch', 79, 'train_loss:', 1.1482804012298584, 'val_loss:', 0.28674573898315431)
('epoch', 80, 'train_loss:', 1.1437049746513366, 'val_loss:', 0.2843539524078369)
('epoch', 81, 'train_loss:', 1.1357924461364746, 'val_loss:', 0.28616032123565671)
('epoch', 82, 'train_loss:', 1.1372600245475768, 'val_loss:', 0.28545213937759401)
('epoch', 83, 'train_loss:', 1.1342559719085694, 'val_loss:', 0.28469702482223513)
('epoch', 84, 'train_loss:', 1.133067500591278, 'val_loss:', 0.28227900981903076)
('epoch', 85, 'train_loss:', 1.1313769316673279, 'val_loss:', 0.28164082050323486)
('epoch', 86, 'train_loss:', 1.1291689276695251, 'val_loss:', 0.28337023258209226)
('epoch', 87, 'train_loss:', 1.1300289201736451, 'val_loss:', 0.2814540982246399)
('epoch', 88, 'train_loss:', 1.1264174032211303, 'val_loss:', 0.28146327257156373)
('epoch', 89, 'train_loss:', 1.1194867444038392, 'val_loss:', 0.27993498325347899)
('epoch', 90, 'train_loss:', 1.1234018230438232, 'val_loss:', 0.27927359819412234)
('epoch', 91, 'train_loss:', 1.1204374170303344, 'val_loss:', 0.27919661045074462)
('epoch', 92, 'train_loss:', 1.1150147032737732, 'val_loss:', 0.27904106616973878)
('epoch', 93, 'train_loss:', 1.1132878565788269, 'val_loss:', 0.27916210651397705)
('epoch', 94, 'train_loss:', 1.1149500513076782, 'val_loss:', 0.2760172438621521)
('epoch', 95, 'train_loss:', 1.1137944102287292, 'val_loss:', 0.27945617437362669)
('epoch', 96, 'train_loss:', 1.1107223343849182, 'val_loss:', 0.27470025062561038)
('epoch', 97, 'train_loss:', 1.1082225966453552, 'val_loss:', 0.27555220365524291)
('epoch', 98, 'train_loss:', 1.1045351457595824, 'val_loss:', 0.27572276115417482)
('epoch', 99, 'train_loss:', 1.1021500849723815, 'val_loss:', 0.27470586061477659)
('epoch', 100, 'train_loss:', 1.1008307027816773, 'val_loss:', 0.2739973306655884)
('epoch', 101, 'train_loss:', 1.0955016279220582, 'val_loss:', 0.27501501560211183)
('epoch', 102, 'train_loss:', 1.0954504060745238, 'val_loss:', 0.27300086259841921)
('epoch', 103, 'train_loss:', 1.0949606227874755, 'val_loss:', 0.27395940065383911)
('epoch', 104, 'train_loss:', 1.0895509243011474, 'val_loss:', 0.2723404026031494)
('epoch', 105, 'train_loss:', 1.0916623425483705, 'val_loss:', 0.27142203092575073)
('epoch', 106, 'train_loss:', 1.0858791804313659, 'val_loss:', 0.27373720169067384)
('epoch', 107, 'train_loss:', 1.085121784210205, 'val_loss:', 0.27175518989562986)
('epoch', 108, 'train_loss:', 1.0832699966430663, 'val_loss:', 0.27010675430297854)
('epoch', 109, 'train_loss:', 1.0823480224609374, 'val_loss:', 0.27045689821243285)
('epoch', 110, 'train_loss:', 1.0809249305725097, 'val_loss:', 0.26882632017135621)
('epoch', 111, 'train_loss:', 1.0745244598388672, 'val_loss:', 0.26826298713684082)
('epoch', 112, 'train_loss:', 1.0756130933761596, 'val_loss:', 0.26988062620162961)
('epoch', 113, 'train_loss:', 1.0734554243087768, 'val_loss:', 0.2660702705383301)
('epoch', 114, 'train_loss:', 1.0725757765769959, 'val_loss:', 0.26813813686370852)
('epoch', 115, 'train_loss:', 1.0701331615447998, 'val_loss:', 0.26688987731933594)
('epoch', 116, 'train_loss:', 1.0633527088165282, 'val_loss:', 0.26561209917068479)
('epoch', 117, 'train_loss:', 1.0631888818740844, 'val_loss:', 0.26586440563201902)
('epoch', 118, 'train_loss:', 1.064439024925232, 'val_loss:', 0.26494153261184694)
('epoch', 119, 'train_loss:', 1.0597762870788574, 'val_loss:', 0.26453393697738647)
('epoch', 120, 'train_loss:', 1.0572300696372985, 'val_loss:', 0.26476790428161623)
('epoch', 121, 'train_loss:', 1.0541326689720154, 'val_loss:', 0.26286268949508668)
('epoch', 122, 'train_loss:', 1.0527482652664184, 'val_loss:', 0.26289515972137451)
('epoch', 123, 'train_loss:', 1.0500180435180664, 'val_loss:', 0.26344343185424807)
('epoch', 124, 'train_loss:', 1.0519632005691528, 'val_loss:', 0.26200721740722654)
('epoch', 125, 'train_loss:', 1.0479447340965271, 'val_loss:', 0.26180463552474975)
('epoch', 126, 'train_loss:', 1.0453858709335326, 'val_loss:', 0.2599265265464783)
('epoch', 127, 'train_loss:', 1.0400409960746766, 'val_loss:', 0.26061309814453126)
('epoch', 128, 'train_loss:', 1.0469278049468995, 'val_loss:', 0.26025757551193235)
('epoch', 129, 'train_loss:', 1.0417176675796509, 'val_loss:', 0.26116432905197146)
('epoch', 130, 'train_loss:', 1.0398167943954468, 'val_loss:', 0.25829808950424193)
('epoch', 131, 'train_loss:', 1.0366343593597411, 'val_loss:', 0.25733930587768555)
('epoch', 132, 'train_loss:', 1.0364270615577698, 'val_loss:', 0.25888789176940918)
('epoch', 133, 'train_loss:', 1.0347474837303161, 'val_loss:', 0.25713064908981326)
('epoch', 134, 'train_loss:', 1.035154812335968, 'val_loss:', 0.25809286355972288)
('epoch', 135, 'train_loss:', 1.0314255928993226, 'val_loss:', 0.25682174682617187)
('epoch', 136, 'train_loss:', 1.0261049175262451, 'val_loss:', 0.25839337825775144)
('epoch', 137, 'train_loss:', 1.027602653503418, 'val_loss:', 0.25691174983978271)
('epoch', 138, 'train_loss:', 1.025274362564087, 'val_loss:', 0.25536767244338987)
('epoch', 139, 'train_loss:', 1.0217366433143615, 'val_loss:', 0.25640500307083131)
('epoch', 140, 'train_loss:', 1.0265963339805604, 'val_loss:', 0.255242075920105)
('epoch', 141, 'train_loss:', 1.0201618480682373, 'val_loss:', 0.25472130060195924)
('epoch', 142, 'train_loss:', 1.0177321195602418, 'val_loss:', 0.25524603605270385)
('epoch', 143, 'train_loss:', 1.0177469182014465, 'val_loss:', 0.25443089008331299)
('epoch', 144, 'train_loss:', 1.0192310214042664, 'val_loss:', 0.25406354188919067)
('epoch', 145, 'train_loss:', 1.0140521073341369, 'val_loss:', 0.25444287061691284)
('epoch', 146, 'train_loss:', 1.0108708715438843, 'val_loss:', 0.25301645755767821)
('epoch', 147, 'train_loss:', 1.0142467617988586, 'val_loss:', 0.2537178349494934)
('epoch', 148, 'train_loss:', 1.0108261704444885, 'val_loss:', 0.25375513315200804)
('epoch', 149, 'train_loss:', 1.0092195677757263, 'val_loss:', 0.25309898853302004)
('epoch', 150, 'train_loss:', 1.0075664114952088, 'val_loss:', 0.25190386772155759)
('epoch', 151, 'train_loss:', 1.0072807264328003, 'val_loss:', 0.25420893907546999)
('epoch', 152, 'train_loss:', 1.0033647298812867, 'val_loss:', 0.25051303148269655)
('epoch', 153, 'train_loss:', 1.0048528146743774, 'val_loss:', 0.25052232742309571)
('epoch', 154, 'train_loss:', 1.0059229111671448, 'val_loss:', 0.25148584365844728)
('epoch', 155, 'train_loss:', 0.99952264070510866, 'val_loss:', 0.25103148698806765)
('epoch', 156, 'train_loss:', 0.99800963640213014, 'val_loss:', 0.25061654329299926)
('epoch', 157, 'train_loss:', 0.99609202146530151, 'val_loss:', 0.24939718723297119)
('epoch', 158, 'train_loss:', 0.99926472663879395, 'val_loss:', 0.24956348896026612)
('epoch', 159, 'train_loss:', 0.99720263242721563, 'val_loss:', 0.24899907827377318)
('epoch', 160, 'train_loss:', 0.99495627880096438, 'val_loss:', 0.24769418239593505)
('epoch', 161, 'train_loss:', 0.99270824909210209, 'val_loss:', 0.25043209552764895)
('epoch', 162, 'train_loss:', 0.99226695060729986, 'val_loss:', 0.24940775871276855)
('epoch', 163, 'train_loss:', 0.99271028518676763, 'val_loss:', 0.24688888788223268)
('epoch', 164, 'train_loss:', 0.99186348915100098, 'val_loss:', 0.24843359470367432)
('epoch', 165, 'train_loss:', 0.99345906496047975, 'val_loss:', 0.24846359491348266)
('epoch', 166, 'train_loss:', 0.98683264732360843, 'val_loss:', 0.24557271480560303)
('epoch', 167, 'train_loss:', 0.99143610000610349, 'val_loss:', 0.24812109708786012)
('epoch', 168, 'train_loss:', 0.98604350566864019, 'val_loss:', 0.24580039739608764)
('epoch', 169, 'train_loss:', 0.98640525817871094, 'val_loss:', 0.2462645673751831)
('epoch', 170, 'train_loss:', 0.98356359958648687, 'val_loss:', 0.24644548892974855)
('epoch', 171, 'train_loss:', 0.9832450675964356, 'val_loss:', 0.24546434640884399)
('epoch', 172, 'train_loss:', 0.98134283065795902, 'val_loss:', 0.24403814315795899)
('epoch', 173, 'train_loss:', 0.97666126966476441, 'val_loss:', 0.24778064012527465)
('epoch', 174, 'train_loss:', 0.9777615427970886, 'val_loss:', 0.24445603370666505)
('epoch', 175, 'train_loss:', 0.97749615430831904, 'val_loss:', 0.24346206188201905)
('epoch', 176, 'train_loss:', 0.97577595949172968, 'val_loss:', 0.2449449896812439)
('epoch', 177, 'train_loss:', 0.97880491971969608, 'val_loss:', 0.24651834964752198)
('epoch', 178, 'train_loss:', 0.97780082225799558, 'val_loss:', 0.24533770084381104)
('epoch', 179, 'train_loss:', 0.97273392677307124, 'val_loss:', 0.24357238292694092)
('epoch', 180, 'train_loss:', 0.972185549736023, 'val_loss:', 0.24218796730041503)
('epoch', 181, 'train_loss:', 0.97177359342575076, 'val_loss:', 0.24214116096496582)
('epoch', 182, 'train_loss:', 0.96801052331924442, 'val_loss:', 0.24323844909667969)
('epoch', 183, 'train_loss:', 0.97081627368927004, 'val_loss:', 0.24346279144287108)
('epoch', 184, 'train_loss:', 0.9682050728797913, 'val_loss:', 0.24190608978271486)
('epoch', 185, 'train_loss:', 0.96884250164031982, 'val_loss:', 0.24146602630615235)
('epoch', 186, 'train_loss:', 0.96801429271698003, 'val_loss:', 0.24164848089218138)
('epoch', 187, 'train_loss:', 0.9659040284156799, 'val_loss:', 0.23932741403579713)
('epoch', 188, 'train_loss:', 0.97023233413696286, 'val_loss:', 0.24032279253005981)
('epoch', 189, 'train_loss:', 0.96265093326568607, 'val_loss:', 0.24074053525924682)
('epoch', 190, 'train_loss:', 0.96456506729125979, 'val_loss:', 0.23938714742660522)
('epoch', 191, 'train_loss:', 0.9621682572364807, 'val_loss:', 0.23875967025756836)
('epoch', 192, 'train_loss:', 0.96331174373626705, 'val_loss:', 0.23885980367660523)
('epoch', 193, 'train_loss:', 0.96003855943679806, 'val_loss:', 0.23907894372940064)
('epoch', 194, 'train_loss:', 0.95890750169754024, 'val_loss:', 0.24006547212600707)
('epoch', 195, 'train_loss:', 0.95909870862960811, 'val_loss:', 0.23907720327377319)
('epoch', 196, 'train_loss:', 0.95227990865707401, 'val_loss:', 0.23782129287719728)
('epoch', 197, 'train_loss:', 0.95720528364181523, 'val_loss:', 0.23859756231307983)
('epoch', 198, 'train_loss:', 0.95394114971160893, 'val_loss:', 0.23889071464538575)
('epoch', 199, 'train_loss:', 0.95129422187805179, 'val_loss:', 0.2389763355255127)
('epoch', 200, 'train_loss:', 0.95509339570999141, 'val_loss:', 0.23741884946823119)
('epoch', 201, 'train_loss:', 0.95125942468643188, 'val_loss:', 0.23823611974716186)
('epoch', 202, 'train_loss:', 0.95271655082702633, 'val_loss:', 0.23828984975814818)
('epoch', 203, 'train_loss:', 0.95148285627365115, 'val_loss:', 0.23828773260116576)
('epoch', 204, 'train_loss:', 0.94814090490341185, 'val_loss:', 0.23793416261672973)
('epoch', 205, 'train_loss:', 0.94743057489395144, 'val_loss:', 0.23686642169952393)
('epoch', 206, 'train_loss:', 0.94429648637771602, 'val_loss:', 0.23607915163040161)
('epoch', 207, 'train_loss:', 0.94702480792999266, 'val_loss:', 0.23526508331298829)
('epoch', 208, 'train_loss:', 0.94681232690811157, 'val_loss:', 0.23498407602310181)
('epoch', 209, 'train_loss:', 0.94180192947387698, 'val_loss:', 0.23619089126586915)
('epoch', 210, 'train_loss:', 0.94486473560333251, 'val_loss:', 0.2357510209083557)
('epoch', 211, 'train_loss:', 0.9446563720703125, 'val_loss:', 0.23513711452484132)
('epoch', 212, 'train_loss:', 0.94313784122467037, 'val_loss:', 0.23658294677734376)
('epoch', 213, 'train_loss:', 0.94299733400344854, 'val_loss:', 0.23643485546112061)
('epoch', 214, 'train_loss:', 0.94155603647232056, 'val_loss:', 0.23476307868957519)
('epoch', 215, 'train_loss:', 0.93802278041839604, 'val_loss:', 0.23491472482681275)
('epoch', 216, 'train_loss:', 0.93737787008285522, 'val_loss:', 0.23675289154052734)
('epoch', 217, 'train_loss:', 0.9383362126350403, 'val_loss:', 0.23543463706970214)
('epoch', 218, 'train_loss:', 0.93771420240402226, 'val_loss:', 0.23446301937103273)
('epoch', 219, 'train_loss:', 0.94162232637405396, 'val_loss:', 0.23469752073287964)
('epoch', 220, 'train_loss:', 0.9353922033309936, 'val_loss:', 0.23431198596954345)
('epoch', 221, 'train_loss:', 0.93642693758010864, 'val_loss:', 0.2332472825050354)
('epoch', 222, 'train_loss:', 0.93223929405212402, 'val_loss:', 0.23363100051879881)
('epoch', 223, 'train_loss:', 0.93444002389907832, 'val_loss:', 0.23499357223510742)
('epoch', 224, 'train_loss:', 0.93342784643173216, 'val_loss:', 0.23379606485366822)
('epoch', 225, 'train_loss:', 0.93093174457550054, 'val_loss:', 0.23351246118545532)
('epoch', 226, 'train_loss:', 0.93107470512390134, 'val_loss:', 0.23200654983520508)
('epoch', 227, 'train_loss:', 0.9282674407958984, 'val_loss:', 0.2323569893836975)
('epoch', 228, 'train_loss:', 0.92352801322937017, 'val_loss:', 0.23273692846298219)
('epoch', 229, 'train_loss:', 0.92804561376571659, 'val_loss:', 0.23140577793121339)
('epoch', 230, 'train_loss:', 0.92469823360443115, 'val_loss:', 0.23368443250656129)
('epoch', 231, 'train_loss:', 0.92648020744323734, 'val_loss:', 0.23173418998718262)
('epoch', 232, 'train_loss:', 0.92069099426269529, 'val_loss:', 0.23235507011413575)
('epoch', 233, 'train_loss:', 0.92594551324844365, 'val_loss:', 0.23159703254699707)
('epoch', 234, 'train_loss:', 0.92382997512817377, 'val_loss:', 0.2320444941520691)
('epoch', 235, 'train_loss:', 0.92439097166061401, 'val_loss:', 0.23229687929153442)
('epoch', 236, 'train_loss:', 0.92632576465606686, 'val_loss:', 0.23191133499145508)
('epoch', 237, 'train_loss:', 0.92216144323349003, 'val_loss:', 0.23094159841537476)
('epoch', 238, 'train_loss:', 0.92201993465423582, 'val_loss:', 0.23273380279541014)
('epoch', 239, 'train_loss:', 0.91859153509140012, 'val_loss:', 0.2310159397125244)
('epoch', 240, 'train_loss:', 0.91607198715209959, 'val_loss:', 0.2306363320350647)
('epoch', 241, 'train_loss:', 0.91902202844619751, 'val_loss:', 0.23048027276992797)
('epoch', 242, 'train_loss:', 0.91726499319076538, 'val_loss:', 0.22972536802291871)
('epoch', 243, 'train_loss:', 0.92122052431106571, 'val_loss:', 0.22952704906463622)
('epoch', 244, 'train_loss:', 0.92147831439971928, 'val_loss:', 0.22904218673706056)
('epoch', 245, 'train_loss:', 0.91516798973083491, 'val_loss:', 0.22881821870803834)
('epoch', 246, 'train_loss:', 0.91931011438369747, 'val_loss:', 0.2286636710166931)
('epoch', 247, 'train_loss:', 0.91762453556060786, 'val_loss:', 0.22935539722442627)
('epoch', 248, 'train_loss:', 0.91364916086196901, 'val_loss:', 0.23007112741470337)
('epoch', 249, 'train_loss:', 0.91162363290786741, 'val_loss:', 0.22792481422424316)
('epoch', 250, 'train_loss:', 0.91586666107177739, 'val_loss:', 0.22944690704345702)
('epoch', 251, 'train_loss:', 0.91075888395309446, 'val_loss:', 0.22651150226593017)
('epoch', 252, 'train_loss:', 0.91253694534301755, 'val_loss:', 0.2292745018005371)
('epoch', 253, 'train_loss:', 0.91127537250518797, 'val_loss:', 0.22912124156951905)
('epoch', 254, 'train_loss:', 0.91155154228210444, 'val_loss:', 0.22864033460617064)
('epoch', 255, 'train_loss:', 0.90568246841430666, 'val_loss:', 0.22831390619277955)
('epoch', 256, 'train_loss:', 0.9143028163909912, 'val_loss:', 0.22693435430526734)
('epoch', 257, 'train_loss:', 0.90709312200546266, 'val_loss:', 0.22718247652053833)
('epoch', 258, 'train_loss:', 0.9067269134521484, 'val_loss:', 0.22760230779647828)
('epoch', 259, 'train_loss:', 0.90898085355758662, 'val_loss:', 0.22864092826843263)
('epoch', 260, 'train_loss:', 0.90864067792892456, 'val_loss:', 0.22748606204986571)
('epoch', 261, 'train_loss:', 0.90697027444839473, 'val_loss:', 0.22655608654022216)
('epoch', 262, 'train_loss:', 0.90550789833068845, 'val_loss:', 0.22907423496246337)
('epoch', 263, 'train_loss:', 0.90998886108398436, 'val_loss:', 0.22684638500213622)
('epoch', 264, 'train_loss:', 0.90669657707214357, 'val_loss:', 0.22885993003845215)
('epoch', 265, 'train_loss:', 0.90508275985717779, 'val_loss:', 0.22677510976791382)
('epoch', 266, 'train_loss:', 0.90304514169692995, 'val_loss:', 0.2272032070159912)
('epoch', 267, 'train_loss:', 0.90401525497436519, 'val_loss:', 0.22801584005355835)
('epoch', 268, 'train_loss:', 0.90347373485565186, 'val_loss:', 0.22810282945632934)
('epoch', 269, 'train_loss:', 0.89934008836746215, 'val_loss:', 0.22619063377380372)
('epoch', 270, 'train_loss:', 0.9006994366645813, 'val_loss:', 0.22618764638900757)
('epoch', 271, 'train_loss:', 0.90101979732513426, 'val_loss:', 0.22426200389862061)
('epoch', 272, 'train_loss:', 0.90070368289947511, 'val_loss:', 0.22713962316513062)
('epoch', 273, 'train_loss:', 0.9005235147476196, 'val_loss:', 0.22545092105865477)
('epoch', 274, 'train_loss:', 0.89401622772216793, 'val_loss:', 0.22546417474746705)
('epoch', 275, 'train_loss:', 0.90018227100372317, 'val_loss:', 0.22474869489669799)
('epoch', 276, 'train_loss:', 0.89721929788589483, 'val_loss:', 0.22606454849243163)
('epoch', 277, 'train_loss:', 0.90117594480514529, 'val_loss:', 0.22462249279022217)
('epoch', 278, 'train_loss:', 0.89726354837417599, 'val_loss:', 0.22375483989715575)
('epoch', 279, 'train_loss:', 0.90054014921188352, 'val_loss:', 0.22510287046432495)
('epoch', 280, 'train_loss:', 0.89466667413711543, 'val_loss:', 0.22283762931823731)
('epoch', 281, 'train_loss:', 0.89629889011383057, 'val_loss:', 0.22261186122894286)
('epoch', 282, 'train_loss:', 0.89462161064147949, 'val_loss:', 0.22366817235946657)
('epoch', 283, 'train_loss:', 0.89166813850402837, 'val_loss:', 0.22373119115829468)
('epoch', 284, 'train_loss:', 0.892019829750061, 'val_loss:', 0.22309412002563478)
('epoch', 285, 'train_loss:', 0.89514048337936403, 'val_loss:', 0.22377986669540406)
('epoch', 286, 'train_loss:', 0.89188346862792967, 'val_loss:', 0.22443763256072999)
('epoch', 287, 'train_loss:', 0.89126772880554195, 'val_loss:', 0.22201081275939941)
('epoch', 288, 'train_loss:', 0.89225434780120849, 'val_loss:', 0.22314289808273316)
('epoch', 289, 'train_loss:', 0.89266948699951176, 'val_loss:', 0.22287575483322145)
('epoch', 290, 'train_loss:', 0.89363713026046754, 'val_loss:', 0.22254682540893556)
('epoch', 291, 'train_loss:', 0.89130529642105105, 'val_loss:', 0.22052548170089722)
('epoch', 292, 'train_loss:', 0.88873969316482548, 'val_loss:', 0.22181459426879882)
('epoch', 293, 'train_loss:', 0.88813885211944577, 'val_loss:', 0.22254987716674804)
('epoch', 294, 'train_loss:', 0.88968673229217532, 'val_loss:', 0.2219849395751953)
('epoch', 295, 'train_loss:', 0.89030899286270138, 'val_loss:', 0.22221946716308594)
('epoch', 296, 'train_loss:', 0.88848750352859496, 'val_loss:', 0.22022053003311157)
('epoch', 297, 'train_loss:', 0.89006589412689208, 'val_loss:', 0.22096800088882446)
('epoch', 298, 'train_loss:', 0.88708005189895633, 'val_loss:', 0.22310658454895019)
('epoch', 299, 'train_loss:', 0.88396217584609982, 'val_loss:', 0.22204953193664551)
('epoch', 300, 'train_loss:', 0.87773487329483035, 'val_loss:', 0.21955311059951782)
('epoch', 301, 'train_loss:', 0.88586338520050045, 'val_loss:', 0.22150283098220824)
('epoch', 302, 'train_loss:', 0.8811774706840515, 'val_loss:', 0.22226633548736571)
('epoch', 303, 'train_loss:', 0.88018279075622563, 'val_loss:', 0.22111568450927735)
('epoch', 304, 'train_loss:', 0.88268659353256229, 'val_loss:', 0.22222978353500367)
('epoch', 305, 'train_loss:', 0.88500009059906004, 'val_loss:', 0.22018967628479003)
('epoch', 306, 'train_loss:', 0.87891598939895632, 'val_loss:', 0.22112396717071534)
('epoch', 307, 'train_loss:', 0.88277820587158207, 'val_loss:', 0.22199805736541747)
('epoch', 308, 'train_loss:', 0.88056467294692997, 'val_loss:', 0.22053658008575439)
('epoch', 309, 'train_loss:', 0.8804340410232544, 'val_loss:', 0.2202102017402649)
('epoch', 310, 'train_loss:', 0.88261382818222045, 'val_loss:', 0.22003211021423341)
('epoch', 311, 'train_loss:', 0.88217998266220088, 'val_loss:', 0.21977381467819213)
('epoch', 312, 'train_loss:', 0.87901252031326294, 'val_loss:', 0.22161835908889771)
('epoch', 313, 'train_loss:', 0.87699399232864383, 'val_loss:', 0.21830500841140746)
('epoch', 314, 'train_loss:', 0.8786464405059814, 'val_loss:', 0.21934746980667114)
('epoch', 315, 'train_loss:', 0.8762130045890808, 'val_loss:', 0.22182170152664185)
('epoch', 316, 'train_loss:', 0.87736372709274291, 'val_loss:', 0.22039870500564576)
('epoch', 317, 'train_loss:', 0.87311228275299069, 'val_loss:', 0.21907542705535887)
('epoch', 318, 'train_loss:', 0.87562117338180545, 'val_loss:', 0.22050435066223145)
('epoch', 319, 'train_loss:', 0.87408581733703616, 'val_loss:', 0.2196088695526123)
('epoch', 320, 'train_loss:', 0.87653985977172855, 'val_loss:', 0.21854875326156617)
('epoch', 321, 'train_loss:', 0.87210628509521482, 'val_loss:', 0.2199884533882141)
('epoch', 322, 'train_loss:', 0.87247231960296634, 'val_loss:', 0.2176273274421692)
('epoch', 323, 'train_loss:', 0.87129339218139645, 'val_loss:', 0.21739152193069458)
('epoch', 324, 'train_loss:', 0.87314228773117064, 'val_loss:', 0.22029978275299072)
('epoch', 325, 'train_loss:', 0.86625204324722294, 'val_loss:', 0.21909672737121583)
('epoch', 326, 'train_loss:', 0.86963996171951297, 'val_loss:', 0.2181874132156372)
('epoch', 327, 'train_loss:', 0.8708985781669617, 'val_loss:', 0.21624209165573119)
('epoch', 328, 'train_loss:', 0.87403267621994019, 'val_loss:', 0.21785525798797609)
('epoch', 329, 'train_loss:', 0.86342650890350336, 'val_loss:', 0.21915383338928224)
('epoch', 330, 'train_loss:', 0.87025433063507085, 'val_loss:', 0.21996767759323121)
('epoch', 331, 'train_loss:', 0.86802911281585693, 'val_loss:', 0.21938187122344971)
('epoch', 332, 'train_loss:', 0.86868478059768672, 'val_loss:', 0.21893409252166748)
('epoch', 333, 'train_loss:', 0.86225822687149045, 'val_loss:', 0.21706885099411011)
('epoch', 334, 'train_loss:', 0.86664075374603267, 'val_loss:', 0.21795516967773437)
('epoch', 335, 'train_loss:', 0.86418969631195064, 'val_loss:', 0.21742195367813111)
('epoch', 336, 'train_loss:', 0.86544551610946652, 'val_loss:', 0.21648037672042847)
('epoch', 337, 'train_loss:', 0.86635448694229122, 'val_loss:', 0.21875319004058838)
('epoch', 338, 'train_loss:', 0.86861904621124264, 'val_loss:', 0.21585271120071411)
('epoch', 339, 'train_loss:', 0.86366437911987304, 'val_loss:', 0.21704162597656251)
('epoch', 340, 'train_loss:', 0.86310236930847173, 'val_loss:', 0.21667498111724853)
('epoch', 341, 'train_loss:', 0.86700641632080078, 'val_loss:', 0.21508774518966675)
('epoch', 342, 'train_loss:', 0.86811764001846314, 'val_loss:', 0.21546051979064942)
('epoch', 343, 'train_loss:', 0.85769961595535282, 'val_loss:', 0.21699956178665161)
('epoch', 344, 'train_loss:', 0.86682747125625614, 'val_loss:', 0.21730535030364989)
('epoch', 345, 'train_loss:', 0.86191273927688594, 'val_loss:', 0.21677515506744385)
('epoch', 346, 'train_loss:', 0.86372910976409911, 'val_loss:', 0.21575884342193605)
('epoch', 347, 'train_loss:', 0.85988591432571415, 'val_loss:', 0.21610719203948975)
('epoch', 348, 'train_loss:', 0.86260269403457646, 'val_loss:', 0.21603767871856688)
('epoch', 349, 'train_loss:', 0.86259928703308109, 'val_loss:', 0.21607193470001221)
('epoch', 350, 'train_loss:', 0.86386466741561885, 'val_loss:', 0.2152824306488037)
('epoch', 351, 'train_loss:', 0.86102683067321772, 'val_loss:', 0.21787484645843505)
('epoch', 352, 'train_loss:', 0.858086245059967, 'val_loss:', 0.21705731391906738)
('epoch', 353, 'train_loss:', 0.8551071953773498, 'val_loss:', 0.21573913097381592)
('epoch', 354, 'train_loss:', 0.85821045875549318, 'val_loss:', 0.21541199445724488)
('epoch', 355, 'train_loss:', 0.85656048297882081, 'val_loss:', 0.21275357723236085)
('epoch', 356, 'train_loss:', 0.85840518951416012, 'val_loss:', 0.21361461162567139)
('epoch', 357, 'train_loss:', 0.85669811010360719, 'val_loss:', 0.21307695627212525)
('epoch', 358, 'train_loss:', 0.85576901435852049, 'val_loss:', 0.21444839715957642)
('epoch', 359, 'train_loss:', 0.85715633153915405, 'val_loss:', 0.21434589147567748)
('epoch', 360, 'train_loss:', 0.85508411884307867, 'val_loss:', 0.21474448919296266)
('epoch', 361, 'train_loss:', 0.8568573498725891, 'val_loss:', 0.2153403353691101)
('epoch', 362, 'train_loss:', 0.85500827789306644, 'val_loss:', 0.21309879302978516)
('epoch', 363, 'train_loss:', 0.8545644187927246, 'val_loss:', 0.21276946783065795)
('epoch', 364, 'train_loss:', 0.8502126598358154, 'val_loss:', 0.21474134445190429)
('epoch', 365, 'train_loss:', 0.85083493947982791, 'val_loss:', 0.21422468423843383)
('epoch', 366, 'train_loss:', 0.85309577941894532, 'val_loss:', 0.21439741611480712)
('epoch', 367, 'train_loss:', 0.85108156442642213, 'val_loss:', 0.21339170217514039)
('epoch', 368, 'train_loss:', 0.8501142454147339, 'val_loss:', 0.21271344900131225)
('epoch', 369, 'train_loss:', 0.855200731754303, 'val_loss:', 0.21570404291152953)
('epoch', 370, 'train_loss:', 0.8477160048484802, 'val_loss:', 0.21587376832962035)
('epoch', 371, 'train_loss:', 0.8501683521270752, 'val_loss:', 0.21194112062454223)
('epoch', 372, 'train_loss:', 0.85030559301376341, 'val_loss:', 0.21331384658813476)
('epoch', 373, 'train_loss:', 0.84782517671585078, 'val_loss:', 0.2113329815864563)
('epoch', 374, 'train_loss:', 0.84757009744644163, 'val_loss:', 0.21329164266586303)
('epoch', 375, 'train_loss:', 0.84238801717758183, 'val_loss:', 0.21432158708572388)
('epoch', 376, 'train_loss:', 0.84667131423950193, 'val_loss:', 0.21110620021820067)
('epoch', 377, 'train_loss:', 0.84934429407119749, 'val_loss:', 0.21319814443588256)
('epoch', 378, 'train_loss:', 0.84791424989700315, 'val_loss:', 0.21337630748748779)
('epoch', 379, 'train_loss:', 0.84609785318374631, 'val_loss:', 0.21301078081130981)
('epoch', 380, 'train_loss:', 0.8460443997383118, 'val_loss:', 0.2128684115409851)
('epoch', 381, 'train_loss:', 0.84525344133377078, 'val_loss:', 0.21259637594223021)
('epoch', 382, 'train_loss:', 0.84923790693283085, 'val_loss:', 0.21167850971221924)
('epoch', 383, 'train_loss:', 0.84721902132034299, 'val_loss:', 0.21277377605438233)
('epoch', 384, 'train_loss:', 0.84293447256088261, 'val_loss:', 0.2112030005455017)
('epoch', 385, 'train_loss:', 0.84459391832351682, 'val_loss:', 0.21248461961746215)
('epoch', 386, 'train_loss:', 0.84454577207565307, 'val_loss:', 0.21012038946151734)
('epoch', 387, 'train_loss:', 0.84259271621704102, 'val_loss:', 0.21174644947052002)
('epoch', 388, 'train_loss:', 0.84470236539840693, 'val_loss:', 0.21243480920791627)
('epoch', 389, 'train_loss:', 0.83980759859085086, 'val_loss:', 0.21050784826278687)
('epoch', 390, 'train_loss:', 0.84203433752059942, 'val_loss:', 0.21104848623275757)
('epoch', 391, 'train_loss:', 0.84176146030426025, 'val_loss:', 0.21185919284820556)
('epoch', 392, 'train_loss:', 0.84155860424041751, 'val_loss:', 0.21037653207778931)
('epoch', 393, 'train_loss:', 0.84390268564224247, 'val_loss:', 0.21170338153839111)
('epoch', 394, 'train_loss:', 0.84064581394195559, 'val_loss:', 0.21202406644821167)
('epoch', 395, 'train_loss:', 0.84075941801071163, 'val_loss:', 0.21037315607070922)
('epoch', 396, 'train_loss:', 0.83550012350082392, 'val_loss:', 0.21228256702423096)
('epoch', 397, 'train_loss:', 0.83838144540786741, 'val_loss:', 0.2089069128036499)
('epoch', 398, 'train_loss:', 0.84048588037490846, 'val_loss:', 0.207491455078125)
('epoch', 399, 'train_loss:', 0.83690488338470459, 'val_loss:', 0.20914533138275146)
('epoch', 400, 'train_loss:', 0.83922568798065189, 'val_loss:', 0.21061089038848876)
('epoch', 401, 'train_loss:', 0.84227454423904424, 'val_loss:', 0.21142907619476317)
('epoch', 402, 'train_loss:', 0.83694077014923096, 'val_loss:', 0.2098948836326599)
('epoch', 403, 'train_loss:', 0.8353302907943726, 'val_loss:', 0.2085931420326233)
('epoch', 404, 'train_loss:', 0.8416954064369202, 'val_loss:', 0.20981268405914308)
('epoch', 405, 'train_loss:', 0.83395626783370969, 'val_loss:', 0.20984580993652344)
('epoch', 406, 'train_loss:', 0.83728301048278808, 'val_loss:', 0.20924318552017213)
('epoch', 407, 'train_loss:', 0.83925918579101566, 'val_loss:', 0.208415949344635)
('epoch', 408, 'train_loss:', 0.83296647787094114, 'val_loss:', 0.20793168306350707)
('epoch', 409, 'train_loss:', 0.83607474088668821, 'val_loss:', 0.21004485845565796)
('epoch', 410, 'train_loss:', 0.83473276853561407, 'val_loss:', 0.2083321452140808)
('epoch', 411, 'train_loss:', 0.8327156782150269, 'val_loss:', 0.20879761934280394)
('epoch', 412, 'train_loss:', 0.83270770072937017, 'val_loss:', 0.20870717287063598)
('epoch', 413, 'train_loss:', 0.83474057197570806, 'val_loss:', 0.20791122436523438)
('epoch', 414, 'train_loss:', 0.83654709815979, 'val_loss:', 0.20863208293914795)
('epoch', 415, 'train_loss:', 0.83360441207885738, 'val_loss:', 0.20938370943069459)
('epoch', 416, 'train_loss:', 0.83000858545303347, 'val_loss:', 0.20930328845977783)
('epoch', 417, 'train_loss:', 0.82732165336608887, 'val_loss:', 0.2085018801689148)
('epoch', 418, 'train_loss:', 0.83107128858566282, 'val_loss:', 0.20995839595794677)
('epoch', 419, 'train_loss:', 0.82660662651062011, 'val_loss:', 0.20960810661315918)
('epoch', 420, 'train_loss:', 0.82627951622009277, 'val_loss:', 0.20729161977767943)
('epoch', 421, 'train_loss:', 0.82788522005081178, 'val_loss:', 0.2084005045890808)
('epoch', 422, 'train_loss:', 0.82821748733520506, 'val_loss:', 0.20642603397369386)
('epoch', 423, 'train_loss:', 0.82479651451110836, 'val_loss:', 0.20746811389923095)
('epoch', 424, 'train_loss:', 0.82844478845596314, 'val_loss:', 0.20715489387512206)
('epoch', 425, 'train_loss:', 0.83016742706298829, 'val_loss:', 0.20820506334304809)
('epoch', 426, 'train_loss:', 0.83143348932266237, 'val_loss:', 0.20699682235717773)
('epoch', 427, 'train_loss:', 0.82563731670379636, 'val_loss:', 0.20993394374847413)
('epoch', 428, 'train_loss:', 0.82611366271972653, 'val_loss:', 0.20519258022308351)
('epoch', 429, 'train_loss:', 0.8267305970191956, 'val_loss:', 0.20781805753707885)
('epoch', 430, 'train_loss:', 0.82360755443572997, 'val_loss:', 0.2066213631629944)
('epoch', 431, 'train_loss:', 0.82581013917922974, 'val_loss:', 0.20872709274291992)
('epoch', 432, 'train_loss:', 0.82026383399963376, 'val_loss:', 0.20564668416976928)
('epoch', 433, 'train_loss:', 0.82630167484283445, 'val_loss:', 0.20708962440490722)
('epoch', 434, 'train_loss:', 0.82398525953292845, 'val_loss:', 0.2075095558166504)
('epoch', 435, 'train_loss:', 0.82130463123321529, 'val_loss:', 0.20720685482025147)
('epoch', 436, 'train_loss:', 0.82357367753982547, 'val_loss:', 0.20712274789810181)
('epoch', 437, 'train_loss:', 0.82488177537918095, 'val_loss:', 0.20700518846511839)
('epoch', 438, 'train_loss:', 0.81829704046249385, 'val_loss:', 0.20661520242691039)
('epoch', 439, 'train_loss:', 0.8222479271888733, 'val_loss:', 0.20639314413070678)
('epoch', 440, 'train_loss:', 0.82664149522781372, 'val_loss:', 0.20623767375946045)
('epoch', 441, 'train_loss:', 0.8224826288223267, 'val_loss:', 0.20663694143295289)
('epoch', 442, 'train_loss:', 0.82456308841705317, 'val_loss:', 0.20737700939178466)
('epoch', 443, 'train_loss:', 0.82062340736389161, 'val_loss:', 0.20854777097702026)
('epoch', 444, 'train_loss:', 0.8146999311447144, 'val_loss:', 0.20764847040176393)
('epoch', 445, 'train_loss:', 0.8187898302078247, 'val_loss:', 0.20663686752319335)
('epoch', 446, 'train_loss:', 0.82220654964447026, 'val_loss:', 0.20384273052215576)
('epoch', 447, 'train_loss:', 0.81850388050079348, 'val_loss:', 0.2049805474281311)
('epoch', 448, 'train_loss:', 0.81840084314346317, 'val_loss:', 0.20519454956054686)
('epoch', 449, 'train_loss:', 0.82226959943771361, 'val_loss:', 0.20632229328155519)
('epoch', 450, 'train_loss:', 0.82307783126831058, 'val_loss:', 0.20714004039764405)
('epoch', 451, 'train_loss:', 0.8149344778060913, 'val_loss:', 0.20505056381225586)
('epoch', 452, 'train_loss:', 0.81449718713760377, 'val_loss:', 0.20499531030654908)
('epoch', 453, 'train_loss:', 0.81542637586593625, 'val_loss:', 0.20330474853515626)
('epoch', 454, 'train_loss:', 0.81691540718078615, 'val_loss:', 0.20589012861251832)
('epoch', 455, 'train_loss:', 0.81944035291671757, 'val_loss:', 0.20356260061264039)
('epoch', 456, 'train_loss:', 0.81811463832855225, 'val_loss:', 0.20456196784973144)
('epoch', 457, 'train_loss:', 0.8181263303756714, 'val_loss:', 0.20452669858932496)
('epoch', 458, 'train_loss:', 0.81529183864593502, 'val_loss:', 0.20200536727905274)
('epoch', 459, 'train_loss:', 0.81481404066085816, 'val_loss:', 0.20374286890029908)
('epoch', 460, 'train_loss:', 0.8114421606063843, 'val_loss:', 0.20460690975189208)
('epoch', 461, 'train_loss:', 0.81226369857788083, 'val_loss:', 0.20505763053894044)
('epoch', 462, 'train_loss:', 0.80971729755401611, 'val_loss:', 0.20417975425720214)
('epoch', 463, 'train_loss:', 0.80565497875213621, 'val_loss:', 0.20415773630142212)
('epoch', 464, 'train_loss:', 0.81250418424606319, 'val_loss:', 0.20478904008865356)
('epoch', 465, 'train_loss:', 0.80995743751525884, 'val_loss:', 0.20503297567367554)
('epoch', 466, 'train_loss:', 0.80522646188735958, 'val_loss:', 0.20434260129928589)
('epoch', 467, 'train_loss:', 0.8079559350013733, 'val_loss:', 0.2037842583656311)
('epoch', 468, 'train_loss:', 0.81224290847778324, 'val_loss:', 0.203952898979187)
('epoch', 469, 'train_loss:', 0.80684886693954472, 'val_loss:', 0.20091528892517091)
('epoch', 470, 'train_loss:', 0.81077836990356444, 'val_loss:', 0.2015712332725525)
('epoch', 471, 'train_loss:', 0.81124671697616579, 'val_loss:', 0.20487734079360961)
('epoch', 472, 'train_loss:', 0.80657496452331545, 'val_loss:', 0.202277250289917)
('epoch', 473, 'train_loss:', 0.80821697711944585, 'val_loss:', 0.20262461423873901)
('epoch', 474, 'train_loss:', 0.81188109159469601, 'val_loss:', 0.20261903285980223)
('epoch', 475, 'train_loss:', 0.80544834852218627, 'val_loss:', 0.2017937731742859)
('epoch', 476, 'train_loss:', 0.80920158147811894, 'val_loss:', 0.20330961942672729)
('epoch', 477, 'train_loss:', 0.80636482954025268, 'val_loss:', 0.20142461538314818)
('epoch', 478, 'train_loss:', 0.80997368812561032, 'val_loss:', 0.20233746051788329)
('epoch', 479, 'train_loss:', 0.80565896272659299, 'val_loss:', 0.2007948350906372)
('epoch', 480, 'train_loss:', 0.80056618690490722, 'val_loss:', 0.20214567184448243)
('epoch', 481, 'train_loss:', 0.80316564559936521, 'val_loss:', 0.20261244535446166)
('epoch', 482, 'train_loss:', 0.80393965244293208, 'val_loss:', 0.20138108968734741)
('epoch', 483, 'train_loss:', 0.80463862419128418, 'val_loss:', 0.20347929239273072)
('epoch', 484, 'train_loss:', 0.80867365837097172, 'val_loss:', 0.20592310667037964)
('epoch', 485, 'train_loss:', 0.80419082403182984, 'val_loss:', 0.20259259939193724)
('epoch', 486, 'train_loss:', 0.80465721130371093, 'val_loss:', 0.20157909631729126)
('epoch', 487, 'train_loss:', 0.80470074176788331, 'val_loss:', 0.20154514551162719)
('epoch', 488, 'train_loss:', 0.80088919878005982, 'val_loss:', 0.20182425260543824)
('epoch', 489, 'train_loss:', 0.80438995599746699, 'val_loss:', 0.19973927974700928)
('epoch', 490, 'train_loss:', 0.79787193059921269, 'val_loss:', 0.20148874044418336)
('epoch', 491, 'train_loss:', 0.79848397254943848, 'val_loss:', 0.20039906740188598)
('epoch', 492, 'train_loss:', 0.79800513982772825, 'val_loss:', 0.20141054868698119)
('epoch', 493, 'train_loss:', 0.79638295412063598, 'val_loss:', 0.20192128181457519)
('epoch', 494, 'train_loss:', 0.79994497776031492, 'val_loss:', 0.20128727674484254)
('epoch', 495, 'train_loss:', 0.79798487663269047, 'val_loss:', 0.19917337179183961)
('epoch', 496, 'train_loss:', 0.8026533031463623, 'val_loss:', 0.20170102357864381)
('epoch', 497, 'train_loss:', 0.80091123342514037, 'val_loss:', 0.19959629058837891)
('epoch', 498, 'train_loss:', 0.7976872634887695, 'val_loss:', 0.20143854141235351)
('epoch', 499, 'train_loss:', 0.79802415132522586, 'val_loss:', 0.20055037021636962)
('epoch', 500, 'train_loss:', 0.7969351649284363, 'val_loss:', 0.2004953169822693)
('epoch', 501, 'train_loss:', 0.79998182058334355, 'val_loss:', 0.20127917528152467)
('epoch', 502, 'train_loss:', 0.7986843276023865, 'val_loss:', 0.20128646850585938)
('epoch', 503, 'train_loss:', 0.79708451271057124, 'val_loss:', 0.20221135616302491)
('epoch', 504, 'train_loss:', 0.79815710067749024, 'val_loss:', 0.20006664991378784)
('epoch', 505, 'train_loss:', 0.7928129196166992, 'val_loss:', 0.19871113061904908)
('epoch', 506, 'train_loss:', 0.79759362936019895, 'val_loss:', 0.19927598953247069)
('epoch', 507, 'train_loss:', 0.79390207290649417, 'val_loss:', 0.20037747859954835)
('epoch', 508, 'train_loss:', 0.79390990495681768, 'val_loss:', 0.20182254791259766)
('epoch', 509, 'train_loss:', 0.79359297513961791, 'val_loss:', 0.19964059352874755)
('epoch', 510, 'train_loss:', 0.79264470577239987, 'val_loss:', 0.19879267692565919)
('epoch', 511, 'train_loss:', 0.7931584239006042, 'val_loss:', 0.19755972623825074)
('epoch', 512, 'train_loss:', 0.79579261064529416, 'val_loss:', 0.19901167869567871)
('epoch', 513, 'train_loss:', 0.79396120786666868, 'val_loss:', 0.19677524089813234)
('epoch', 514, 'train_loss:', 0.79130269289016719, 'val_loss:', 0.19827556371688843)
('epoch', 515, 'train_loss:', 0.78766585111618037, 'val_loss:', 0.19847452640533447)
('epoch', 516, 'train_loss:', 0.79223340749740601, 'val_loss:', 0.1993107032775879)
('epoch', 517, 'train_loss:', 0.78842473030090332, 'val_loss:', 0.19935333251953125)
('epoch', 518, 'train_loss:', 0.79331728458404538, 'val_loss:', 0.19828105688095093)
('epoch', 519, 'train_loss:', 0.78670800209045411, 'val_loss:', 0.19739138603210449)
('epoch', 520, 'train_loss:', 0.78966380357742305, 'val_loss:', 0.19728085517883301)
('epoch', 521, 'train_loss:', 0.78466885328292846, 'val_loss:', 0.19703594207763672)
('epoch', 522, 'train_loss:', 0.78819450855255124, 'val_loss:', 0.19669008016586303)
('epoch', 523, 'train_loss:', 0.78711049556732182, 'val_loss:', 0.19801429986953736)
('epoch', 524, 'train_loss:', 0.78891027927398683, 'val_loss:', 0.19756787061691283)
('epoch', 525, 'train_loss:', 0.78585458755493165, 'val_loss:', 0.19778179168701171)
('epoch', 526, 'train_loss:', 0.78195743799209594, 'val_loss:', 0.19657147407531739)
('epoch', 527, 'train_loss:', 0.7886502909660339, 'val_loss:', 0.19745977878570556)
('epoch', 528, 'train_loss:', 0.78361110925674438, 'val_loss:', 0.19819411277770996)
('epoch', 529, 'train_loss:', 0.78603559732437134, 'val_loss:', 0.19742502212524415)
('epoch', 530, 'train_loss:', 0.78474623918533326, 'val_loss:', 0.19735517024993895)
('epoch', 531, 'train_loss:', 0.78502561807632443, 'val_loss:', 0.19820431947708131)
('epoch', 532, 'train_loss:', 0.78626020908355709, 'val_loss:', 0.19469017267227173)
('epoch', 533, 'train_loss:', 0.78848327398300166, 'val_loss:', 0.19826377630233766)
('epoch', 534, 'train_loss:', 0.7789992761611938, 'val_loss:', 0.19559537887573242)
('epoch', 535, 'train_loss:', 0.78193994522094723, 'val_loss:', 0.19632262945175172)
('epoch', 536, 'train_loss:', 0.78229743242263794, 'val_loss:', 0.19747450828552246)
('epoch', 537, 'train_loss:', 0.78688787698745732, 'val_loss:', 0.19604560613632202)
('epoch', 538, 'train_loss:', 0.7768477201461792, 'val_loss:', 0.19540804386138916)
('epoch', 539, 'train_loss:', 0.77828651905059809, 'val_loss:', 0.197732572555542)
('epoch', 540, 'train_loss:', 0.77956633090972904, 'val_loss:', 0.19918508291244508)
('epoch', 541, 'train_loss:', 0.78250276565551757, 'val_loss:', 0.19651510715484619)
('epoch', 542, 'train_loss:', 0.78097010374069209, 'val_loss:', 0.19885436773300172)
('epoch', 543, 'train_loss:', 0.77883505344390869, 'val_loss:', 0.19467750787734986)
('epoch', 544, 'train_loss:', 0.77688667535781863, 'val_loss:', 0.19535029411315918)
('epoch', 545, 'train_loss:', 0.78102769136428829, 'val_loss:', 0.19562020301818847)
('epoch', 546, 'train_loss:', 0.77955839872360233, 'val_loss:', 0.19530463933944703)
('epoch', 547, 'train_loss:', 0.78223909616470333, 'val_loss:', 0.19539998769760131)
('epoch', 548, 'train_loss:', 0.77652043104171753, 'val_loss:', 0.19640130043029785)
('epoch', 549, 'train_loss:', 0.77762193202972407, 'val_loss:', 0.19461620092391968)
('epoch', 550, 'train_loss:', 0.78263308525085451, 'val_loss:', 0.19665127754211426)
('epoch', 551, 'train_loss:', 0.77273323059082033, 'val_loss:', 0.19466058254241944)
('epoch', 552, 'train_loss:', 0.77596513986587523, 'val_loss:', 0.19580009698867798)
('epoch', 553, 'train_loss:', 0.77684669733047484, 'val_loss:', 0.19545818328857423)
('epoch', 554, 'train_loss:', 0.77078122615814204, 'val_loss:', 0.1933158016204834)
('epoch', 555, 'train_loss:', 0.77805296659469603, 'val_loss:', 0.19445262908935546)
('epoch', 556, 'train_loss:', 0.77035378694534307, 'val_loss:', 0.19435109138488771)
('epoch', 557, 'train_loss:', 0.77426566123962404, 'val_loss:', 0.19310568571090697)
('epoch', 558, 'train_loss:', 0.77312359333038327, 'val_loss:', 0.19356831789016724)
('epoch', 559, 'train_loss:', 0.76911872625350952, 'val_loss:', 0.19446322917938233)
('epoch', 560, 'train_loss:', 0.77472089052200321, 'val_loss:', 0.19581032991409303)
('epoch', 561, 'train_loss:', 0.77466166496276856, 'val_loss:', 0.1949438738822937)
('epoch', 562, 'train_loss:', 0.77084312200546268, 'val_loss:', 0.19453078746795655)
('epoch', 563, 'train_loss:', 0.77195959568023687, 'val_loss:', 0.19442770719528199)
('epoch', 564, 'train_loss:', 0.77304763555526734, 'val_loss:', 0.19730203866958618)
('epoch', 565, 'train_loss:', 0.76755427598953252, 'val_loss:', 0.19622367858886719)
('epoch', 566, 'train_loss:', 0.76807192325592044, 'val_loss:', 0.19333986043930054)
('epoch', 567, 'train_loss:', 0.77258579730987553, 'val_loss:', 0.19438271760940551)
('epoch', 568, 'train_loss:', 0.77110243797302247, 'val_loss:', 0.19310650110244751)
('epoch', 569, 'train_loss:', 0.76857230901718143, 'val_loss:', 0.19425948858261108)
('epoch', 570, 'train_loss:', 0.77238072395324708, 'val_loss:', 0.1941241765022278)
('epoch', 571, 'train_loss:', 0.7712020349502563, 'val_loss:', 0.19358496427536009)
('epoch', 572, 'train_loss:', 0.77123766183853149, 'val_loss:', 0.19290167570114136)
('epoch', 573, 'train_loss:', 0.77009682178497318, 'val_loss:', 0.19322136878967286)
('epoch', 574, 'train_loss:', 0.76434213638305659, 'val_loss:', 0.19514523029327394)
('epoch', 575, 'train_loss:', 0.7711977887153626, 'val_loss:', 0.19242146492004394)
('epoch', 576, 'train_loss:', 0.7713634133338928, 'val_loss:', 0.19395577907562256)
('epoch', 577, 'train_loss:', 0.76550837755203249, 'val_loss:', 0.19407838821411133)
('epoch', 578, 'train_loss:', 0.77138937950134279, 'val_loss:', 0.19311434030532837)
('epoch', 579, 'train_loss:', 0.76661888599395756, 'val_loss:', 0.1893463134765625)
('epoch', 580, 'train_loss:', 0.77159454107284542, 'val_loss:', 0.1934363865852356)
('epoch', 581, 'train_loss:', 0.76445482015609745, 'val_loss:', 0.19425094127655029)
('epoch', 582, 'train_loss:', 0.76107817888259888, 'val_loss:', 0.19241055727005005)
('epoch', 583, 'train_loss:', 0.76535191774368283, 'val_loss:', 0.19230916023254394)
('epoch', 584, 'train_loss:', 0.76627826690673828, 'val_loss:', 0.1950834560394287)
('epoch', 585, 'train_loss:', 0.7610015869140625, 'val_loss:', 0.1913245964050293)
('epoch', 586, 'train_loss:', 0.76185881733894345, 'val_loss:', 0.1938760733604431)
('epoch', 587, 'train_loss:', 0.76500692844390872, 'val_loss:', 0.19129536151885987)
('epoch', 588, 'train_loss:', 0.76520193815231319, 'val_loss:', 0.1932465887069702)
('epoch', 589, 'train_loss:', 0.76458667993545537, 'val_loss:', 0.19195732355117798)
('epoch', 590, 'train_loss:', 0.75743032693862911, 'val_loss:', 0.19359615325927734)
('epoch', 591, 'train_loss:', 0.7646627044677734, 'val_loss:', 0.19231763124465942)
('epoch', 592, 'train_loss:', 0.75951519012451174, 'val_loss:', 0.1929314136505127)
('epoch', 593, 'train_loss:', 0.76144449710845952, 'val_loss:', 0.19305756092071533)
('epoch', 594, 'train_loss:', 0.76203214049339296, 'val_loss:', 0.19070456504821778)
('epoch', 595, 'train_loss:', 0.76069373607635493, 'val_loss:', 0.19001540660858154)
('epoch', 596, 'train_loss:', 0.76046909809112551, 'val_loss:', 0.19212961435317993)
('epoch', 597, 'train_loss:', 0.75665951490402217, 'val_loss:', 0.18854928255081177)
('epoch', 598, 'train_loss:', 0.75789779186248785, 'val_loss:', 0.19119099617004395)
('epoch', 599, 'train_loss:', 0.75543759584426884, 'val_loss:', 0.19305322408676148)
