(keras)skoppula@sls-sm-7:~/lstm-testing/rnn-from-scratch$ CUDA_VISIBLE_DEVICES=2 python lstm_all_variants.py -t -d sms -v additive_sigmoid
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
{'dataset': 'sms', 'train': True, 'variant': 'additive_sigmoid', 'generate': False, 'num_words': None}
('fetched data. trn/test data shape: ', (9292, 30), (9292, 30), (2323, 30), (2323, 30))
('num steps in trn and val epochs', 36, 9)
('Model name', 'lstm_all_variants')
building graph...
created model.
starting to train model!
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:81:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:81:00.0)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3167 get requests, put_count=2833 evicted_count=1000 eviction_rate=0.352983 and unsatisfied allocation rate=0.452794
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3342 get requests, put_count=3498 evicted_count=1000 eviction_rate=0.285878 and unsatisfied allocation rate=0.259425
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
('epoch', 0, 'train_loss:', 1.7661132001876831, 'val_loss:', 0.43284665584564208)
('epoch', 1, 'train_loss:', 1.7046848487854005, 'val_loss:', 0.41959027767181395)
('epoch', 2, 'train_loss:', 1.6582647562026978, 'val_loss:', 0.40906509399414065)
('epoch', 3, 'train_loss:', 1.6213754272460938, 'val_loss:', 0.40047993659973147)
('epoch', 4, 'train_loss:', 1.5923597478866578, 'val_loss:', 0.39441492080688478)
('epoch', 5, 'train_loss:', 1.5670052909851073, 'val_loss:', 0.38885471343994138)
('epoch', 6, 'train_loss:', 1.5463618087768554, 'val_loss:', 0.38432311058044433)
('epoch', 7, 'train_loss:', 1.5295509624481201, 'val_loss:', 0.37907130718231202)
('epoch', 8, 'train_loss:', 1.5136173486709594, 'val_loss:', 0.37669294834136963)
('epoch', 9, 'train_loss:', 1.5007327890396118, 'val_loss:', 0.37265224933624269)
('epoch', 10, 'train_loss:', 1.483904013633728, 'val_loss:', 0.36936501979827879)
('epoch', 11, 'train_loss:', 1.4665314674377441, 'val_loss:', 0.36369235515594484)
('epoch', 12, 'train_loss:', 1.4457822823524475, 'val_loss:', 0.3580589699745178)
('epoch', 13, 'train_loss:', 1.4207139873504639, 'val_loss:', 0.3500273370742798)
('epoch', 14, 'train_loss:', 1.3925861215591431, 'val_loss:', 0.34348561048507692)
('epoch', 15, 'train_loss:', 1.3592379069328309, 'val_loss:', 0.33620124578475952)
('epoch', 16, 'train_loss:', 1.3362429356575012, 'val_loss:', 0.32981282234191894)
('epoch', 17, 'train_loss:', 1.315109257698059, 'val_loss:', 0.32596643209457399)
('epoch', 18, 'train_loss:', 1.2975693202018739, 'val_loss:', 0.32151695013046266)
('epoch', 19, 'train_loss:', 1.2833777070045471, 'val_loss:', 0.31877875328063965)
('epoch', 20, 'train_loss:', 1.2716431784629822, 'val_loss:', 0.31458305835723877)
('epoch', 21, 'train_loss:', 1.2585476303100587, 'val_loss:', 0.31406441211700442)
('epoch', 22, 'train_loss:', 1.2447681546211242, 'val_loss:', 0.31023684501647947)
('epoch', 23, 'train_loss:', 1.2392814445495606, 'val_loss:', 0.30759986162185671)
('epoch', 24, 'train_loss:', 1.2260282158851623, 'val_loss:', 0.30619268178939818)
('epoch', 25, 'train_loss:', 1.2218623423576356, 'val_loss:', 0.302954638004303)
('epoch', 26, 'train_loss:', 1.2127144503593446, 'val_loss:', 0.30099533557891844)
('epoch', 27, 'train_loss:', 1.1966815018653869, 'val_loss:', 0.2989848256111145)
('epoch', 28, 'train_loss:', 1.1941760897636413, 'val_loss:', 0.29773057460784913)
('epoch', 29, 'train_loss:', 1.1869957470893859, 'val_loss:', 0.29715213537216184)
('epoch', 30, 'train_loss:', 1.1782494592666626, 'val_loss:', 0.29374125957489011)
('epoch', 31, 'train_loss:', 1.1727685832977295, 'val_loss:', 0.29232958316802976)
('epoch', 32, 'train_loss:', 1.164565544128418, 'val_loss:', 0.29101442098617553)
('epoch', 33, 'train_loss:', 1.1567367911338806, 'val_loss:', 0.28815732479095457)
('epoch', 34, 'train_loss:', 1.1551241445541383, 'val_loss:', 0.28680595636367801)
('epoch', 35, 'train_loss:', 1.1454362225532533, 'val_loss:', 0.28403959989547728)
('epoch', 36, 'train_loss:', 1.1391564631462097, 'val_loss:', 0.28344344854354858)
('epoch', 37, 'train_loss:', 1.1347044801712036, 'val_loss:', 0.28232736587524415)
('epoch', 38, 'train_loss:', 1.127980899810791, 'val_loss:', 0.28055575132369998)
('epoch', 39, 'train_loss:', 1.1202217912673951, 'val_loss:', 0.27798285961151126)
('epoch', 40, 'train_loss:', 1.1160985565185546, 'val_loss:', 0.27749364852905273)
('epoch', 41, 'train_loss:', 1.1069212365150451, 'val_loss:', 0.27629620552062989)
('epoch', 42, 'train_loss:', 1.1047344970703126, 'val_loss:', 0.27330387592315675)
('epoch', 43, 'train_loss:', 1.0956771516799926, 'val_loss:', 0.27165370464324951)
('epoch', 44, 'train_loss:', 1.0900652647018432, 'val_loss:', 0.2710652709007263)
('epoch', 45, 'train_loss:', 1.0834631657600402, 'val_loss:', 0.26934994697570802)
('epoch', 46, 'train_loss:', 1.0768291330337525, 'val_loss:', 0.26792845487594602)
('epoch', 47, 'train_loss:', 1.0718605184555055, 'val_loss:', 0.26691815137863162)
('epoch', 48, 'train_loss:', 1.0644974660873414, 'val_loss:', 0.26506603717803956)
('epoch', 49, 'train_loss:', 1.0601414108276368, 'val_loss:', 0.2644577360153198)
('epoch', 50, 'train_loss:', 1.0541745543479919, 'val_loss:', 0.26199898242950437)
('epoch', 51, 'train_loss:', 1.0482135343551635, 'val_loss:', 0.26030503034591673)
('epoch', 52, 'train_loss:', 1.0421547722816467, 'val_loss:', 0.25935986995697019)
('epoch', 53, 'train_loss:', 1.0379275727272033, 'val_loss:', 0.25877404451370239)
('epoch', 54, 'train_loss:', 1.028126473426819, 'val_loss:', 0.25771425008773802)
('epoch', 55, 'train_loss:', 1.0253591561317443, 'val_loss:', 0.25598051071166994)
('epoch', 56, 'train_loss:', 1.0246599340438842, 'val_loss:', 0.25565548658370973)
('epoch', 57, 'train_loss:', 1.0165508532524108, 'val_loss:', 0.25512742757797241)
('epoch', 58, 'train_loss:', 1.0136825656890869, 'val_loss:', 0.25399790763854979)
('epoch', 59, 'train_loss:', 1.007673032283783, 'val_loss:', 0.25264915943145749)
('epoch', 60, 'train_loss:', 1.0061039137840271, 'val_loss:', 0.25178646564483642)
('epoch', 61, 'train_loss:', 1.0010785007476806, 'val_loss:', 0.25050107240676878)
('epoch', 62, 'train_loss:', 0.99492425918579097, 'val_loss:', 0.25024867296218872)
('epoch', 63, 'train_loss:', 0.99342360258102413, 'val_loss:', 0.24912342548370361)
('epoch', 64, 'train_loss:', 0.99319417715072633, 'val_loss:', 0.24904880762100221)
('epoch', 65, 'train_loss:', 0.98457291364669797, 'val_loss:', 0.24783303260803222)
('epoch', 66, 'train_loss:', 0.98880170345306395, 'val_loss:', 0.24712472677230835)
('epoch', 67, 'train_loss:', 0.98196251630783082, 'val_loss:', 0.24793240547180176)
('epoch', 68, 'train_loss:', 0.9787645602226257, 'val_loss:', 0.24569644689559936)
('epoch', 69, 'train_loss:', 0.97512631893157964, 'val_loss:', 0.24562978744506836)
('epoch', 70, 'train_loss:', 0.97631655454635624, 'val_loss:', 0.24207548379898072)
('epoch', 71, 'train_loss:', 0.96963722944259645, 'val_loss:', 0.24264944791793824)
('epoch', 72, 'train_loss:', 0.96560010194778445, 'val_loss:', 0.24310187816619874)
('epoch', 73, 'train_loss:', 0.96449965715408326, 'val_loss:', 0.24047243118286132)
('epoch', 74, 'train_loss:', 0.96446717500686641, 'val_loss:', 0.24065619945526123)
('epoch', 75, 'train_loss:', 0.95907862663269039, 'val_loss:', 0.24097708463668824)
('epoch', 76, 'train_loss:', 0.96070149183273312, 'val_loss:', 0.23935593366622926)
('epoch', 77, 'train_loss:', 0.96035227775573728, 'val_loss:', 0.23887561798095702)
('epoch', 78, 'train_loss:', 0.95257165908813479, 'val_loss:', 0.23727900981903077)
('epoch', 79, 'train_loss:', 0.95439593791961674, 'val_loss:', 0.23980416297912599)
('epoch', 80, 'train_loss:', 0.94567535400390623, 'val_loss:', 0.23729338407516479)
('epoch', 81, 'train_loss:', 0.94592964410781866, 'val_loss:', 0.23618198394775392)
('epoch', 82, 'train_loss:', 0.94751299619674678, 'val_loss:', 0.23748467922210692)
('epoch', 83, 'train_loss:', 0.94286662578582758, 'val_loss:', 0.23464127540588378)
('epoch', 84, 'train_loss:', 0.93754861354827879, 'val_loss:', 0.23501149415969849)
('epoch', 85, 'train_loss:', 0.9386180925369263, 'val_loss:', 0.23337240219116212)
('epoch', 86, 'train_loss:', 0.93367357969284059, 'val_loss:', 0.23508166313171386)
('epoch', 87, 'train_loss:', 0.93761192560195927, 'val_loss:', 0.23294719457626342)
('epoch', 88, 'train_loss:', 0.93002761840820314, 'val_loss:', 0.23467808246612548)
('epoch', 89, 'train_loss:', 0.93231462001800536, 'val_loss:', 0.23273589849472046)
('epoch', 90, 'train_loss:', 0.9312716317176819, 'val_loss:', 0.23157361030578613)
('epoch', 91, 'train_loss:', 0.9242693829536438, 'val_loss:', 0.23240603923797606)
('epoch', 92, 'train_loss:', 0.92504569053649899, 'val_loss:', 0.23319632053375244)
('epoch', 93, 'train_loss:', 0.92350277662277225, 'val_loss:', 0.231461124420166)
('epoch', 94, 'train_loss:', 0.91980982542037959, 'val_loss:', 0.22665195941925048)
('epoch', 95, 'train_loss:', 0.91550675868988041, 'val_loss:', 0.22979698657989503)
('epoch', 96, 'train_loss:', 0.91552738189697269, 'val_loss:', 0.22900269508361817)
('epoch', 97, 'train_loss:', 0.91823193550109861, 'val_loss:', 0.22911348819732666)
('epoch', 98, 'train_loss:', 0.91301548242568975, 'val_loss:', 0.22974385261535646)
('epoch', 99, 'train_loss:', 0.91315405845642095, 'val_loss:', 0.22710227727890014)
('epoch', 100, 'train_loss:', 0.90923735857009891, 'val_loss:', 0.22777386903762817)
('epoch', 101, 'train_loss:', 0.91109135627746585, 'val_loss:', 0.22608666896820068)
('epoch', 102, 'train_loss:', 0.9050434970855713, 'val_loss:', 0.22582686424255372)
('epoch', 103, 'train_loss:', 0.90373096704483036, 'val_loss:', 0.22777185678482056)
('epoch', 104, 'train_loss:', 0.90587159395217898, 'val_loss:', 0.22761646986007691)
('epoch', 105, 'train_loss:', 0.90537945032119749, 'val_loss:', 0.22690649271011354)
('epoch', 106, 'train_loss:', 0.90020915985107419, 'val_loss:', 0.22570161581039427)
('epoch', 107, 'train_loss:', 0.90155107975006099, 'val_loss:', 0.22525044202804564)
('epoch', 108, 'train_loss:', 0.89818757534027105, 'val_loss:', 0.22511751890182496)
('epoch', 109, 'train_loss:', 0.89876352548599248, 'val_loss:', 0.22254255056381225)
('epoch', 110, 'train_loss:', 0.89247588872909545, 'val_loss:', 0.22365740060806275)
('epoch', 111, 'train_loss:', 0.89143488407135008, 'val_loss:', 0.22354198932647706)
('epoch', 112, 'train_loss:', 0.89243281841278077, 'val_loss:', 0.22366805315017702)
('epoch', 113, 'train_loss:', 0.88705924034118655, 'val_loss:', 0.22239764690399169)
('epoch', 114, 'train_loss:', 0.88872732639312746, 'val_loss:', 0.22143736600875855)
('epoch', 115, 'train_loss:', 0.88505815029144286, 'val_loss:', 0.22174125432968139)
('epoch', 116, 'train_loss:', 0.88538673877716068, 'val_loss:', 0.22292414903640748)
('epoch', 117, 'train_loss:', 0.88245357751846309, 'val_loss:', 0.2223649001121521)
('epoch', 118, 'train_loss:', 0.88230103492736811, 'val_loss:', 0.21967344045639037)
('epoch', 119, 'train_loss:', 0.88154156446456911, 'val_loss:', 0.22025290489196778)
('epoch', 120, 'train_loss:', 0.88075181484222409, 'val_loss:', 0.22173610925674439)
('epoch', 121, 'train_loss:', 0.87463812589645384, 'val_loss:', 0.21848641633987426)
('epoch', 122, 'train_loss:', 0.87807345390319824, 'val_loss:', 0.21999965906143187)
('epoch', 123, 'train_loss:', 0.87439087629318235, 'val_loss:', 0.21935568094253541)
('epoch', 124, 'train_loss:', 0.87534204244613645, 'val_loss:', 0.21929291725158692)
('epoch', 125, 'train_loss:', 0.87065564155578612, 'val_loss:', 0.21766958236694336)
('epoch', 126, 'train_loss:', 0.87294333934783941, 'val_loss:', 0.21830923318862916)
('epoch', 127, 'train_loss:', 0.86974674224853521, 'val_loss:', 0.21886445999145507)
('epoch', 128, 'train_loss:', 0.86694982290267941, 'val_loss:', 0.22097358226776123)
('epoch', 129, 'train_loss:', 0.8656128406524658, 'val_loss:', 0.21703231573104859)
('epoch', 130, 'train_loss:', 0.86750424146652216, 'val_loss:', 0.21692507505416869)
('epoch', 131, 'train_loss:', 0.8622255611419678, 'val_loss:', 0.21525155305862426)
('epoch', 132, 'train_loss:', 0.86232807397842404, 'val_loss:', 0.2157220697402954)
('epoch', 133, 'train_loss:', 0.85859480619430539, 'val_loss:', 0.21468714475631714)
('epoch', 134, 'train_loss:', 0.86325283765792848, 'val_loss:', 0.21755295515060424)
('epoch', 135, 'train_loss:', 0.85767194747924802, 'val_loss:', 0.21485371112823487)
('epoch', 136, 'train_loss:', 0.85701077222824096, 'val_loss:', 0.21334035396575929)
('epoch', 137, 'train_loss:', 0.8542073035240173, 'val_loss:', 0.21478139400482177)
('epoch', 138, 'train_loss:', 0.85506245374679568, 'val_loss:', 0.21491608858108521)
('epoch', 139, 'train_loss:', 0.85327792644500733, 'val_loss:', 0.21370708465576171)
('epoch', 140, 'train_loss:', 0.84994391918182377, 'val_loss:', 0.21255927085876464)
('epoch', 141, 'train_loss:', 0.84850043773651118, 'val_loss:', 0.21299706935882567)
('epoch', 142, 'train_loss:', 0.84925352096557616, 'val_loss:', 0.21249059438705445)
('epoch', 143, 'train_loss:', 0.85077465057373047, 'val_loss:', 0.21148012399673463)
('epoch', 144, 'train_loss:', 0.84599246025085451, 'val_loss:', 0.21300826072692872)
('epoch', 145, 'train_loss:', 0.84583257436752324, 'val_loss:', 0.21183557748794557)
('epoch', 146, 'train_loss:', 0.84513969421386714, 'val_loss:', 0.21049188375473021)
('epoch', 147, 'train_loss:', 0.84489160299301147, 'val_loss:', 0.21244390010833741)
('epoch', 148, 'train_loss:', 0.8399744725227356, 'val_loss:', 0.21001690149307251)
('epoch', 149, 'train_loss:', 0.84445324420928958, 'val_loss:', 0.21093914985656739)
('epoch', 150, 'train_loss:', 0.84012873888015749, 'val_loss:', 0.21001125097274781)
('epoch', 151, 'train_loss:', 0.84103468418121341, 'val_loss:', 0.21129393577575684)
('epoch', 152, 'train_loss:', 0.8320175981521607, 'val_loss:', 0.21300854921340942)
('epoch', 153, 'train_loss:', 0.83911318778991695, 'val_loss:', 0.21058183908462524)
('epoch', 154, 'train_loss:', 0.83211929082870484, 'val_loss:', 0.20981390714645387)
('epoch', 155, 'train_loss:', 0.82974395990371708, 'val_loss:', 0.20757757425308226)
('epoch', 156, 'train_loss:', 0.83130613088607785, 'val_loss:', 0.20992759704589845)
('epoch', 157, 'train_loss:', 0.82986124038696285, 'val_loss:', 0.20784732103347778)
('epoch', 158, 'train_loss:', 0.82845815420150759, 'val_loss:', 0.20936547994613647)
('epoch', 159, 'train_loss:', 0.82872848033905033, 'val_loss:', 0.20798390388488769)
('epoch', 160, 'train_loss:', 0.82622993469238282, 'val_loss:', 0.20829691410064696)
('epoch', 161, 'train_loss:', 0.82512660741806032, 'val_loss:', 0.20510248184204102)
('epoch', 162, 'train_loss:', 0.82615367650985716, 'val_loss:', 0.20955498218536378)
('epoch', 163, 'train_loss:', 0.82051609039306639, 'val_loss:', 0.20462025880813597)
('epoch', 164, 'train_loss:', 0.82338292837142946, 'val_loss:', 0.20613755464553832)
('epoch', 165, 'train_loss:', 0.82122764110565183, 'val_loss:', 0.20521223545074463)
('epoch', 166, 'train_loss:', 0.82094228267669678, 'val_loss:', 0.2047030210494995)
('epoch', 167, 'train_loss:', 0.8200576066970825, 'val_loss:', 0.20519353151321412)
('epoch', 168, 'train_loss:', 0.81723543882369998, 'val_loss:', 0.20491427898406983)
('epoch', 169, 'train_loss:', 0.81524425745010376, 'val_loss:', 0.20652840852737428)
('epoch', 170, 'train_loss:', 0.81244917631149294, 'val_loss:', 0.20341188669204713)
('epoch', 171, 'train_loss:', 0.81456664085388186, 'val_loss:', 0.20493517637252809)
('epoch', 172, 'train_loss:', 0.81311114549636843, 'val_loss:', 0.20303826808929443)
('epoch', 173, 'train_loss:', 0.81315310239791871, 'val_loss:', 0.20301053047180176)
('epoch', 174, 'train_loss:', 0.80881239175796504, 'val_loss:', 0.20447446584701537)
('epoch', 175, 'train_loss:', 0.80805157899856572, 'val_loss:', 0.20294029712677003)
('epoch', 176, 'train_loss:', 0.80343814611434938, 'val_loss:', 0.2008467984199524)
('epoch', 177, 'train_loss:', 0.80790227174758911, 'val_loss:', 0.20156723260879517)
('epoch', 178, 'train_loss:', 0.80649310350418091, 'val_loss:', 0.2041646957397461)
('epoch', 179, 'train_loss:', 0.80956378936767581, 'val_loss:', 0.20306782245635988)
('epoch', 180, 'train_loss:', 0.80629492282867432, 'val_loss:', 0.20149733543395995)
('epoch', 181, 'train_loss:', 0.79896715402603147, 'val_loss:', 0.20265541791915895)
('epoch', 182, 'train_loss:', 0.80585504770278926, 'val_loss:', 0.20088593959808348)
('epoch', 183, 'train_loss:', 0.80195667266845705, 'val_loss:', 0.203412024974823)
('epoch', 184, 'train_loss:', 0.80106619596481321, 'val_loss:', 0.20113308906555175)
('epoch', 185, 'train_loss:', 0.8012569880485535, 'val_loss:', 0.20160174608230591)
('epoch', 186, 'train_loss:', 0.79763164758682248, 'val_loss:', 0.20095303297042846)
('epoch', 187, 'train_loss:', 0.79479146718978877, 'val_loss:', 0.20111221551895142)
('epoch', 188, 'train_loss:', 0.79141617536544795, 'val_loss:', 0.20071185350418091)
('epoch', 189, 'train_loss:', 0.79195217370986937, 'val_loss:', 0.19980830192565918)
('epoch', 190, 'train_loss:', 0.79031212568283082, 'val_loss:', 0.2001003384590149)
('epoch', 191, 'train_loss:', 0.79578833580017094, 'val_loss:', 0.19878604888916016)
('epoch', 192, 'train_loss:', 0.78876972675323487, 'val_loss:', 0.19870969772338867)
('epoch', 193, 'train_loss:', 0.79040844202041627, 'val_loss:', 0.1986517596244812)
('epoch', 194, 'train_loss:', 0.79566344738006589, 'val_loss:', 0.19780064821243287)
('epoch', 195, 'train_loss:', 0.7852580285072327, 'val_loss:', 0.19906443119049072)
('epoch', 196, 'train_loss:', 0.79188584327697753, 'val_loss:', 0.19728957176208495)
('epoch', 197, 'train_loss:', 0.78883152484893804, 'val_loss:', 0.19871501445770265)
('epoch', 198, 'train_loss:', 0.78748819351196286, 'val_loss:', 0.19805076122283935)
('epoch', 199, 'train_loss:', 0.78878339767456052, 'val_loss:', 0.19682616949081422)
('epoch', 200, 'train_loss:', 0.78276276111602783, 'val_loss:', 0.19639532566070556)
('epoch', 201, 'train_loss:', 0.78228332042694093, 'val_loss:', 0.19546143293380738)
('epoch', 202, 'train_loss:', 0.77952935695648196, 'val_loss:', 0.19556320905685426)
('epoch', 203, 'train_loss:', 0.78108229398727413, 'val_loss:', 0.19495894193649291)
('epoch', 204, 'train_loss:', 0.77649715423583987, 'val_loss:', 0.19373264074325561)
('epoch', 205, 'train_loss:', 0.77340513229370123, 'val_loss:', 0.1962685203552246)
('epoch', 206, 'train_loss:', 0.77437881946563725, 'val_loss:', 0.19546905755996705)
('epoch', 207, 'train_loss:', 0.77505029201507569, 'val_loss:', 0.19702262401580811)
('epoch', 208, 'train_loss:', 0.77459794521331782, 'val_loss:', 0.19429899215698243)
('epoch', 209, 'train_loss:', 0.77140972852706913, 'val_loss:', 0.19545159816741944)
('epoch', 210, 'train_loss:', 0.77211985588073728, 'val_loss:', 0.19602165460586549)
('epoch', 211, 'train_loss:', 0.76736144542694096, 'val_loss:', 0.19515940427780151)
('epoch', 212, 'train_loss:', 0.77154132127761843, 'val_loss:', 0.1931131362915039)
('epoch', 213, 'train_loss:', 0.77245198488235478, 'val_loss:', 0.19348581552505492)
('epoch', 214, 'train_loss:', 0.76994364023208617, 'val_loss:', 0.19251575469970703)
('epoch', 215, 'train_loss:', 0.7673761916160583, 'val_loss:', 0.19353190898895264)
('epoch', 216, 'train_loss:', 0.76476543903350835, 'val_loss:', 0.19252880096435546)
('epoch', 217, 'train_loss:', 0.76215821504592896, 'val_loss:', 0.19104792594909667)
('epoch', 218, 'train_loss:', 0.7654142093658447, 'val_loss:', 0.19406502723693847)
('epoch', 219, 'train_loss:', 0.76162260293960571, 'val_loss:', 0.19290956020355224)
('epoch', 220, 'train_loss:', 0.76772377252578738, 'val_loss:', 0.19054049730300904)
('epoch', 221, 'train_loss:', 0.75823432922363276, 'val_loss:', 0.19295259237289428)
('epoch', 222, 'train_loss:', 0.75828666687011714, 'val_loss:', 0.19164870977401732)
('epoch', 223, 'train_loss:', 0.76166024923324582, 'val_loss:', 0.19088785409927367)
('epoch', 224, 'train_loss:', 0.76084285259246831, 'val_loss:', 0.19127421140670775)
('epoch', 225, 'train_loss:', 0.75594216346740728, 'val_loss:', 0.18950571298599242)
('epoch', 226, 'train_loss:', 0.75442981719970703, 'val_loss:', 0.18838366270065307)
('epoch', 227, 'train_loss:', 0.75971067667007441, 'val_loss:', 0.19067155122756957)
('epoch', 228, 'train_loss:', 0.75318158626556397, 'val_loss:', 0.18976675510406493)
('epoch', 229, 'train_loss:', 0.75558871269226069, 'val_loss:', 0.18934433937072753)
('epoch', 230, 'train_loss:', 0.75514973163604737, 'val_loss:', 0.1919751524925232)
('epoch', 231, 'train_loss:', 0.75120463609695431, 'val_loss:', 0.1889192295074463)
('epoch', 232, 'train_loss:', 0.74894848585128782, 'val_loss:', 0.18823458909988403)
('epoch', 233, 'train_loss:', 0.74361404418945309, 'val_loss:', 0.18851983547210693)
('epoch', 234, 'train_loss:', 0.74879074096679688, 'val_loss:', 0.1893332052230835)
('epoch', 235, 'train_loss:', 0.74809754133224482, 'val_loss:', 0.18728746414184572)
('epoch', 236, 'train_loss:', 0.7467628729343414, 'val_loss:', 0.18674394369125366)
('epoch', 237, 'train_loss:', 0.74421869039535526, 'val_loss:', 0.18800745248794556)
('epoch', 238, 'train_loss:', 0.74262470960617066, 'val_loss:', 0.18879603147506713)
('epoch', 239, 'train_loss:', 0.74416669368743893, 'val_loss:', 0.1856282377243042)
('epoch', 240, 'train_loss:', 0.74406257390975949, 'val_loss:', 0.190211660861969)
('epoch', 241, 'train_loss:', 0.74533317804336552, 'val_loss:', 0.18696112871170045)
('epoch', 242, 'train_loss:', 0.74006062865257261, 'val_loss:', 0.18556893110275269)
('epoch', 243, 'train_loss:', 0.74240808248519896, 'val_loss:', 0.18745716094970702)
('epoch', 244, 'train_loss:', 0.74188665986061098, 'val_loss:', 0.18478170394897461)
('epoch', 245, 'train_loss:', 0.73696637034416201, 'val_loss:', 0.18712886333465575)
('epoch', 246, 'train_loss:', 0.73133542656898498, 'val_loss:', 0.18464179277420045)
('epoch', 247, 'train_loss:', 0.73796968340873714, 'val_loss:', 0.18411349773406982)
('epoch', 248, 'train_loss:', 0.73658469319343567, 'val_loss:', 0.18405206441879274)
('epoch', 249, 'train_loss:', 0.73416941523551937, 'val_loss:', 0.18410716772079469)
('epoch', 250, 'train_loss:', 0.73484684467315675, 'val_loss:', 0.18378889322280884)
('epoch', 251, 'train_loss:', 0.73518183946609494, 'val_loss:', 0.18541818618774414)
('epoch', 252, 'train_loss:', 0.73227143049240118, 'val_loss:', 0.18466391324996947)
('epoch', 253, 'train_loss:', 0.73165340304374693, 'val_loss:', 0.18254343271255494)
('epoch', 254, 'train_loss:', 0.73231213212013246, 'val_loss:', 0.18537595033645629)
('epoch', 255, 'train_loss:', 0.73055935978889464, 'val_loss:', 0.18360574603080748)
('epoch', 256, 'train_loss:', 0.72946956634521487, 'val_loss:', 0.18116676807403564)
('epoch', 257, 'train_loss:', 0.72760729432106019, 'val_loss:', 0.18473072886466979)
('epoch', 258, 'train_loss:', 0.72348219633102417, 'val_loss:', 0.18355427980422973)
('epoch', 259, 'train_loss:', 0.72587185740470883, 'val_loss:', 0.18195091843605041)
('epoch', 260, 'train_loss:', 0.72696946740150448, 'val_loss:', 0.18411967515945435)
('epoch', 261, 'train_loss:', 0.71728838086128233, 'val_loss:', 0.18138111352920533)
('epoch', 262, 'train_loss:', 0.7248513150215149, 'val_loss:', 0.18259957313537598)
('epoch', 263, 'train_loss:', 0.72214326739311219, 'val_loss:', 0.18255449295043946)
('epoch', 264, 'train_loss:', 0.72045628547668461, 'val_loss:', 0.18230696558952331)
('epoch', 265, 'train_loss:', 0.72291199922561644, 'val_loss:', 0.18046528458595276)
('epoch', 266, 'train_loss:', 0.71847823143005374, 'val_loss:', 0.18155688524246216)
('epoch', 267, 'train_loss:', 0.71623401165008549, 'val_loss:', 0.18118436694145201)
('epoch', 268, 'train_loss:', 0.7148421978950501, 'val_loss:', 0.18133813858032227)
('epoch', 269, 'train_loss:', 0.71508001685142519, 'val_loss:', 0.18054396271705628)
('epoch', 270, 'train_loss:', 0.71307636022567744, 'val_loss:', 0.17964847207069398)
('epoch', 271, 'train_loss:', 0.7136559534072876, 'val_loss:', 0.17936337471008301)
('epoch', 272, 'train_loss:', 0.71173658251762395, 'val_loss:', 0.17911714553833008)
('epoch', 273, 'train_loss:', 0.71911218285560607, 'val_loss:', 0.18015836477279662)
('epoch', 274, 'train_loss:', 0.71017060637474061, 'val_loss:', 0.17758689045906068)
('epoch', 275, 'train_loss:', 0.71515265464782718, 'val_loss:', 0.179181067943573)
('epoch', 276, 'train_loss:', 0.70635969281196598, 'val_loss:', 0.17859624862670898)
('epoch', 277, 'train_loss:', 0.70827973961830137, 'val_loss:', 0.17899466991424562)
('epoch', 278, 'train_loss:', 0.70940260291099544, 'val_loss:', 0.17699266076087952)
('epoch', 279, 'train_loss:', 0.70854079246520996, 'val_loss:', 0.17934109449386595)
('epoch', 280, 'train_loss:', 0.70586587667465206, 'val_loss:', 0.17990441322326661)
('epoch', 281, 'train_loss:', 0.70532252192497258, 'val_loss:', 0.17757603287696838)
('epoch', 282, 'train_loss:', 0.70151973366737364, 'val_loss:', 0.17710424542427064)
('epoch', 283, 'train_loss:', 0.70378612995147705, 'val_loss:', 0.17691191911697388)
('epoch', 284, 'train_loss:', 0.70245122075080868, 'val_loss:', 0.17750113487243652)
('epoch', 285, 'train_loss:', 0.70192896008491512, 'val_loss:', 0.17796893477439879)
('epoch', 286, 'train_loss:', 0.69626682043075561, 'val_loss:', 0.17487343430519103)
('epoch', 287, 'train_loss:', 0.70226434946060179, 'val_loss:', 0.17887041568756104)
('epoch', 288, 'train_loss:', 0.69996346831321721, 'val_loss:', 0.17635501861572267)
('epoch', 289, 'train_loss:', 0.70010094761848451, 'val_loss:', 0.17674763798713683)
('epoch', 290, 'train_loss:', 0.69593480825424192, 'val_loss:', 0.17639514565467834)
('epoch', 291, 'train_loss:', 0.69865660905838012, 'val_loss:', 0.17522531628608704)
('epoch', 292, 'train_loss:', 0.69552452564239498, 'val_loss:', 0.17583394646644593)
('epoch', 293, 'train_loss:', 0.69667239546775817, 'val_loss:', 0.17514289021492005)
('epoch', 294, 'train_loss:', 0.68993587970733639, 'val_loss:', 0.17686394453048707)
('epoch', 295, 'train_loss:', 0.69770552992820745, 'val_loss:', 0.1765574860572815)
('epoch', 296, 'train_loss:', 0.69199658393859864, 'val_loss:', 0.1745331120491028)
('epoch', 297, 'train_loss:', 0.69248159408569332, 'val_loss:', 0.17454510331153869)
('epoch', 298, 'train_loss:', 0.68940792441368104, 'val_loss:', 0.17478609561920166)
('epoch', 299, 'train_loss:', 0.68540048480033877, 'val_loss:', 0.17218727231025696)
('epoch', 300, 'train_loss:', 0.69164883136749267, 'val_loss:', 0.17504707694053651)
('epoch', 301, 'train_loss:', 0.68936047196388239, 'val_loss:', 0.17110444903373717)
('epoch', 302, 'train_loss:', 0.68928146004676816, 'val_loss:', 0.17347946763038635)
('epoch', 303, 'train_loss:', 0.68562345862388607, 'val_loss:', 0.17409596920013429)
('epoch', 304, 'train_loss:', 0.68660052180290221, 'val_loss:', 0.17446132659912109)
('epoch', 305, 'train_loss:', 0.68457846760749819, 'val_loss:', 0.17233110070228577)
('epoch', 306, 'train_loss:', 0.68097751855850219, 'val_loss:', 0.17359840393066406)
('epoch', 307, 'train_loss:', 0.68447184681892392, 'val_loss:', 0.17491345763206481)
('epoch', 308, 'train_loss:', 0.68857212185859684, 'val_loss:', 0.17372824430465697)
('epoch', 309, 'train_loss:', 0.68750801444053655, 'val_loss:', 0.17202357053756714)
('epoch', 310, 'train_loss:', 0.68073996663093572, 'val_loss:', 0.17261015534400939)
('epoch', 311, 'train_loss:', 0.67810743212699887, 'val_loss:', 0.17346762895584106)
('epoch', 312, 'train_loss:', 0.67875073313713075, 'val_loss:', 0.17128657102584838)
('epoch', 313, 'train_loss:', 0.67527896285057065, 'val_loss:', 0.17176195740699768)
('epoch', 314, 'train_loss:', 0.67739078879356385, 'val_loss:', 0.17034413576126098)
('epoch', 315, 'train_loss:', 0.67654091954231266, 'val_loss:', 0.17046368360519409)
('epoch', 316, 'train_loss:', 0.67656293392181399, 'val_loss:', 0.17167280673980712)
('epoch', 317, 'train_loss:', 0.67359068989753723, 'val_loss:', 0.17070117115974426)
('epoch', 318, 'train_loss:', 0.67694634556770328, 'val_loss:', 0.17083411693572997)
('epoch', 319, 'train_loss:', 0.67595555663108831, 'val_loss:', 0.17141614913940428)
('epoch', 320, 'train_loss:', 0.67513430714607237, 'val_loss:', 0.16968487143516542)
('epoch', 321, 'train_loss:', 0.67065083146095272, 'val_loss:', 0.17111881256103514)
('epoch', 322, 'train_loss:', 0.67001370668411253, 'val_loss:', 0.17077257156372069)
('epoch', 323, 'train_loss:', 0.67208587288856503, 'val_loss:', 0.1686067509651184)
('epoch', 324, 'train_loss:', 0.6716578161716461, 'val_loss:', 0.16895302891731262)
('epoch', 325, 'train_loss:', 0.67086167812347408, 'val_loss:', 0.17024231553077698)
('epoch', 326, 'train_loss:', 0.66718720436096191, 'val_loss:', 0.16971392273902894)
('epoch', 327, 'train_loss:', 0.66988723516464233, 'val_loss:', 0.1693491232395172)
('epoch', 328, 'train_loss:', 0.66488804340362551, 'val_loss:', 0.16965481638908386)
('epoch', 329, 'train_loss:', 0.66174440383911137, 'val_loss:', 0.16626631498336791)
('epoch', 330, 'train_loss:', 0.66764469027519224, 'val_loss:', 0.1700074028968811)
('epoch', 331, 'train_loss:', 0.66548537254333495, 'val_loss:', 0.16791939735412598)
('epoch', 332, 'train_loss:', 0.66195991635322571, 'val_loss:', 0.16961702466011047)
('epoch', 333, 'train_loss:', 0.66406995773315425, 'val_loss:', 0.16711285710334778)
('epoch', 334, 'train_loss:', 0.65563737869262695, 'val_loss:', 0.16774580836296082)
('epoch', 335, 'train_loss:', 0.66091350436210627, 'val_loss:', 0.17021269917488099)
('epoch', 336, 'train_loss:', 0.65951080918312077, 'val_loss:', 0.16904596686363221)
('epoch', 337, 'train_loss:', 0.66263834714889525, 'val_loss:', 0.16799111485481263)
('epoch', 338, 'train_loss:', 0.66199281930923459, 'val_loss:', 0.16838229775428773)
('epoch', 339, 'train_loss:', 0.65935679912567136, 'val_loss:', 0.16775669455528258)
('epoch', 340, 'train_loss:', 0.65810006260871889, 'val_loss:', 0.16531034588813781)
('epoch', 341, 'train_loss:', 0.65800863742828364, 'val_loss:', 0.1656734275817871)
('epoch', 342, 'train_loss:', 0.65345353603363032, 'val_loss:', 0.16693706512451173)
('epoch', 343, 'train_loss:', 0.65705791473388675, 'val_loss:', 0.16783795475959778)
('epoch', 344, 'train_loss:', 0.65613477230072026, 'val_loss:', 0.16548509955406188)
('epoch', 345, 'train_loss:', 0.64580051779747005, 'val_loss:', 0.16805227518081664)
('epoch', 346, 'train_loss:', 0.64663484454154974, 'val_loss:', 0.16423750638961793)
('epoch', 347, 'train_loss:', 0.65520464897155761, 'val_loss:', 0.16631368398666382)
('epoch', 348, 'train_loss:', 0.65223352313041683, 'val_loss:', 0.16751031279563905)
('epoch', 349, 'train_loss:', 0.6517632341384888, 'val_loss:', 0.16503712773323059)
('epoch', 350, 'train_loss:', 0.64536393761634825, 'val_loss:', 0.1640605866909027)
('epoch', 351, 'train_loss:', 0.65129152178764338, 'val_loss:', 0.16421467661857606)
('epoch', 352, 'train_loss:', 0.64841089010238651, 'val_loss:', 0.16433249831199645)
('epoch', 353, 'train_loss:', 0.64815248966217043, 'val_loss:', 0.16514026761054992)
('epoch', 354, 'train_loss:', 0.64603807926177981, 'val_loss:', 0.16484039664268493)
('epoch', 355, 'train_loss:', 0.64193955540657044, 'val_loss:', 0.16503507494926453)
('epoch', 356, 'train_loss:', 0.64743908286094665, 'val_loss:', 0.16476452946662903)
('epoch', 357, 'train_loss:', 0.63874664664268499, 'val_loss:', 0.16391156554222108)
('epoch', 358, 'train_loss:', 0.64195052623748783, 'val_loss:', 0.16396612882614137)
('epoch', 359, 'train_loss:', 0.64183333635330198, 'val_loss:', 0.16496505141258239)
('epoch', 360, 'train_loss:', 0.64832383036613461, 'val_loss:', 0.16394792795181273)
('epoch', 361, 'train_loss:', 0.63884782075881963, 'val_loss:', 0.16436116933822631)
('epoch', 362, 'train_loss:', 0.64171722769737238, 'val_loss:', 0.16321865320205689)
('epoch', 363, 'train_loss:', 0.64090460300445562, 'val_loss:', 0.16121879816055298)
('epoch', 364, 'train_loss:', 0.63781983017921451, 'val_loss:', 0.16305876731872559)
('epoch', 365, 'train_loss:', 0.63560984849929814, 'val_loss:', 0.16135749578475952)
('epoch', 366, 'train_loss:', 0.63357363224029539, 'val_loss:', 0.16107998847961424)
('epoch', 367, 'train_loss:', 0.63477953076362614, 'val_loss:', 0.16175399184226991)
('epoch', 368, 'train_loss:', 0.63655212402343753, 'val_loss:', 0.16342697858810426)
('epoch', 369, 'train_loss:', 0.63586132168769838, 'val_loss:', 0.15984255075454712)
('epoch', 370, 'train_loss:', 0.63796548247337337, 'val_loss:', 0.16138971090316773)
('epoch', 371, 'train_loss:', 0.62898976087570191, 'val_loss:', 0.16313846826553344)
('epoch', 372, 'train_loss:', 0.63497946500778202, 'val_loss:', 0.16213770270347594)
('epoch', 373, 'train_loss:', 0.63334015965461732, 'val_loss:', 0.1598874032497406)
('epoch', 374, 'train_loss:', 0.63032005906105038, 'val_loss:', 0.16202986359596253)
('epoch', 375, 'train_loss:', 0.63145252346992498, 'val_loss:', 0.15879843592643739)
('epoch', 376, 'train_loss:', 0.6304912102222443, 'val_loss:', 0.16170044898986816)
('epoch', 377, 'train_loss:', 0.63094107389450071, 'val_loss:', 0.16154734015464783)
('epoch', 378, 'train_loss:', 0.63153666734695435, 'val_loss:', 0.1602211344242096)
('epoch', 379, 'train_loss:', 0.62549109578132633, 'val_loss:', 0.15896490573883057)
('epoch', 380, 'train_loss:', 0.62775838017463681, 'val_loss:', 0.15939861297607422)
('epoch', 381, 'train_loss:', 0.63109701633453374, 'val_loss:', 0.15953384160995485)
('epoch', 382, 'train_loss:', 0.62157122373580931, 'val_loss:', 0.16188888549804686)
('epoch', 383, 'train_loss:', 0.62209523797035216, 'val_loss:', 0.15840137958526612)
('epoch', 384, 'train_loss:', 0.62386706948280335, 'val_loss:', 0.16035567641258239)
('epoch', 385, 'train_loss:', 0.6273463892936707, 'val_loss:', 0.15927982568740845)
('epoch', 386, 'train_loss:', 0.62230689406394957, 'val_loss:', 0.15669042348861695)
('epoch', 387, 'train_loss:', 0.625983909368515, 'val_loss:', 0.15960919618606567)
('epoch', 388, 'train_loss:', 0.62706509828567503, 'val_loss:', 0.15908318996429444)
('epoch', 389, 'train_loss:', 0.61884082674980168, 'val_loss:', 0.15673886418342589)
('epoch', 390, 'train_loss:', 0.61992493867874143, 'val_loss:', 0.15758004665374756)
('epoch', 391, 'train_loss:', 0.62236124515533442, 'val_loss:', 0.15897955656051635)
('epoch', 392, 'train_loss:', 0.62123176217079168, 'val_loss:', 0.15895387530326843)
('epoch', 393, 'train_loss:', 0.61822394609451292, 'val_loss:', 0.159055655002594)
('epoch', 394, 'train_loss:', 0.61981850743293765, 'val_loss:', 0.1571424424648285)
('epoch', 395, 'train_loss:', 0.61721324443817138, 'val_loss:', 0.15790511488914491)
('epoch', 396, 'train_loss:', 0.61362183690071104, 'val_loss:', 0.15743650913238524)
('epoch', 397, 'train_loss:', 0.61733471035957332, 'val_loss:', 0.15657249093055725)
('epoch', 398, 'train_loss:', 0.61699680805206303, 'val_loss:', 0.15684455513954162)
('epoch', 399, 'train_loss:', 0.61482381105422979, 'val_loss:', 0.15749470114707947)
('epoch', 400, 'train_loss:', 0.61287011146545411, 'val_loss:', 0.15750619530677795)
('epoch', 401, 'train_loss:', 0.61157491445541379, 'val_loss:', 0.15729819297790526)
('epoch', 402, 'train_loss:', 0.61007120847702023, 'val_loss:', 0.16080234050750733)
('epoch', 403, 'train_loss:', 0.60971273899078371, 'val_loss:', 0.15517479419708252)
('epoch', 404, 'train_loss:', 0.61227527260780334, 'val_loss:', 0.15538527369499205)
('epoch', 405, 'train_loss:', 0.61341263771057131, 'val_loss:', 0.15393527865409851)
('epoch', 406, 'train_loss:', 0.60583901047706601, 'val_loss:', 0.15678057312965393)
('epoch', 407, 'train_loss:', 0.60650790572166446, 'val_loss:', 0.15674407839775084)
('epoch', 408, 'train_loss:', 0.61099190115928648, 'val_loss:', 0.15611499428749084)
('epoch', 409, 'train_loss:', 0.60966589093208312, 'val_loss:', 0.15270265579223632)
('epoch', 410, 'train_loss:', 0.60239086031913758, 'val_loss:', 0.15703222990036012)
('epoch', 411, 'train_loss:', 0.60381695270538327, 'val_loss:', 0.15666796565055846)
('epoch', 412, 'train_loss:', 0.60367259621620173, 'val_loss:', 0.15555708765983581)
('epoch', 413, 'train_loss:', 0.61261577248573307, 'val_loss:', 0.15436793804168703)
('epoch', 414, 'train_loss:', 0.60258856058120724, 'val_loss:', 0.15399866342544555)
('epoch', 415, 'train_loss:', 0.60103065609931949, 'val_loss:', 0.15479884743690492)
('epoch', 416, 'train_loss:', 0.60470224738121037, 'val_loss:', 0.15903829097747801)
('epoch', 417, 'train_loss:', 0.60070655345916746, 'val_loss:', 0.15371667146682738)
('epoch', 418, 'train_loss:', 0.60008171677589417, 'val_loss:', 0.15471116304397584)
('epoch', 419, 'train_loss:', 0.59976631283760073, 'val_loss:', 0.15549683094024658)
('epoch', 420, 'train_loss:', 0.59935528874397281, 'val_loss:', 0.15358415722846985)
('epoch', 421, 'train_loss:', 0.59937944054603576, 'val_loss:', 0.15239619612693786)
('epoch', 422, 'train_loss:', 0.60075922250747682, 'val_loss:', 0.15535476684570312)
('epoch', 423, 'train_loss:', 0.59958329796791077, 'val_loss:', 0.15223621249198913)
('epoch', 424, 'train_loss:', 0.59636448383331297, 'val_loss:', 0.15405423879623414)
('epoch', 425, 'train_loss:', 0.59443100333213805, 'val_loss:', 0.15409379482269286)
('epoch', 426, 'train_loss:', 0.59613657116889951, 'val_loss:', 0.15237417459487915)
('epoch', 427, 'train_loss:', 0.59406911730766299, 'val_loss:', 0.15065423130989075)
('epoch', 428, 'train_loss:', 0.60520207762718203, 'val_loss:', 0.15137023687362672)
('epoch', 429, 'train_loss:', 0.59583860635757446, 'val_loss:', 0.15459245443344116)
('epoch', 430, 'train_loss:', 0.59236233711242681, 'val_loss:', 0.15400536417961119)
('epoch', 431, 'train_loss:', 0.59440803408622744, 'val_loss:', 0.15156625628471374)
('epoch', 432, 'train_loss:', 0.59447217226028437, 'val_loss:', 0.14953526616096496)
('epoch', 433, 'train_loss:', 0.59358236312866208, 'val_loss:', 0.14923418402671815)
('epoch', 434, 'train_loss:', 0.59563210368156438, 'val_loss:', 0.15166532635688781)
('epoch', 435, 'train_loss:', 0.59454169511795041, 'val_loss:', 0.15143224954605103)
('epoch', 436, 'train_loss:', 0.5853294956684113, 'val_loss:', 0.15235858559608459)
('epoch', 437, 'train_loss:', 0.5910798120498657, 'val_loss:', 0.15253762602806092)
('epoch', 438, 'train_loss:', 0.58950557231903078, 'val_loss:', 0.15011390447616577)
('epoch', 439, 'train_loss:', 0.58912278652191163, 'val_loss:', 0.15084316134452819)
('epoch', 440, 'train_loss:', 0.59396168828010554, 'val_loss:', 0.15404098629951476)
('epoch', 441, 'train_loss:', 0.58621987223625183, 'val_loss:', 0.15205748677253722)
('epoch', 442, 'train_loss:', 0.58950573682785035, 'val_loss:', 0.14978333711624145)
('epoch', 443, 'train_loss:', 0.58511270761489864, 'val_loss:', 0.14899868607521058)
('epoch', 444, 'train_loss:', 0.57989017248153685, 'val_loss:', 0.15017619013786315)
('epoch', 445, 'train_loss:', 0.58812976360321045, 'val_loss:', 0.15050985693931579)
('epoch', 446, 'train_loss:', 0.57991096735000613, 'val_loss:', 0.15283631205558776)
('epoch', 447, 'train_loss:', 0.58093574404716497, 'val_loss:', 0.14850524783134461)
('epoch', 448, 'train_loss:', 0.58582429647445677, 'val_loss:', 0.14812806606292725)
('epoch', 449, 'train_loss:', 0.58723910212516783, 'val_loss:', 0.15135868906974792)
('epoch', 450, 'train_loss:', 0.5849939322471619, 'val_loss:', 0.14975533723831178)
('epoch', 451, 'train_loss:', 0.5808294582366943, 'val_loss:', 0.15207485556602479)
('epoch', 452, 'train_loss:', 0.5832345688343048, 'val_loss:', 0.1521584165096283)
('epoch', 453, 'train_loss:', 0.57575919151306154, 'val_loss:', 0.14843347668647766)
('epoch', 454, 'train_loss:', 0.58288106083869939, 'val_loss:', 0.14986467003822326)
('epoch', 455, 'train_loss:', 0.57796553015708918, 'val_loss:', 0.14798893570899962)
('epoch', 456, 'train_loss:', 0.58139755249023439, 'val_loss:', 0.14958157658576965)
('epoch', 457, 'train_loss:', 0.58059463977813719, 'val_loss:', 0.14893167495727538)
('epoch', 458, 'train_loss:', 0.57491690397262574, 'val_loss:', 0.14977026104927063)
('epoch', 459, 'train_loss:', 0.57773362636566161, 'val_loss:', 0.1487860631942749)
('epoch', 460, 'train_loss:', 0.57454466342926025, 'val_loss:', 0.14768652796745299)
('epoch', 461, 'train_loss:', 0.57845444440841676, 'val_loss:', 0.14931580185890198)
('epoch', 462, 'train_loss:', 0.57661459207534793, 'val_loss:', 0.14980093836784364)
('epoch', 463, 'train_loss:', 0.57827478885650629, 'val_loss:', 0.14925673961639405)
('epoch', 464, 'train_loss:', 0.57571175813674924, 'val_loss:', 0.14874448895454406)
('epoch', 465, 'train_loss:', 0.57326411366462704, 'val_loss:', 0.15019038438796997)
('epoch', 466, 'train_loss:', 0.57361686110496524, 'val_loss:', 0.14500931262969971)
('epoch', 467, 'train_loss:', 0.5728773450851441, 'val_loss:', 0.14727585673332214)
('epoch', 468, 'train_loss:', 0.56921637535095215, 'val_loss:', 0.14786282777786255)
('epoch', 469, 'train_loss:', 0.57791472673416133, 'val_loss:', 0.14735315918922423)
('epoch', 470, 'train_loss:', 0.57208625793457035, 'val_loss:', 0.14844440340995788)
('epoch', 471, 'train_loss:', 0.57220813989639285, 'val_loss:', 0.14743801116943359)
('epoch', 472, 'train_loss:', 0.57325616478919983, 'val_loss:', 0.14902574539184571)
('epoch', 473, 'train_loss:', 0.57175440907478337, 'val_loss:', 0.14757161259651183)
('epoch', 474, 'train_loss:', 0.56643579006195066, 'val_loss:', 0.14520901799201966)
('epoch', 475, 'train_loss:', 0.57410729169845576, 'val_loss:', 0.1480829119682312)
('epoch', 476, 'train_loss:', 0.57010548710823061, 'val_loss:', 0.14755460858345032)
('epoch', 477, 'train_loss:', 0.56379708647727966, 'val_loss:', 0.14568852424621581)
('epoch', 478, 'train_loss:', 0.56835868835449221, 'val_loss:', 0.1454549789428711)
('epoch', 479, 'train_loss:', 0.56998659610748292, 'val_loss:', 0.14619881272315979)
('epoch', 480, 'train_loss:', 0.56458073616027837, 'val_loss:', 0.1453123676776886)
('epoch', 481, 'train_loss:', 0.56379361271858219, 'val_loss:', 0.14836902499198915)
('epoch', 482, 'train_loss:', 0.56734537005424501, 'val_loss:', 0.1445436656475067)
('epoch', 483, 'train_loss:', 0.56535666823387143, 'val_loss:', 0.14721503376960754)
('epoch', 484, 'train_loss:', 0.56686814904212957, 'val_loss:', 0.14543633222579955)
('epoch', 485, 'train_loss:', 0.56378774046897884, 'val_loss:', 0.14495384335517883)
('epoch', 486, 'train_loss:', 0.5644581162929535, 'val_loss:', 0.14457543492317199)
('epoch', 487, 'train_loss:', 0.56068984508514408, 'val_loss:', 0.14460681080818177)
('epoch', 488, 'train_loss:', 0.55932440876960754, 'val_loss:', 0.14577646017074586)
('epoch', 489, 'train_loss:', 0.55854457497596743, 'val_loss:', 0.14610348343849183)
('epoch', 490, 'train_loss:', 0.55936159372329708, 'val_loss:', 0.14743524432182312)
('epoch', 491, 'train_loss:', 0.55893161296844485, 'val_loss:', 0.1471082878112793)
('epoch', 492, 'train_loss:', 0.55724035263061522, 'val_loss:', 0.14577819347381593)
('epoch', 493, 'train_loss:', 0.5630625629425049, 'val_loss:', 0.14319829702377318)
('epoch', 494, 'train_loss:', 0.56416427612304687, 'val_loss:', 0.14539652466773986)
('epoch', 495, 'train_loss:', 0.55743517518043517, 'val_loss:', 0.14443556785583497)
('epoch', 496, 'train_loss:', 0.55635664463043211, 'val_loss:', 0.14429223537445068)
('epoch', 497, 'train_loss:', 0.55971294403076177, 'val_loss:', 0.14398383975028992)
('epoch', 498, 'train_loss:', 0.55511233925819392, 'val_loss:', 0.14275503277778626)
('epoch', 499, 'train_loss:', 0.55812609314918515, 'val_loss:', 0.14347797989845276)
('epoch', 500, 'train_loss:', 0.55019297838211056, 'val_loss:', 0.14403087258338929)
('epoch', 501, 'train_loss:', 0.55522643923759463, 'val_loss:', 0.14439652800559999)
('epoch', 502, 'train_loss:', 0.55158964753150941, 'val_loss:', 0.14353729844093321)
('epoch', 503, 'train_loss:', 0.55840752243995662, 'val_loss:', 0.1439274287223816)
('epoch', 504, 'train_loss:', 0.56082513689994817, 'val_loss:', 0.1440991997718811)
('epoch', 505, 'train_loss:', 0.55341334581375123, 'val_loss:', 0.14436327695846557)
('epoch', 506, 'train_loss:', 0.54735991954803465, 'val_loss:', 0.14255099892616271)
('epoch', 507, 'train_loss:', 0.55434976100921629, 'val_loss:', 0.14459349513053893)
('epoch', 508, 'train_loss:', 0.55136332988739012, 'val_loss:', 0.141484637260437)
('epoch', 509, 'train_loss:', 0.55093375444412229, 'val_loss:', 0.14560756444931031)
('epoch', 510, 'train_loss:', 0.55042030453681945, 'val_loss:', 0.14248708724975587)
('epoch', 511, 'train_loss:', 0.54613181829452517, 'val_loss:', 0.14331075787544251)
('epoch', 512, 'train_loss:', 0.54607266902923579, 'val_loss:', 0.14321429491043092)
('epoch', 513, 'train_loss:', 0.54974742770195006, 'val_loss:', 0.14171831369400023)
('epoch', 514, 'train_loss:', 0.54529729127883908, 'val_loss:', 0.14274237275123597)
('epoch', 515, 'train_loss:', 0.55084340691566469, 'val_loss:', 0.14375407338142396)
('epoch', 516, 'train_loss:', 0.54602469921112062, 'val_loss:', 0.14205470681190491)
('epoch', 517, 'train_loss:', 0.54679418087005616, 'val_loss:', 0.14065248250961304)
('epoch', 518, 'train_loss:', 0.55004520773887633, 'val_loss:', 0.14106732964515686)
('epoch', 519, 'train_loss:', 0.5481712818145752, 'val_loss:', 0.14329211831092833)
('epoch', 520, 'train_loss:', 0.54982312321662907, 'val_loss:', 0.14601938247680665)
('epoch', 521, 'train_loss:', 0.54697988390922547, 'val_loss:', 0.1422621190547943)
('epoch', 522, 'train_loss:', 0.54621081113815306, 'val_loss:', 0.1428075361251831)
('epoch', 523, 'train_loss:', 0.54243820190429692, 'val_loss:', 0.14367976307868957)
('epoch', 524, 'train_loss:', 0.54483993887901305, 'val_loss:', 0.14278657078742982)
('epoch', 525, 'train_loss:', 0.54663684487342834, 'val_loss:', 0.14214593768119813)
('epoch', 526, 'train_loss:', 0.5445106697082519, 'val_loss:', 0.13995375037193297)
('epoch', 527, 'train_loss:', 0.54383638024330139, 'val_loss:', 0.13980164766311645)
('epoch', 528, 'train_loss:', 0.54179995059967045, 'val_loss:', 0.1401734745502472)
('epoch', 529, 'train_loss:', 0.5393056738376617, 'val_loss:', 0.14376145124435424)
('epoch', 530, 'train_loss:', 0.5403337574005127, 'val_loss:', 0.14154960989952087)
('epoch', 531, 'train_loss:', 0.53935973525047298, 'val_loss:', 0.14244892954826355)
('epoch', 532, 'train_loss:', 0.54371060371398927, 'val_loss:', 0.1413611352443695)
('epoch', 533, 'train_loss:', 0.54141041398048406, 'val_loss:', 0.14011776685714722)
('epoch', 534, 'train_loss:', 0.53341642379760745, 'val_loss:', 0.14086165428161621)
('epoch', 535, 'train_loss:', 0.54564514398574826, 'val_loss:', 0.14222308874130249)
('epoch', 536, 'train_loss:', 0.53619207978248595, 'val_loss:', 0.13916654229164124)
('epoch', 537, 'train_loss:', 0.5400981771945953, 'val_loss:', 0.14133517384529115)
('epoch', 538, 'train_loss:', 0.53351804852485651, 'val_loss:', 0.13912853360176086)
('epoch', 539, 'train_loss:', 0.53439838647842408, 'val_loss:', 0.14074031949043275)
('epoch', 540, 'train_loss:', 0.53586421847343446, 'val_loss:', 0.1400248122215271)
('epoch', 541, 'train_loss:', 0.53453424930572513, 'val_loss:', 0.1400907528400421)
('epoch', 542, 'train_loss:', 0.53617851972579955, 'val_loss:', 0.13882412672042846)
('epoch', 543, 'train_loss:', 0.53199179291725163, 'val_loss:', 0.13987097144126892)
('epoch', 544, 'train_loss:', 0.53380209088325503, 'val_loss:', 0.14035245180130004)
('epoch', 545, 'train_loss:', 0.5323123955726623, 'val_loss:', 0.14024652481079103)
('epoch', 546, 'train_loss:', 0.53012925386428833, 'val_loss:', 0.13882022857666015)
('epoch', 547, 'train_loss:', 0.53449521303176883, 'val_loss:', 0.14007720708847046)
('epoch', 548, 'train_loss:', 0.53244174361228946, 'val_loss:', 0.13903632998466492)
('epoch', 549, 'train_loss:', 0.52962933301925663, 'val_loss:', 0.13930428147315979)
('epoch', 550, 'train_loss:', 0.52967870950698848, 'val_loss:', 0.13808354139328002)
('epoch', 551, 'train_loss:', 0.53120767474174502, 'val_loss:', 0.13933317899703979)
('epoch', 552, 'train_loss:', 0.53157786607742308, 'val_loss:', 0.13825692415237426)
('epoch', 553, 'train_loss:', 0.53506120562553405, 'val_loss:', 0.14033812999725342)
('epoch', 554, 'train_loss:', 0.53262419581413267, 'val_loss:', 0.13765520095825196)
('epoch', 555, 'train_loss:', 0.52995469927787786, 'val_loss:', 0.13813137412071227)
('epoch', 556, 'train_loss:', 0.52943438172340396, 'val_loss:', 0.13912984609603882)
('epoch', 557, 'train_loss:', 0.52867206215858464, 'val_loss:', 0.13830886363983155)
('epoch', 558, 'train_loss:', 0.53001234412193299, 'val_loss:', 0.13841586470603942)
('epoch', 559, 'train_loss:', 0.52394439339637755, 'val_loss:', 0.1377897834777832)
('epoch', 560, 'train_loss:', 0.53127072811126708, 'val_loss:', 0.13900261163711547)
('epoch', 561, 'train_loss:', 0.52578081846237179, 'val_loss:', 0.13989852666854857)
('epoch', 562, 'train_loss:', 0.5302115070819855, 'val_loss:', 0.1385102653503418)
('epoch', 563, 'train_loss:', 0.52329279541969298, 'val_loss:', 0.13768702983856201)
('epoch', 564, 'train_loss:', 0.52554828763008121, 'val_loss:', 0.1395667600631714)
('epoch', 565, 'train_loss:', 0.52509292960166931, 'val_loss:', 0.13850414752960205)
('epoch', 566, 'train_loss:', 0.52479297280311588, 'val_loss:', 0.13591478824615477)
('epoch', 567, 'train_loss:', 0.52582754373550411, 'val_loss:', 0.1376293969154358)
('epoch', 568, 'train_loss:', 0.52259152412414556, 'val_loss:', 0.13948364257812501)
('epoch', 569, 'train_loss:', 0.52607226490974424, 'val_loss:', 0.13480523705482483)
('epoch', 570, 'train_loss:', 0.519814385175705, 'val_loss:', 0.13543116927146912)
('epoch', 571, 'train_loss:', 0.52476070046424861, 'val_loss:', 0.13685047745704651)
('epoch', 572, 'train_loss:', 0.52008859992027279, 'val_loss:', 0.13659625887870788)
('epoch', 573, 'train_loss:', 0.52605068445205694, 'val_loss:', 0.13801474690437318)
('epoch', 574, 'train_loss:', 0.52452641487121587, 'val_loss:', 0.13597307682037355)
('epoch', 575, 'train_loss:', 0.5205829727649689, 'val_loss:', 0.1375962495803833)
('epoch', 576, 'train_loss:', 0.51881750822067263, 'val_loss:', 0.13567314982414247)
('epoch', 577, 'train_loss:', 0.52126058697700506, 'val_loss:', 0.13567403316497803)
('epoch', 578, 'train_loss:', 0.52058987855911254, 'val_loss:', 0.13754436969757081)
('epoch', 579, 'train_loss:', 0.51528545618057253, 'val_loss:', 0.13349304795265199)
('epoch', 580, 'train_loss:', 0.51297135710716246, 'val_loss:', 0.13506894946098327)
('epoch', 581, 'train_loss:', 0.51749485850334165, 'val_loss:', 0.13562373638153077)
('epoch', 582, 'train_loss:', 0.51846647977828975, 'val_loss:', 0.13715608000755311)
('epoch', 583, 'train_loss:', 0.5186927735805511, 'val_loss:', 0.13376737356185914)
('epoch', 584, 'train_loss:', 0.51567533016204836, 'val_loss:', 0.13701456189155578)
('epoch', 585, 'train_loss:', 0.52118163943290707, 'val_loss:', 0.13556537866592408)
('epoch', 586, 'train_loss:', 0.51684349775314331, 'val_loss:', 0.13504909396171569)
('epoch', 587, 'train_loss:', 0.52090090990066529, 'val_loss:', 0.13522656679153441)
('epoch', 588, 'train_loss:', 0.52115375518798823, 'val_loss:', 0.13284478425979615)
('epoch', 589, 'train_loss:', 0.51514317512512209, 'val_loss:', 0.13425759553909303)
('epoch', 590, 'train_loss:', 0.51530581951141352, 'val_loss:', 0.13637934803962706)
('epoch', 591, 'train_loss:', 0.51789761900901798, 'val_loss:', 0.13788654923439025)
('epoch', 592, 'train_loss:', 0.50910482168197635, 'val_loss:', 0.13464770436286927)
('epoch', 593, 'train_loss:', 0.51657326579093932, 'val_loss:', 0.13612955093383788)
('epoch', 594, 'train_loss:', 0.50807497620582576, 'val_loss:', 0.13614139914512635)
('epoch', 595, 'train_loss:', 0.51494302153587346, 'val_loss:', 0.13762277841567994)
('epoch', 596, 'train_loss:', 0.51125672936439515, 'val_loss:', 0.13559903860092162)
('epoch', 597, 'train_loss:', 0.51392552852630613, 'val_loss:', 0.13466917753219604)
('epoch', 598, 'train_loss:', 0.51267133712768553, 'val_loss:', 0.13599251031875612)
('epoch', 599, 'train_loss:', 0.51134668350219725, 'val_loss:', 0.13508550763130189)

